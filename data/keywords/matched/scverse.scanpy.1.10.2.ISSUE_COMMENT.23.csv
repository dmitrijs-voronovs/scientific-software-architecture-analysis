id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1733:593,safety,test,test,593,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:907,safety,test,test,907,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:1002,safety,test,tests,1002,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,security,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,testability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:593,testability,test,test,593,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:907,testability,test,test,907,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:1002,testability,test,tests,1002,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,deployability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,integrability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,interoperability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,modifiability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:20,reliability,doe,does,20,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,reliability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:248,safety,test,test,248,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:291,safety,test,testing,291,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,security,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:248,testability,test,test,248,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:291,testability,test,testing,291,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:523,testability,integr,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```. df.sort_values( . ['highly_variable_nbatches', 'highly_variable_rank'], . ascending=[False, True], . na_position='last', . inplace=True, . ) . ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:736,availability,sli,slightly,736,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:47,deployability,fail,failed,47,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:221,energy efficiency,Current,Currently,221,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:552,integrability,batch,batch,552,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:552,performance,batch,batch,552,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:1001,performance,time,time,1001,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:47,reliability,fail,failed,47,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:736,reliability,sli,slightly,736,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:199,safety,test,test,199,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:199,testability,test,test,199,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:852,testability,assert,assert,852,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function? And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```. df = sc.pp.highly_variable_genes(. pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False. ). df = df.loc[df['highly_variable'],:]. seurat_hvg_info_batch = pd.read_csv(. FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}. ). # ranks might be slightly different due to many genes having same normalized var. seu = pd.Index(seurat_hvg_info_batch['x'].values). assert len(seu.intersection(df.index)) / 4000 > 0.95. ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:175,reliability,doe,doesn,175,"@jlause, thanks for figuring this out as well! A separate PR for this fix would be great. There should definitely be tests targeting this, since changing how the sort is done doesn't seem to break anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:117,safety,test,tests,117,"@jlause, thanks for figuring this out as well! A separate PR for this fix would be great. There should definitely be tests targeting this, since changing how the sort is done doesn't seem to break anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:117,testability,test,tests,117,"@jlause, thanks for figuring this out as well! A separate PR for this fix would be great. There should definitely be tests targeting this, since changing how the sort is done doesn't seem to break anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/pull/1735:854,deployability,fail,fail,854,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:917,deployability,fail,failing,917,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:854,reliability,fail,fail,854,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:917,reliability,fail,failing,917,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:84,safety,test,tests,84,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:132,safety,avoid,avoid,132,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:336,safety,test,testing,336,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:848,safety,test,tests,848,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:925,safety,test,tests,925,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:162,security,modif,modification,162,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:84,testability,test,tests,84,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:336,testability,test,testing,336,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:848,testability,test,tests,848,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:925,testability,test,tests,925,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:219,usability,user,user-images,219,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:427,usability,user,user-images,427,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:730,usability,user,user-images,730,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:897,usability,help,helping,897,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:175,deployability,fail,failing,175,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:408,deployability,Updat,Updating,408,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1208,deployability,manag,managed,1208,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1208,energy efficiency,manag,managed,1208,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:248,modifiability,concern,concerns,248,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:754,modifiability,interm,intermediate,754,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1018,modifiability,exten,extend,1018,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:854,performance,time,times,854,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:21,reliability,Doe,Does,21,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:175,reliability,fail,failing,175,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:806,reliability,Doe,Does,806,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:128,safety,Test,Test,128,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:229,safety,test,test,229,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:319,safety,test,tests,319,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:408,safety,Updat,Updating,408,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:849,safety,test,test,849,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1046,safety,test,test,1046,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1208,safety,manag,managed,1208,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:408,security,Updat,Updating,408,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:128,testability,Test,Test,128,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:229,testability,test,test,229,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:248,testability,concern,concerns,248,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:319,testability,test,tests,319,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:849,testability,test,test,849,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:1046,testability,test,test,1046,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/pull/1735:31,usability,interact,interact,31,"> ## Question. > . > Does this interact with group colors and dendrograms at all? Dendrogram and colors seem not affected. > ## Test change. > . > All of the plots will start failing because this will change the output for every test. I have a few concerns here:. > . > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%. > . > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)? > . > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again. > . > . > Proposed solution:. > . > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735
https://github.com/scverse/scanpy/issues/1739:321,deployability,pipelin,pipeline,321,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739
https://github.com/scverse/scanpy/issues/1739:321,integrability,pipelin,pipeline,321,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739
https://github.com/scverse/scanpy/issues/1739:125,interoperability,compatib,compatibility,125,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739
https://github.com/scverse/scanpy/issues/1739:333,testability,understand,understand,333,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739
https://github.com/scverse/scanpy/issues/1739:238,usability,user,users,238,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739
https://github.com/scverse/scanpy/pull/1740:1127,availability,slo,slow,1127,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:54,deployability,continu,continuos,54,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:157,deployability,API,API,157,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:157,integrability,API,API,157,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:157,interoperability,API,API,157,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:280,interoperability,standard,standard,280,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:810,performance,network,network,810,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:880,performance,perform,performance,880,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:994,performance,time,time,994,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1059,performance,time,time,1059,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1127,reliability,slo,slow,1127,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1026,safety,test,testing,1026,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:810,security,network,network,810,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:602,testability,understand,understand,602,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1026,testability,test,testing,1026,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:364,usability,user,user,364,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:880,usability,perform,performance,880,"> only working on genes. technically it could work on continuos covariates as well, should I add that option? Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in? > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:288,energy efficiency,cool,cool,288,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:24,performance,time,time,24,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:72,performance,parallel,parallel,72,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:125,performance,parallel,parallel,125,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:209,safety,compl,completely,209,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:391,safety,test,tests,391,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:209,security,compl,completely,209,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:340,testability,simpl,simplified,340,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:391,testability,test,tests,391,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:340,usability,simpl,simplified,340,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells. I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:462,integrability,transform,transformation,462,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:534,integrability,transform,transformation,534,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:549,integrability,transform,transformation,549,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:625,integrability,transform,transformation,625,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:794,integrability,transform,transformation,794,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1246,integrability,coupl,couple,1246,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:462,interoperability,transform,transformation,462,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:534,interoperability,transform,transformation,534,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:549,interoperability,transform,transformation,549,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:625,interoperability,transform,transformation,625,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:794,interoperability,transform,transformation,794,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:666,modifiability,layer,layer,666,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:777,modifiability,layer,layer,777,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:783,modifiability,layer,layer,783,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1246,modifiability,coupl,couple,1246,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1268,safety,test,tests,1268,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1246,testability,coupl,couple,1246,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1268,testability,test,tests,1268,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1143,usability,user,user-images,1143,"reproducibility with pysal:  . ```python. import scanpy as sc. import squidpy as sq. adata = sq.datasets.visium_hne_adata(). adj = adata.obsp[""connectivities""]. dat = adata.X.T. moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal. import esda. adj = adj.tolil(). neighbors = dict(enumerate(adj.rows)). weights = dict(enumerate(adj.data)). W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):. mi = esda.moran.Moran(. y, w, transformation=transformation, permutations=permutations. ). return mi.I. moran_list = []. transformation = ""O"" # original weights. layer = None. permutations = None. for g in adata.var_names.values:. mi = _compute_moran(. adata.obs_vector(g, layer=layer), W, transformation, permutations. ). moran_list.append(mi). moran_esda = np.array(moran_list). from scipy.stats import pearsonr. import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda). g.set_xlabel(""scanpy""). g.set_ylabel(""esda""). p = pearsonr(moran_scanpy, moran_esda)[0]. g.set_title(f""pearson_{round(p, 5)}""). ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:107,availability,consist,consistency,107,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:181,availability,error,errors,181,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:181,performance,error,errors,181,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:23,safety,test,tests,23,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:76,safety,test,tested,76,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:181,safety,error,errors,181,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:225,safety,review,review,225,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:301,safety,test,tests,301,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:363,security,auth,author,363,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:23,testability,test,tests,23,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:76,testability,test,tested,76,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:225,testability,review,review,225,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:301,testability,test,tests,301,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:107,usability,consist,consistency,107,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:181,usability,error,errors,181,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you! btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:696,availability,consist,consistent,696,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:1087,modifiability,interm,intermediate,1087,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:157,performance,parallel,parallelized,157,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:603,performance,performance issu,performance issue,603,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:318,reliability,pra,prange,318,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:178,safety,compl,completely,178,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:940,safety,test,tests,940,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:178,security,compl,completely,178,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:909,security,modif,modified,909,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:940,testability,test,tests,940,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:603,usability,perform,performance,603,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:696,usability,consist,consistent,696,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results. * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`. * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:247,deployability,releas,release,247,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details.  remember having a look but not quite getting it the first time... thank you! > Argument order.  . > Minor things.   . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:175,performance,time,time,175,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details.  remember having a look but not quite getting it the first time... thank you! > Argument order.  . > Minor things.   . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:16,reliability,pra,prange,16,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details.  remember having a look but not quite getting it the first time... thank you! > Argument order.  . > Minor things.   . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/pull/1740:117,safety,reme,remember,117,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details.  remember having a look but not quite getting it the first time... thank you! > Argument order.  . > Minor things.   . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740
https://github.com/scverse/scanpy/issues/1742:165,integrability,event,eventually,165,"Hi @venomandvenus, it's not clear if this is really a bug or you are just getting a bunch of warnings from seaborn, which can happen quite often. Did `sc.pl.violin` eventually output the plot you wanted? Also can you post a link to the tutorial you are following?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742
https://github.com/scverse/scanpy/issues/1742:28,usability,clear,clear,28,"Hi @venomandvenus, it's not clear if this is really a bug or you are just getting a bunch of warnings from seaborn, which can happen quite often. Did `sc.pl.violin` eventually output the plot you wanted? Also can you post a link to the tutorial you are following?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742
https://github.com/scverse/scanpy/issues/1744:46,availability,mask,mask,46,"I've been thinking it would be good to add a `mask` argument to a number of functions. I think `mask_vars=~(adata.var[""mito""] | adata.var[""ribo""])` could work here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1744
https://github.com/scverse/scanpy/issues/1745:41,deployability,fail,failed,41,"Nevermind, this is actually used, I just failed at searching  .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1745
https://github.com/scverse/scanpy/issues/1745:41,reliability,fail,failed,41,"Nevermind, this is actually used, I just failed at searching  .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1745
https://github.com/scverse/scanpy/issues/1746:160,deployability,fail,fails,160,Is it normal that https://scanpy.readthedocs.io/en/stable/ works just fine but https://scanpy.readthedocs.io redirects to the hosted readthedocs and eventually fails?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:149,integrability,event,eventually,149,Is it normal that https://scanpy.readthedocs.io/en/stable/ works just fine but https://scanpy.readthedocs.io redirects to the hosted readthedocs and eventually fails?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:160,reliability,fail,fails,160,Is it normal that https://scanpy.readthedocs.io/en/stable/ works just fine but https://scanpy.readthedocs.io redirects to the hosted readthedocs and eventually fails?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:51,deployability,fail,fails,51,"Weird, I guess I usually use the latter, which now fails (but didn't previously). First link works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:51,reliability,fail,fails,51,"Weird, I guess I usually use the latter, which now fails (but didn't previously). First link works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:387,deployability,fail,fail,387,"I can't tell if this is an issue with `readthedocs` or hosted redirects. I'm also not sure why we have hosted readthedocs since we pay for readthedocs hosting. I assume this is related to `Custom Domains` in the `readthedocs` settings:. <img width=""610"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412288-41f9c500-8730-11eb-99a6-775ca23d9a3e.png"">. But I get a fail whale if I try to edit that:. <img width=""257"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412338-58a01c00-8730-11eb-964b-d13743e7fbfe.png"">. Any suggestions @falexwolf @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:387,reliability,fail,fail,387,"I can't tell if this is an issue with `readthedocs` or hosted redirects. I'm also not sure why we have hosted readthedocs since we pay for readthedocs hosting. I assume this is related to `Custom Domains` in the `readthedocs` settings:. <img width=""610"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412288-41f9c500-8730-11eb-99a6-775ca23d9a3e.png"">. But I get a fail whale if I try to edit that:. <img width=""257"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412338-58a01c00-8730-11eb-964b-d13743e7fbfe.png"">. Any suggestions @falexwolf @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:189,usability,Custom,Custom,189,"I can't tell if this is an issue with `readthedocs` or hosted redirects. I'm also not sure why we have hosted readthedocs since we pay for readthedocs hosting. I assume this is related to `Custom Domains` in the `readthedocs` settings:. <img width=""610"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412288-41f9c500-8730-11eb-99a6-775ca23d9a3e.png"">. But I get a fail whale if I try to edit that:. <img width=""257"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412338-58a01c00-8730-11eb-964b-d13743e7fbfe.png"">. Any suggestions @falexwolf @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:279,usability,user,user-images,279,"I can't tell if this is an issue with `readthedocs` or hosted redirects. I'm also not sure why we have hosted readthedocs since we pay for readthedocs hosting. I assume this is related to `Custom Domains` in the `readthedocs` settings:. <img width=""610"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412288-41f9c500-8730-11eb-99a6-775ca23d9a3e.png"">. But I get a fail whale if I try to edit that:. <img width=""257"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412338-58a01c00-8730-11eb-964b-d13743e7fbfe.png"">. Any suggestions @falexwolf @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:464,usability,user,user-images,464,"I can't tell if this is an issue with `readthedocs` or hosted redirects. I'm also not sure why we have hosted readthedocs since we pay for readthedocs hosting. I assume this is related to `Custom Domains` in the `readthedocs` settings:. <img width=""610"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412288-41f9c500-8730-11eb-99a6-775ca23d9a3e.png"">. But I get a fail whale if I try to edit that:. <img width=""257"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/111412338-58a01c00-8730-11eb-964b-d13743e7fbfe.png"">. Any suggestions @falexwolf @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:62,deployability,updat,update,62,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:62,safety,updat,update,62,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:62,security,updat,update,62,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:33,testability,plan,plan,33,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1746:26,usability,custom,custom,26,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746
https://github.com/scverse/scanpy/issues/1747:322,deployability,api,api,322,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:322,integrability,api,api,322,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:322,interoperability,api,api,322,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:81,reliability,doe,doesn,81,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:280,safety,avoid,avoided,280,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:367,usability,interact,interact,367,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:598,modifiability,variab,variable,598,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:125,reliability,pra,practical,125,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:283,reliability,doe,doesn,283,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:291,reliability,pra,practically,291,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:921,reliability,doe,does,921,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:392,security,control,control,392,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:630,security,modif,modify,630,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:392,testability,control,control,392,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:374,usability,User,Users,374,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons). 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for .)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:90,availability,error,errors,90,"Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:90,performance,error,errors,90,"Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:90,safety,error,errors,90,"Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:90,usability,error,errors,90,"Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:512,availability,operat,operations,512,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:553,availability,slo,slow,553,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1109,availability,operat,operations,1109,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:996,energy efficiency,measur,measures,996,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1574,interoperability,convers,conversion,1574,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:624,modifiability,variab,variable,624,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:811,modifiability,variab,variable,811,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:577,performance,time,time,577,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:553,reliability,slo,slow,553,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1399,reliability,Doe,Does,1399,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:279,safety,reme,remember,279,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:30,security,control,control,30,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:843,security,modif,modify,843,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1138,security,ident,identical,1138,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:30,testability,control,control,30,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:12,usability,User,Users,12,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:169,usability,behavi,behaviour,169,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:194,usability,close,closest,194,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:92,availability,error,errors,92,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:296,availability,error,error,296,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:92,performance,error,errors,92,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:296,performance,error,error,296,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:92,safety,error,errors,92,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:296,safety,error,error,296,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:92,usability,error,errors,92,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:273,usability,user,user,273,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:296,usability,error,error,296,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:305,usability,clear,clear,305,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:427,availability,operat,operations,427,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:468,availability,slo,slow,468,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1560,availability,operat,operations,1560,"they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2314,deployability,automat,automatically,2314," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2813,deployability,automat,automatic,2813," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1447,energy efficiency,measur,measures,1447,"s are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:767,interoperability,convers,conversion,767,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2659,interoperability,standard,standard,2659," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2823,interoperability,convers,conversion,2823," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:539,modifiability,variab,variable,539,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1018,modifiability,variab,variable,1018,"ok your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1672,modifiability,interm,intermediate,1672,"it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:492,performance,time,time,492,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:468,reliability,slo,slow,468,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2189,reliability,doe,doesn,2189," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2211,reliability,pra,practical,2211," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2465,reliability,Doe,Does,2465," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:188,safety,reme,remember,188,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:645,safety,compl,complain,645,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:645,security,compl,complain,645,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:829,security,control,control,829,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:1589,security,ident,identical,1589,"don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2343,security,control,control,2343," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:829,testability,control,control,829,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2314,testability,automat,automatically,2314," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2343,testability,control,control,2343," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2813,testability,automat,automatic,2813," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:54,usability,behavi,behaviour,54,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:79,usability,close,closest,79,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:666,usability,user,user,666,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:946,usability,user,user,946,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this. > . > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any go",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1747:2603,usability,User,Users,2603," would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? I don't think there is any good way for a converter to know which columns are intended to be categorical/factors and which have just ended up that way after running Scanpy functions on the object. Getting factors back doesn't really make a practical difference (and there are advantages to storing things like that), it's just that it happens automatically with no way to control it I don't like. . > Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors? Not sure if there are alternatives to factors but I probably wouldn't use them even if there are. Users (and possibly functions) would be confused by non-standard types. Plus it assumes there is something inherently wrong with R factors which is another philosophical issue . Also that wouldn't address the automatic conversion problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747
https://github.com/scverse/scanpy/issues/1748:57,integrability,filter,filtered,57,"What I noticed is that these NaN appears when I used the filtered data frame. Instead of removing these filtered genes, I think the code is just making them as NaN...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1748
https://github.com/scverse/scanpy/issues/1748:104,integrability,filter,filtered,104,"What I noticed is that these NaN appears when I used the filtered data frame. Instead of removing these filtered genes, I think the code is just making them as NaN...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1748
https://github.com/scverse/scanpy/issues/1749:118,availability,sli,slightly,118,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:51,deployability,releas,release,51,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1018,deployability,log,logical,1018,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1070,deployability,updat,updated,1070,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1026,energy efficiency,CPU,CPU,1026,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1030,energy efficiency,core,cores,1030,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:357,modifiability,deco,decorator,357,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:603,modifiability,pac,packaging,603,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1026,performance,CPU,CPU,1026,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:118,reliability,sli,slightly,118,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1018,safety,log,logical,1018,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1070,safety,updat,updated,1070,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1018,security,log,logical,1018,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1050,security,Session,Session,1050,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1070,security,updat,updated,1070,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1018,testability,log,logical,1018,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>. <summary> My environment </summary>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. IPython 7.21.0. PIL 8.1.0. anndata 0.7.5. backcall 0.2.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 3.1.0. ipython_genutils 0.2.0. jedi 0.17.2. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. ptyprocess 0.7.0. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. scanpy 1.7.1. scipy 1.6.1. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. storemagic NA. tables 3.6.1. traitlets 5.0.5. wcwidth 0.2.5. -----. Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]. macOS-10.15.7-x86_64-i386-64bit. 16 logical CPU cores, i386. -----. Session information updated at 2021-03-20 16:27. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:25,deployability,updat,updated,25,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:36,deployability,contain,container,36,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:57,deployability,version,versions,57,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:100,deployability,updat,updated,100,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:731,deployability,updat,updated,731,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2445,deployability,log,logical,2445,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2491,deployability,updat,updated,2491,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:993,energy efficiency,cloud,cloudpickle,993,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2453,energy efficiency,CPU,CPU,2453,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2457,energy efficiency,core,cores,2457,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:57,integrability,version,versions,57,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:57,modifiability,version,versions,57,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1110,modifiability,deco,decorator,1110,"mple to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1619,modifiability,pac,packaging,1619,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:902,performance,bottleneck,bottleneck,902,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2453,performance,CPU,CPU,2453,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:16,safety,input,input,16,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:25,safety,updat,updated,25,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:100,safety,updat,updated,100,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:731,safety,updat,updated,731,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2445,safety,log,logical,2445,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2491,safety,updat,updated,2491,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:25,security,updat,updated,25,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:100,security,updat,updated,100,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:704,security,ident,identical,704,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:731,security,updat,updated,731,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:945,security,certif,certifi,945,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:1998,security,soc,socks,1998,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2445,security,log,logical,2445,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2471,security,Session,Session,2471,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2491,security,updat,updated,2491,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2445,testability,log,logical,2445,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:16,usability,input,input,16,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []. for i in range(10):. adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:2087,usability,tool,toolz,2087,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal). ```. Output:. ```pytd. env: PYTHONHASHSEED=0. 0.6. ```. In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. anyio NA. attr 20.3.0. babel 2.9.0. backcall 0.2.0. bottleneck 1.3.2. brotli NA. cairo 1.20.0. certifi 2020.12.05. cffi 1.14.5. chardet 4.0.0. cloudpickle 1.6.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. google NA. h5py 3.1.0. idna 2.10. igraph 0.8.3. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.17.2. jinja2 2.11.3. joblib 1.0.1. json5 NA. jsonschema 3.2.0. jupyter_server 1.4.1. jupyterlab_server 2.3.0. kiwisolver 1.3.1. legacy_api_wrap 0.0.0. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. markupsafe 1.1.1. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. nbclassic NA. nbformat 5.1.2. numba 0.52.0. numexpr 2.7.2. numpy 1.20.1. packaging 20.9. pandas 1.2.2. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.8. psutil 5.8.0. ptyprocess 0.7.0. pvectorc NA. pygments 2.8.1. pyparsing 2.4.7. pyrsistent NA. pytz 2021.1. requests 2.25.1. scanpy 1.7.1. scipy 1.6.1. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. sniffio 1.2.0. socks 1.7.1. sphinxcontrib NA. storemagic NA. tables 3.6.1. texttable 1.6.3. tlz 0.11.0. toolz 0.11.1. tornado 6.1. tqdm 4.59.0. traitlets 5.0.5. typing_extensions NA. urllib3 1.26.3. wcwidth 0.2.5. yaml 5.4.1. zmq 22.0.3. zope NA. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. jupyterlab 3.0.10. notebook 6.2.0. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.8.0-44-generic-x86_64-with-glibc2.10. 28 logical CPU cores. -----. Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:688,usability,indicat,indicates,688,"I've been digging a little deeper and I get it more reproducible on my system. I changed dataset from `pmc3k_processed` to `pbmc3k`. Now, I call `sc.pp._pca._pca_with_sparse` directly. I get different results for `X_pca` only for the first call. But from then on, the result are reproduced for 2 consecutive calls. ```python. %env PYTHONHASHSEED=0. import numpy as np. np.random.seed(42). import scanpy as sc. import scipy as sp. adata = sc.datasets.pbmc3k(). sc.settings.verbosity = 3. for i in range(5):. print(np.array_equal(. sc.pp._pca._pca_with_sparse(adata.X, 50, random_state=42)['X_pca'],. sc.pp._pca._pca_with_sparse(adata.X, 50, random_state=42)['X_pca']. )). ```. The `False` indicates that the first two PCAs are different. ```pytd. env: PYTHONHASHSEED=0. False. True. True. True. True. ```. I looked a bit into `_pca_with_sparse` and it seems that `random_init` is reproduced, but the `u` and `v` returned by `svds` do change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:91,usability,help,helpful,91,"If you can have a reproducible example of `svds` being non-deterministic, it would be very helpful to open an issue for that over on `scipy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/issues/1749:186,performance,time,time,186,"Hi, I had this problem too and using svd_solver=""full"" solved it for me. . It takes a bit longer to run the calculation (~50 seconds instead of ~15), but it returns the same result each time I run it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749
https://github.com/scverse/scanpy/pull/1753:41,deployability,automat,automatically,41,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:89,deployability,api,api,89,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:89,integrability,api,api,89,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:89,interoperability,api,api,89,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:99,modifiability,extens,extension,99,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:41,testability,automat,automatically,41,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:55,usability,clear,clear,55,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:980,deployability,api,api,980,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:980,integrability,api,api,980,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:1608,integrability,sub,subdirectory,1608,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:546,interoperability,stub,stub,546,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:980,interoperability,api,api,980,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:1133,interoperability,specif,specifically,1133,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:103,modifiability,interm,intermingling,103,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:1205,reliability,Doe,Doesn,1205,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:546,testability,stub,stub,546,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:231,usability,navigat,navigate,231,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:323,usability,user,user-images,323,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:486,usability,document,documentation,486,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:898,usability,user,user-defined-redirects,898,"> Regarding generated, theres both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>. <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity lets use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:361,deployability,api,api,361,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:732,deployability,API,APIs,732,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:746,deployability,fail,failed,746,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:361,integrability,api,api,361,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:732,integrability,API,APIs,732,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:103,interoperability,plug,plugin,103,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:361,interoperability,api,api,361,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:732,interoperability,API,APIs,732,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:746,reliability,fail,failed,746,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:32,usability,navigat,navigate,32,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1753:202,usability,user,user-images,202,"> This is not a pleasant way to navigate files:. <details>. <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select Hide Ignored Files. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if its possible! > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and dont do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html`  `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753
https://github.com/scverse/scanpy/pull/1754:166,reliability,doe,doesn,166,"I added the readthedocssphinx search when experimenting with which one to use. DocSearch just seemed a lot nicer. It's got fuzzy matching, so using UK vs US spelling doesn't matter, also gives more context about results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1754
https://github.com/scverse/scanpy/pull/1754:123,testability,fuzzy,fuzzy,123,"I added the readthedocssphinx search when experimenting with which one to use. DocSearch just seemed a lot nicer. It's got fuzzy matching, so using UK vs US spelling doesn't matter, also gives more context about results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1754
https://github.com/scverse/scanpy/pull/1754:198,testability,context,context,198,"I added the readthedocssphinx search when experimenting with which one to use. DocSearch just seemed a lot nicer. It's got fuzzy matching, so using UK vs US spelling doesn't matter, also gives more context about results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1754
https://github.com/scverse/scanpy/pull/1754:41,performance,time,times,41,"Looking good! But somehow theres like 3 times the same for each result  I assume two are expected as one is the page title and one the doc item title (e.g. function name), and we have one page per function/class. But shouldnt one side of the vertical line be e.g. the first line of the docstring? . ![popup](https://user-images.githubusercontent.com/291575/113139611-a1270180-9227-11eb-9d8c-54789c280fc0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1754
https://github.com/scverse/scanpy/pull/1754:319,usability,user,user-images,319,"Looking good! But somehow theres like 3 times the same for each result  I assume two are expected as one is the page title and one the doc item title (e.g. function name), and we have one page per function/class. But shouldnt one side of the vertical line be e.g. the first line of the docstring? . ![popup](https://user-images.githubusercontent.com/291575/113139611-a1270180-9227-11eb-9d8c-54789c280fc0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1754
https://github.com/scverse/scanpy/issues/1756:37,availability,failur,failure,37,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:474,availability,error,errors,474,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3973,availability,error,errors,3973,"-> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4481,availability,error,errors,4481,"uclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5267,availability,state,state,5267,"return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5295,availability,state,state,5295,"/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patch",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5555,availability,state,state,5555,"ib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5751,availability,state,state,5751,"tdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5865,availability,avail,available,5865,"es/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6037,availability,state,state,6037,", targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6057,availability,state,state,6057,",. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6183,availability,state,state,6183,"n3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6446,availability,state,state,6446,"y/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6588,availability,state,state,6588,"n self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7685,availability,state,state,7685,"te-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7778,availability,state,state,7778,"quire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lowe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7800,availability,state,state,7800,"gs, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7846,availability,state,state,7846,"func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builde",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7866,availability,state,state,7866,"). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7970,availability,state,state,7970,"er_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9558,availability,error,errors,9558,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:37,deployability,fail,failure,37,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1350,deployability,modul,module,1350,"------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2771,deployability,instal,installed,2771,"ssing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2964,deployability,modul,module,2964,"). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3154,deployability,modul,module,3154,".py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3421,deployability,modul,module,3421,"ties = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5005,deployability,pipelin,pipeline,5005,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5117,deployability,pipelin,pipeline,5117,"gs, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5875,deployability,pipelin,pipelines,5875,"ore/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 retur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7734,deployability,pipelin,pipeline,7734,"e_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8843,deployability,build,builder,8843,"tate.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9825,deployability,Fail,Failed,9825,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9849,deployability,pipelin,pipeline,9849,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:469,energy efficiency,core,core,469,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:635,energy efficiency,core,core,635,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:835,energy efficiency,core,core,835,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1022,energy efficiency,core,core,1022,"a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3475,energy efficiency,Reduc,Reduced,3475," 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 excep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3560,energy efficiency,core,core,3560,"/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3790,energy efficiency,core,core,3790,"from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4083,energy efficiency,core,core,4083," 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4337,energy efficiency,core,core,4337,"ayout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4591,energy efficiency,core,core,4591,"c). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4879,energy efficiency,core,core,4879,"v_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipeli",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5211,energy efficiency,core,core,5211,"lf._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_nam",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5485,energy efficiency,core,core,5485,".TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5701,energy efficiency,core,core,5701,"). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5950,energy efficiency,core,core,5950,"n_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6143,energy efficiency,core,core,6143,". 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6406,energy efficiency,core,core,6406,"=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6706,energy efficiency,core,core,6706,"mpiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TOD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6962,energy efficiency,core,core,6962,"py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7343,energy efficiency,core,core,7343,"(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lowe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7646,energy efficiency,core,core,7646,"se""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7931,energy efficiency,core,core,7931,"ython3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8232,energy efficiency,core,core,8232,"291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8487,energy efficiency,core,core,8487," if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8757,energy efficiency,core,core,8757,"owering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9009,energy efficiency,core,core,9009,"ctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9553,energy efficiency,core,core,9553,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9718,energy efficiency,core,core,9718,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2721,integrability,filter,filterwarnings,2721,"/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 ret",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2746,integrability,messag,message,2746,"e-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3248,integrability,sub,submatrix,3248,"_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3582,integrability,wrap,wrapper,3582,"_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5005,integrability,pipelin,pipeline,5005,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5117,integrability,pipelin,pipeline,5117,"gs, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5267,integrability,state,state,5267,"return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5295,integrability,state,state,5295,"/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patch",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5555,integrability,state,state,5555,"ib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5751,integrability,state,state,5751,"tdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5875,integrability,pipelin,pipelines,5875,"ore/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 retur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6037,integrability,state,state,6037,", targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6057,integrability,state,state,6057,",. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6183,integrability,state,state,6183,"n3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6446,integrability,state,state,6446,"y/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6588,integrability,state,state,6588,"n self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7685,integrability,state,state,7685,"te-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7734,integrability,pipelin,pipeline,7734,"e_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7778,integrability,state,state,7778,"quire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lowe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7800,integrability,state,state,7800,"gs, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7846,integrability,state,state,7846,"func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builde",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7866,integrability,state,state,7866,"). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7970,integrability,state,state,7970,"er_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9849,integrability,pipelin,pipeline,9849,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10236,integrability,pub,public,10236,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2746,interoperability,messag,message,2746,"e-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3582,interoperability,wrapper,wrapper,3582,"_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:454,modifiability,pac,packages,454,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:620,modifiability,pac,packages,620,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:820,modifiability,pac,packages,820,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1007,modifiability,pac,packages,1007,"m now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1350,modifiability,modul,module,1350,"------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1451,modifiability,pac,packages,1451,"ackages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1752,modifiability,pac,packages,1752,"31 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2123,modifiability,pac,packages,2123," . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2537,modifiability,pac,packages,2537,", use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2934,modifiability,pac,packages,2934,"arnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2964,modifiability,modul,module,2964,"). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3127,modifiability,pac,packages,3127,"/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3154,modifiability,modul,module,3154,".py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3392,modifiability,pac,packages,3392,"_distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_ty",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3421,modifiability,modul,module,3421,"ties = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3545,modifiability,pac,packages,3545,"npy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3565,modifiability,deco,decorators,3565,"_.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3775,modifiability,pac,packages,3775,"d""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4068,modifiability,pac,packages,4068,"import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4322,modifiability,pac,packages,4322,"56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4576,modifiability,pac,packages,4576,"n wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4864,modifiability,pac,packages,4864,"pile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All ava",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5196,modifiability,pac,packages,5196,", retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5470,modifiability,pac,packages,5470,"except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5686,modifiability,pac,packages,5686,"tion(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5935,modifiability,pac,packages,5935,", args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6128,modifiability,pac,packages,6128,"e_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_tim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6391,modifiability,pac,packages,6391,"nc_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6691,modifiability,pac,packages,6691,"numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 46",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6947,modifiability,pac,packages,6947,"ore/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7328,modifiability,pac,packages,7328,"ency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7631,modifiability,pac,packages,7631,"acy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7916,modifiability,pac,packages,7916,"nvs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8217,modifiability,pac,packages,8217,"alize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(blo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8472,modifiability,pac,packages,8472,"r_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8742,modifiability,pac,packages,8742,"> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8994,modifiability,pac,packages,8994,".Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9538,modifiability,pac,packages,9538,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10001,modifiability,pac,packages,10001,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10295,modifiability,pac,packages,10295,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:37,performance,failur,failure,37,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:474,performance,error,errors,474,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3973,performance,error,errors,3973,"-> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4481,performance,error,errors,4481,"uclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9558,performance,error,errors,9558,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:37,reliability,fail,failure,37,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5865,reliability,availab,available,5865,"es/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9825,reliability,Fail,Failed,9825,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:474,safety,error,errors,474,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:558,safety,except,except,558,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1225,safety,except,exception,1225,"it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1244,safety,except,exception,1244,"rts that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1323,safety,input,input-,1323,"---------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1350,safety,modul,module,1350,"------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2964,safety,modul,module,2964,"). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3154,safety,modul,module,3154,".py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3421,safety,modul,module,3421,"ties = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3966,safety,except,except,3966,"e>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3973,safety,error,errors,3973,"-> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4474,safety,except,except,4474,"duced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4481,safety,error,errors,4481,"uclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5865,safety,avail,available,5865,"es/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9403,safety,except,except,9403,"= self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9485,safety,except,exception,9485,"e/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9558,safety,error,errors,9558,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5865,security,availab,available,5865,"es/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7817,security,sign,signature,7817,"with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7836,security,sign,signature,7836," return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:367,testability,Assert,AssertionError,367,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:382,testability,Trace,Traceback,382,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1102,testability,Assert,AssertionError,1102,"d to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1130,testability,Assert,AssertionError,1130,"ors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1279,testability,Trace,Traceback,1279,"bors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1946,testability,simpl,simplefilter,1946,"rget.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5543,testability,assert,assert,5543,"a/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7107,testability,Simpl,SimpleTimer,7107,"python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7201,testability,Simpl,SimpleTimer,7201,"ne_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9264,testability,context,contextlib,9264,"if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/en",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9309,testability,trace,traceback,9309,"lower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9387,testability,trace,traceback,9387,"self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:474,usability,error,errors,474,"Hello,. I am now facing a problem of failure in computing neighbours when using scanpy or scvelo. when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`. or. `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`. it will always reports that. ```pytb. `computing neighbors. ---------------------------------------------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1323,usability,input,input-,1323,"---------------------------------------. AssertionError Traceback (most recent call last). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 743 try:. --> 744 yield. 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst). 327 val = self.lower_assign(ty, inst). --> 328 self.storevar(val, inst.target.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1946,usability,simpl,simplefilter,1946,"rget.name). 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name). 1277 name=name). -> 1278 raise AssertionError(msg). 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). <ipython-input-37-db298150880d> in <module>. ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy). 62 . 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):. ---> 64 neighbors(. 65 adata,. 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy). 161 warnings.simplefilter(""ignore""). 162 neighbors = Neighbors(adata). --> 163 neighbors.compute_neighbors(. 164 n_neighbors=n_neighbors,. 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 748 # we need self._distances also for method == 'gauss' if we didn't. 749 # use dense distances. --> 750 self._distances, self._connectivities = _compute_connectivities_umap(. 751 knn_indices,. 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 353 # umap 0.5.0. 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 355 from umap.umap_ import fuzzy_simplicial_set. 356 . 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3973,usability,error,errors,3973,"-> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4194,usability,statu,status,4194,"ral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4258,usability,statu,status,4258,"-> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4481,usability,error,errors,4481,"uclidean distance. 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5757,usability,statu,status,5757,".typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). --> 606 return pipeline.compile_extra(func). 607 . 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 351 self.state.lifted = (). 352 self.state.lifted_from = None. --> 353 return self._compile_bytecode(). 354 . 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 413 """""". 414 assert self.state.func_ir is None. --> 415 return self._compile_core(). 416 . 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 393 self.state.status.fail_reason = e. 394 if is_final_pipeline:. --> 395 raise e. 396 else:. 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 384 res = None. 385 try:. --> 386 pm.run(self.state). 387 if self.state.cr is not None:. 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7107,usability,Simpl,SimpleTimer,7107,"python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 337 (self.pipeline_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7201,usability,Simpl,SimpleTimer,7201,"ne_name, pass_desc). 338 patched_exception = self._patch_error(msg, e). --> 339 raise patched_exception. 340 . 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 328 pass_inst = _pass_registry.get(pss).pass_inst. 329 if isinstance(pass_inst, CompilerPass):. --> 330 self._runPass(idx, pass_inst, state). 331 else:. 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 287 mutated |= check(pss.run_initialization, internal_state). 288 with SimpleTimer() as pass_time:. --> 289 mutated |= check(pss.run_pass, internal_state). 290 with SimpleTimer() as finalize_time:. 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 260 . 261 def check(func, compiler_state):. --> 262 mangled = func(compiler_state). 263 if mangled not in (True, False):. 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8678,usability,Close,Close,8678,"lf, state). 461 . 462 # TODO: Pull this out into the pipeline. --> 463 NativeLowering().run_pass(state). 464 lowered = state['cr']. 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 382 lower = lowering.Lower(targetctx, library, fndesc, interp,. 383 metadata=metadata). --> 384 lower.lower(). 385 if not flags.no_cpython_wrapper:. 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self). 134 if self.generator_info is None:. 135 self.genlower = None. --> 136 self.lower_normal_function(self.fndesc). 137 else:. 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9410,usability,Stop,StopIteration,9410,"ratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9448,usability,Stop,StopIteration,9448,"b/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Wil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9558,usability,error,errors,9558,"ion(self, fndesc). 188 # Init argument values. 189 self.extract_function_arguments(). --> 190 entry_block_tail = self.lower_function_body(). 191 . 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self). 214 bb = self.blkmap[offset]. 215 self.builder.position_at_end(bb). --> 216 self.lower_block(block). 217 self.post_lower(). 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block). 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 229 loc=self.loc, errcls_=defaulterrcls):. --> 230 self.lower_inst(inst). 231 self.post_block(block). 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback). 133 value = type(). 134 try:. --> 135 self.gen.throw(type, value, traceback). 136 except StopIteration as exc:. 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs). 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)). 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None. --> 751 raise newerr.with_traceback(tb). 752 . 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend). Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:. def rdist(x, y):. <source elided>. result = 0.0. dim = x.shape[0]. ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52). ```. . sc.pp.filter_cells(unspliced, min_genes=200). dyn.pl.basic_stats(spliced)`. I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:54,deployability,version,version,54,which numba do you have? can you try with most recent version ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:54,integrability,version,version,54,which numba do you have? can you try with most recent version ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:54,modifiability,version,version,54,which numba do you have? can you try with most recent version ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:59,deployability,version,version,59,I'm running into this issue too and I have the most recent version of numba 0.53.1. Any info I can provide to help troubleshoot this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:59,integrability,version,version,59,I'm running into this issue too and I have the most recent version of numba 0.53.1. Any info I can provide to help troubleshoot this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:59,modifiability,version,version,59,I'm running into this issue too and I have the most recent version of numba 0.53.1. Any info I can provide to help troubleshoot this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:110,usability,help,help,110,I'm running into this issue too and I have the most recent version of numba 0.53.1. Any info I can provide to help troubleshoot this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11,availability,down,downgrade,11,"Hi, I have downgrade my numab version to 0.51, it works",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:30,deployability,version,version,30,"Hi, I have downgrade my numab version to 0.51, it works",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:30,integrability,version,version,30,"Hi, I have downgrade my numab version to 0.51, it works",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:30,modifiability,version,version,30,"Hi, I have downgrade my numab version to 0.51, it works",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9,availability,down,downgraded,9,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:29,deployability,version,version,29,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:29,integrability,version,version,29,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:29,modifiability,version,version,29,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:117,usability,help,help,117,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:30,deployability,continu,continue,30,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`. Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:253,deployability,releas,releases,253,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`. Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:138,usability,learn,learn,138,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`. Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:182,usability,learn,learn,182,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`. Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:332,usability,support,supports,332,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`. Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:201,energy efficiency,draw,drawback,201,"hey @WeilerP ,. > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:23,usability,learn,learn,23,"hey @WeilerP ,. > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:115,usability,support,supports,115,"hey @WeilerP ,. > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6,usability,confirm,confirming,6,"FWIW, confirming `umap-learn 0.5.1` works with `numba==0.53.1` on my machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:23,usability,learn,learn,23,"FWIW, confirming `umap-learn 0.5.1` works with `numba==0.53.1` on my machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:192,deployability,version,versions,192,"This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:192,integrability,version,versions,192,"This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:192,modifiability,version,versions,192,"This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:76,reliability,doe,does,76,"This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:86,deployability,version,versions,86,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:237,deployability,version,version,237,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:86,integrability,version,versions,86,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:237,integrability,version,version,237,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:86,modifiability,version,versions,86,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:237,modifiability,version,version,237,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:141,testability,understand,understanding,141,> @giovp I'm not sure I follow. What would change in scanpy? Shouldn't allowing newer versions of umap in scvelo get around these issues? my understanding is that the issue above appearss also for `sc.pp.neighbors` and so fixing a numba version would also be useful for scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:62,availability,replic,replicating,62,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:25,deployability,version,versions,25,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:98,deployability,version,version,98,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:25,integrability,version,versions,25,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:98,integrability,version,version,98,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:13,interoperability,conflict,conflicting,13,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:25,modifiability,version,versions,25,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:98,modifiability,version,version,98,"What are the conflicting versions exactly? I'm having trouble replicating. Also, what's the exact version bounds change that's being suggested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:61,availability,error,error,61,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2245,availability,state,state,2245,"op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2793,availability,error,errors,2793,"port umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetct",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3323,availability,error,errors,3323,"anpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4304,availability,state,state,4304,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4312,availability,State,StateDict,4312,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:375,deployability,modul,module,375,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1364,deployability,instal,installed,1364,"> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1568,deployability,modul,module,1568,"copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1782,deployability,modul,module,1782,"compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1978,deployability,modul,module,1978,"# use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2177,deployability,modul,module,2177,"connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3878,deployability,pipelin,pipeline,3878,"b/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </detai",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3905,deployability,pipelin,pipeline,3905,"umba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4013,deployability,pipelin,pipeline,4013,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4830,deployability,modul,module,4830,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2369,energy efficiency,core,core,2369,"ed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2610,energy efficiency,core,core,2610,"warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdesc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2914,energy efficiency,core,core,2914,"nvs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3179,energy efficiency,core,core,3179,"le>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, ret",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3444,energy efficiency,core,core,3444,"isp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3743,energy efficiency,core,core,3743,"compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party exten",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4106,energy efficiency,core,core,4106,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4388,energy efficiency,core,core,4388,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4666,energy efficiency,core,core,4666,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4671,energy efficiency,cpu,cpu,4671,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4725,energy efficiency,load,load,4725,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4766,energy efficiency,core,core,4766,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4863,energy efficiency,core,core,4863,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1314,integrability,filter,filterwarnings,1314,"recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/en",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1339,integrability,messag,message,1339,"ython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/pyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2245,integrability,state,state,2245,"op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2391,integrability,wrap,wrapper,2391,"umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3878,integrability,pipelin,pipeline,3878,"b/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </detai",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3905,integrability,pipelin,pipeline,3905,"umba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4013,integrability,pipelin,pipeline,4013,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4304,integrability,state,state,4304,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4312,integrability,State,StateDict,4312,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1339,interoperability,messag,message,1339,"ython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/pyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2391,interoperability,wrapper,wrapper,2391,"umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:375,modifiability,modul,module,375,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1538,modifiability,pac,packages,1538,"ric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1568,modifiability,modul,module,1568,"copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1755,modifiability,pac,packages,1755,"/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1782,modifiability,modul,module,1782,"compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1950,modifiability,pac,packages,1950," 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1978,modifiability,modul,module,1978,"# use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2150,modifiability,pac,packages,2150,"s/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2177,modifiability,modul,module,2177,"connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2354,modifiability,pac,packages,2354,"w not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2374,modifiability,deco,decorators,2374,"> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2595,modifiability,pac,packages,2595," warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2899,modifiability,pac,packages,2899,"/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 60",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3164,modifiability,pac,packages,3164,"s.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, libra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3429,modifiability,pac,packages,3429,"_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3728,modifiability,pac,packages,3728,"cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4091,modifiability,pac,packages,4091,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4373,modifiability,pac,packages,4373,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4651,modifiability,pac,packages,4651,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4740,modifiability,extens,extensions,4740,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4830,modifiability,modul,module,4830,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:61,performance,error,error,61,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2793,performance,error,errors,2793,"port umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetct",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3323,performance,error,errors,3323,"anpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4671,performance,cpu,cpu,4671,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4725,performance,load,load,4725,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:61,safety,error,error,61,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:349,safety,input,input-,349,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:375,safety,modul,module,375,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1568,safety,modul,module,1568,"copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1782,safety,modul,module,1782,"compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1978,safety,modul,module,1978,"# use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2177,safety,modul,module,2177,"connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2786,safety,except,except,2786,". 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2793,safety,error,errors,2793,"port umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetct",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3316,safety,except,except,3316,"envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3323,safety,error,errors,3323,"anpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4830,safety,modul,module,4830,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:305,testability,Trace,Traceback,305,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1622,testability,simpl,simplefilter,1622,"bors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4543,testability,context,context,4543,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:61,usability,error,error,61,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:349,usability,input,input-,349,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). sc.pp.neighbors(adata). ```. <details>. <summary>Details</summary>. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-4-5d47edb05ae7> in <module>. ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1622,usability,simpl,simplefilter,1622,"bors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 806 # we need self._distances also for method == 'gauss' if we didn't. 807 # use dense distances. --> 808 self._distances, self._connectivities = _compute_connectivities_umap(. 809 knn_indices,. 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 385 # umap 0.5.0. 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 387 from umap.umap_ import fuzzy_simplicial_set. 388 . 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>. 1 from warnings import warn, catch_warnings, simplefilter. ----> 2 from .umap_ import UMAP. 3 . 4 try:. 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>. 30 import umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2793,usability,error,errors,2793,"port umap.distances as dist. 31 . ---> 32 import umap.sparse as sparse. 33 . 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>. 10 import numpy as np. 11 . ---> 12 from umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetct",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3025,usability,statu,status,3025,"umap.utils import norm. 13 . 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3089,usability,statu,status,3089,"IC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>. 38 . 39 @numba.njit(""i4(i8[:])""). ---> 40 def tau_rand_int(state):. 41 """"""A fast (pseudo)-random number generator. 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3323,usability,error,errors,3323,"anpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func). 219 with typeinfer.register_dispatcher(disp):. 220 for sig in sigs:. --> 221 disp.compile(sig). 222 disp.disable_compile(). 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig). 907 with ev.trigger_event(""numba:compile"", data=ev_details):. 908 try:. --> 909 cres = self._compiler.compile(args, return_type). 910 except errors.ForceLiteralArg as e:. 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4925,usability,learn,learn,4925,"mpile(self, args, return_type). 77 . 78 def compile(self, args, return_type):. ---> 79 status, retval = self._compile_cached(args, return_type). 80 if status:. 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 91 . 92 try:. ---> 93 retval = self._compile_core(args, return_type). 94 except errors.TypingError as e:. 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 104 . 105 impl = self._get_implementation(args, {}). --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,. 107 self.targetdescr.target_context,. 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 602 compiler pipeline. 603 """""". --> 604 pipeline = pipeline_class(typingctx, targetctx, library,. 605 args, return_type, flags, locals). 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals). 308 config.reload_config(). 309 typingctx.refresh(). --> 310 targetctx.refresh(). 311 . 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self). 282 pass. 283 self.install_registry(builtin_registry). --> 284 self.load_additional_registries(). 285 # Also refresh typing context, since @overload declarations can. 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self). 76 . 77 # load 3rd party extensions. ---> 78 numba.core.entrypoints.init_all(). 79 . 80 @property. AttributeError: module 'numba' has no attribute 'core'. ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:354,availability,down,downstream,354,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:61,deployability,instal,install,61,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:303,deployability,version,versions,303,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:303,integrability,version,versions,303,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:227,interoperability,specif,specifically,227,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:303,modifiability,version,versions,303,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:19,usability,learn,learn,19,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:532,availability,down,downstream,532,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:372,deployability,version,version,372,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:481,deployability,version,versions,481,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:692,deployability,version,versions,692,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:372,integrability,version,version,372,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:481,integrability,version,versions,481,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:692,integrability,version,versions,692,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:405,interoperability,specif,specifically,405,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:372,modifiability,version,version,372,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:481,modifiability,version,versions,481,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:692,modifiability,version,versions,692,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:78,reliability,doe,does,78,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:154,safety,test,tests,154,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:132,security,hack,hack,132,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:149,testability,unit,unit,149,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:154,testability,test,tests,154,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:309,testability,plan,plan,309,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:661,usability,learn,learn,661,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5? This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1067,availability,error,errors,1067,"= call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2903,availability,state,state,2903,"er_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2960,availability,state,state,2960,"conda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2982,availability,state,state,2982,"es\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_sta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6059,availability,error,errors,6059,"module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, libra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6568,availability,error,errors,6568,"ced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7691,availability,state,state,7691," in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7719,availability,state,state,7719,", return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7976,availability,state,state,7976,"n compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8169,availability,state,state,8169,"a(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8283,availability,avail,available,8283,"te-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8452,availability,state,state,8452,"ss(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8472,availability,state,state,8472,"ctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initial",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8595,availability,state,state,8595,"ta\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8855,availability,state,state,8855,":\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mang",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8997,availability,state,state,8997,"-> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\Pro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10082,availability,state,state,10082,"a3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11686,availability,error,errors,11686," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:386,deployability,Fail,Failed,386,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:410,deployability,pipelin,pipeline,410,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2273,deployability,build,builder,2273,"c=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2399,deployability,build,builder,2399,"ite-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2444,deployability,build,builder,2444,"inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2510,deployability,build,builder,2510,"val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <nu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2879,deployability,build,builder,2879," -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3782,deployability,modul,module,3782," C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectiv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4872,deployability,instal,installed,4872,"\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5062,deployability,modul,module,5062,"ta.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5249,deployability,modul,module,5249,"\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5513,deployability,modul,module,5513,"._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7432,deployability,pipelin,pipeline,7432,".py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7544,deployability,pipelin,pipeline,7544,"n_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8293,deployability,pipelin,pipelines,8293,"s\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10983,deployability,build,builder,10983," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:482,energy efficiency,core,core,482,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:497,energy efficiency,model,models,497,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1062,energy efficiency,core,core,1062,"ion.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1225,energy efficiency,core,core,1225,"nda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, exp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1422,energy efficiency,core,core,1422,": native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1617,energy efficiency,core,core,1617,"ges\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\Progra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1803,energy efficiency,core,core,1803,"ad_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1994,energy efficiency,core,core,1994,"cent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2173,energy efficiency,core,core,2173," C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2368,energy efficiency,core,core,2368,"). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2659,energy efficiency,core,core,2659,"nst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3079,energy efficiency,core,core,3079,"_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3315,energy efficiency,core,core,3315,". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3518,energy efficiency,core,core,3518,"elf._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3533,energy efficiency,model,models,3533,"loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5567,energy efficiency,Reduc,Reduced,5567,"nn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except err",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5649,energy efficiency,core,core,5649,"py\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5876,energy efficiency,core,core,5876,"led""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.target",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6166,energy efficiency,core,core,6166,"4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6421,energy efficiency,core,core,6421," optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_cla",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6676,energy efficiency,core,core,6676,"(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, fun",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6961,energy efficiency,core,core,6961,"ta=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7306,energy efficiency,core,core,7306,"e_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhau",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7635,energy efficiency,core,core,7635,"Data\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7906,energy efficiency,core,core,7906,"l,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8119,energy efficiency,core,core,8119,"ion(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8365,energy efficiency,core,core,8365," args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8555,energy efficiency,core,core,8555,"ile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8815,energy efficiency,core,core,8815,"func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9112,energy efficiency,core,core,9112,"umba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Low",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9365,energy efficiency,core,core,9365,"re\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9743,energy efficiency,core,core,9743,"ncy_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10043,energy efficiency,core,core,10043,"cy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10341,energy efficiency,core,core,10341,"b\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10593,energy efficiency,core,core,10593,"312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10897,energy efficiency,core,core,10897," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11146,energy efficiency,core,core,11146," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11681,energy efficiency,core,core,11681," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:410,integrability,pipelin,pipeline,410,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2675,integrability,wrap,wrapper,2675,"nstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2903,integrability,state,state,2903,"er_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2960,integrability,state,state,2960,"conda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2982,integrability,state,state,2982,"es\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_sta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4822,integrability,filter,filterwarnings,4822,"bors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4847,integrability,messag,message,4847,"\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5343,integrability,sub,submatrix,5343,"write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if stat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5671,integrability,wrap,wrapper,5671," in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7432,integrability,pipelin,pipeline,7432,".py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7544,integrability,pipelin,pipeline,7544,"n_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7691,integrability,state,state,7691," in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). -->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7719,integrability,state,state,7719,", return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7976,integrability,state,state,7976,"n compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8169,integrability,state,state,8169,"a(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8293,integrability,pipelin,pipelines,8293,"s\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8452,integrability,state,state,8452,"ss(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8472,integrability,state,state,8472,"ctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initial",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8595,integrability,state,state,8595,"ta\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8855,integrability,state,state,8855,":\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mang",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8997,integrability,state,state,8997,"-> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\Pro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10082,integrability,state,state,10082,"a3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2675,interoperability,wrapper,wrapper,2675,"nstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4847,interoperability,messag,message,4847,"\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5671,interoperability,wrapper,wrapper,5671," in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:241,modifiability,pac,packages,241,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:614,modifiability,pac,packages,614,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:937,modifiability,pac,packages,937,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1047,modifiability,pac,packages,1047,"$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 el",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1210,modifiability,pac,packages,1210,"ramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1407,modifiability,pac,packages,1407,"ipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, arg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1602,modifiability,pac,packages,1602,"ib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1788,modifiability,pac,packages,1788,"m, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\Pro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1979,modifiability,pac,packages,1979,"back (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2158,modifiability,pac,packages,2158,"aError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2353,modifiability,pac,packages,2353,"t_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2644,modifiability,pac,packages,2644,"n(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2820,modifiability,pac,packages,2820,"ower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3064,modifiability,pac,packages,3064,"= self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3300,modifiability,pac,packages,3300,"34 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3782,modifiability,modul,module,3782," C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectiv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3883,modifiability,pac,packages,3883," sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4227,modifiability,pac,packages,4227,"ld_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:4638,modifiability,pac,packages,4638,"eption, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5032,modifiability,pac,packages,5032,"137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5062,modifiability,modul,module,5062,"ta.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5222,modifiability,pac,packages,5222,"-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5249,modifiability,modul,module,5249,"\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5484,modifiability,pac,packages,5484,"-> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5513,modifiability,modul,module,5513,"._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5634,modifiability,pac,packages,5634,"packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5654,modifiability,deco,decorators,5654,"bors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5861,modifiability,pac,packages,5861,"ow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6151,modifiability,pac,packages,6151,"issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6406,modifiability,pac,packages,6406,"uclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6661,modifiability,pac,packages,6661,"py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_ex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6946,modifiability,pac,packages,6946,":compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7291,modifiability,pac,packages,7291," self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7620,modifiability,pac,packages,7620,"e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_na",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7891,modifiability,pac,packages,7891,"ext,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8104,modifiability,pac,packages,8104,"t_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\nu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8350,modifiability,pac,packages,8350,"getctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-pack",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8540,modifiability,pac,packages,8540,"pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8800,modifiability,pac,packages,8800,"ile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 27",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9097,modifiability,pac,packages,9097,"te-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9350,modifiability,pac,packages,9350,"ages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9728,modifiability,pac,packages,9728,"0 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10028,modifiability,pac,packages,10028,"ception(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10326,modifiability,pac,packages,10326,"\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10578,modifiability,pac,packages,10578,"rnal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress Sto",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10882,modifiability,pac,packages,10882," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11131,modifiability,pac,packages,11131," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11666,modifiability,pac,packages,11666," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1067,performance,error,errors,1067,"= call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6059,performance,error,errors,6059,"module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, libra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6568,performance,error,errors,6568,"ced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11686,performance,error,errors,11686," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:386,reliability,Fail,Failed,386,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8283,reliability,availab,available,8283,"te-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1067,safety,error,errors,1067,"= call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1151,safety,except,except,1151,"kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3639,safety,except,exception,3639,"packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packag",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3658,safety,except,exception,3658,"\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3782,safety,modul,module,3782," C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectiv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5062,safety,modul,module,5062,"ta.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5249,safety,modul,module,5249,"\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:5513,safety,modul,module,5513,"._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 390 # umap 0.5.0. 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 392 from umap.umap_ import fuzzy_simplicial_set. 393 . 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6052,safety,except,except,6052,"py in <module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6059,safety,error,errors,6059,"module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, libra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6561,safety,except,except,6561,"""""""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_ext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6568,safety,error,errors,6568,"ced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8283,safety,avail,available,8283,"te-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11534,safety,except,except,11534," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11616,safety,except,exception,11616," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11686,safety,error,errors,11686," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:497,security,model,models,497,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2106,security,sign,signature,2106,"**kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2230,security,sign,signature,2230,"ing.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3533,security,model,models,3533,"loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8283,security,availab,available,8283,"te-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:978,testability,Trace,Traceback,978,"**solution please :** . scanpy . During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2870,testability,context,context,2870," 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2917,testability,context,context,2917,"expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2995,testability,context,context,2995,"lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:3693,testability,Trace,Traceback,3693,"). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 809 # we need self._distances also for method == 'gauss' if we didn't. 810 # use dense distances. --> 811 self._distances, self._connectivities = _compute_connectivities_umap(. 812 knn_indices,. 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:7964,testability,assert,assert,7964,"ompiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9510,testability,Simpl,SimpleTimer,9510,"Data\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9604,testability,Simpl,SimpleTimer,9604,"lf.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11398,testability,context,contextlib,11398," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11442,testability,trace,traceback,11442," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11518,testability,trace,traceback,11518," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:1067,usability,error,errors,1067,"= call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). computing neighbors. using 'X_pca' with n_pcs = 40. . LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 822 try:. --> 823 yield. 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst). 438 ty = self.typeof(inst.target.name). --> 439 val = self.lower_assign(ty, inst). 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst). 625 elif isinstance(value, ir.Expr):. --> 626 return self.lower_expr(ty, value). 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr). 1161 elif expr.op == 'call':. -> 1162 res = self.lower_call(resty, expr). 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2966,usability,stop,stop,2966,"3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:2973,usability,stop,stop,2973,"ite-packages\numba\core\lowering.py in lower_call(self, resty, expr). 890 else:. --> 891 res = self._lower_call_normal(fnty, expr, signature). 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature). 1132 . -> 1133 res = impl(self.builder, argvals, self.loc). 1134 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc). 1189 def __call__(self, builder, args, loc=None):. -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc). 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs). 1219 kwargs.pop('loc') # drop unused loc. -> 1220 return fn(*args, **kwargs). 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args). 37 state.start = context.get_constant(int_type, 0). ---> 38 state.stop = stop. 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value). 163 return super(_StructProxy, self).__setattr__(field, value). --> 164 self[self._datamodel.get_field_position(field)] = value. 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value). 187 else:. --> 188 raise TypeError(""Invalid store of {value.type} to "". 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last). ~\AppData\Local\Temp\ipykernel_11084\169722070.py in <module>. ----> 1 sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6059,usability,error,errors,6059,"module>. ----> 1 from .umap_ import UMAP. 2 . 3 # Workaround: https://github.com/numba/numba/issues/3341. 4 import numba. 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>. 52 from umap.spectral import spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, libra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6279,usability,statu,status,6279,"port spectral_layout. 53 from umap.utils import deheap_sort, submatrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6344,usability,statu,status,6344,"atrix. ---> 54 from umap.layouts import (. 55 optimize_layout_euclidean,. 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>. 37 },. 38 ). ---> 39 def rdist(x, y):. 40 """"""Reduced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:6568,usability,error,errors,6568,"ced Euclidean distance. 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func). 217 with typeinfer.register_dispatcher(disp):. 218 for sig in sigs:. --> 219 disp.compile(sig). 220 disp.disable_compile(). 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig). 963 with ev.trigger_event(""numba:compile"", data=ev_details):. 964 try:. --> 965 cres = self._compiler.compile(args, return_type). 966 except errors.ForceLiteralArg as e:. 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type). 123 . 124 def compile(self, args, return_type):. --> 125 status, retval = self._compile_cached(args, return_type). 126 if status:. 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type). 137 . 138 try:. --> 139 retval = self._compile_core(args, return_type). 140 except errors.TypingError as e:. 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 150 . 151 impl = self._get_implementation(args, {}). --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:8175,usability,statu,status,8175,".targetdescr.typing_context,. 153 self.targetdescr.target_context,. 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 714 pipeline = pipeline_class(typingctx, targetctx, library,. 715 args, return_type, flags, locals). --> 716 return pipeline.compile_extra(func). 717 . 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func). 450 self.state.lifted = (). 451 self.state.lifted_from = None. --> 452 return self._compile_bytecode(). 453 . 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self). 518 """""". 519 assert self.state.func_ir is None. --> 520 return self._compile_core(). 521 . 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 497 self.state.status.fail_reason = e. 498 if is_final_pipeline:. --> 499 raise e. 500 else:. 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self). 484 res = None. 485 try:. --> 486 pm.run(self.state). 487 if self.state.cr is not None:. 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9510,usability,Simpl,SimpleTimer,9510,"Data\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 366 (self.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:9604,usability,Simpl,SimpleTimer,9604,"lf.pipeline_name, pass_desc). 367 patched_exception = self._patch_error(msg, e). --> 368 raise patched_exception. 369 . 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state). 354 pass_inst = _pass_registry.get(pss).pass_inst. 355 if isinstance(pass_inst, CompilerPass):. --> 356 self._runPass(idx, pass_inst, state). 357 else:. 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 33 def _acquire_compile_lock(*args, **kwargs):. 34 with self:. ---> 35 return func(*args, **kwargs). 36 return _acquire_compile_lock. 37 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state). 309 mutated |= check(pss.run_initialization, internal_state). 310 with SimpleTimer() as pass_time:. --> 311 mutated |= check(pss.run_pass, internal_state). 312 with SimpleTimer() as finalize_time:. 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:10784,usability,Close,Close,10784,"mpiler_state). 271 . 272 def check(func, compiler_state):. --> 273 mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11541,usability,Stop,StopIteration,11541," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11579,usability,Stop,StopIteration,11579," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1756:11686,usability,error,errors,11686," mangled = func(compiler_state). 274 if mangled not in (True, False):. 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state). 392 lower = lowering.Lower(targetctx, library, fndesc, interp,. 393 metadata=metadata). --> 394 lower.lower(). 395 if not flags.no_cpython_wrapper:. 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self). 166 if self.generator_info is None:. 167 self.genlower = None. --> 168 self.lower_normal_function(self.fndesc). 169 else:. 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc). 220 # Init argument values. 221 self.extract_function_arguments(). --> 222 entry_block_tail = self.lower_function_body(). 223 . 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self). 249 bb = self.blkmap[offset]. 250 self.builder.position_at_end(bb). --> 251 self.lower_block(block). 252 self.post_lower(). 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block). 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,. 264 loc=self.loc, errcls_=defaulterrcls):. --> 265 self.lower_inst(inst). 266 self.post_block(block). 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback). 135 value = typ(). 136 try:. --> 137 self.gen.throw(typ, value, traceback). 138 except StopIteration as exc:. 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs). 835 else:. 836 tb = None. --> 837 raise newerr.with_traceback(tb). 838 elif use_new_style_errors():. 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756
https://github.com/scverse/scanpy/issues/1757:1099,availability,avail,available,1099,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:505,deployability,api,api,505,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:505,integrability,api,api,505,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1012,integrability,wrap,wrapping,1012,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:505,interoperability,api,api,505,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:422,modifiability,scal,scaling,422,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:989,reliability,doe,doesn,989,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1099,reliability,availab,available,1099,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1099,safety,avail,available,1099,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1099,security,availab,available,1099,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python. normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None). Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable. normalization_axis: Literal[""var"", ""group""] (default: ""var""). If a `normalization` is passed, which dimension of the data to normalize along. ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:427,deployability,scale,scaled,427,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:427,energy efficiency,scale,scaled,427,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:162,modifiability,concern,concerns,162,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:427,modifiability,scal,scaled,427,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:96,performance,time,time,96,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:427,performance,scale,scaled,427,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:226,safety,safe,safer,226,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:162,testability,concern,concerns,162,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:300,usability,user,user,300,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:48,modifiability,scal,scaling,48,"Btw I think the color map contributes to minmax scaling being more misleading in the upper dotplot, i.e. white color gives the impression that the gene is not expressed. So I wonder if it's equally misleading if a divergent color palette is used (e.g. RdBu_r)... Just a thought :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:711,deployability,log,log-norm,711,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1549,deployability,scale,scale,1549,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1359,energy efficiency,current,currently,1359,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1549,energy efficiency,scale,scale,1549,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:293,interoperability,standard,standardisation,293,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:900,interoperability,standard,standardisation,900,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:62,modifiability,scal,scaling,62,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1216,modifiability,variab,variable,1216,"y more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethso",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1549,modifiability,scal,scale,1549,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1594,modifiability,variab,variability,1594,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1810,modifiability,variab,variability,1810,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1844,modifiability,scal,scaling,1844,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1549,performance,scale,scale,1549,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1922,performance,time,time,1922,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1776,reliability,doe,does,1776,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:2246,reliability,pra,practice,2246,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:711,safety,log,log-norm,711,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:711,security,log,log-norm,711,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:711,testability,log,log-norm,711,"A few comments from my endeavors:. - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes diff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1243,usability,user,user-images,1243,"than for a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best pra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1443,usability,user,user-images,1443,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:1624,usability,user,user-images,1624,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:2107,usability,user,user-images,2107,"or a lowly expressed one. - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher. - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe). - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:. - [0,1] on groups - seems very variable. ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png). - no normalisation (currently only other option) - bad for multiple marker comparison. ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png). - max_abs scale on groups - probably still exaggerates variability. ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png). - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups. ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:458,availability,avail,available,458,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:335,integrability,transform,transformation,335,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:335,interoperability,transform,transformation,335,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:458,reliability,availab,available,458,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:97,safety,compl,complicated,97,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:458,safety,avail,available,458,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:97,security,compl,complicated,97,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:458,security,availab,available,458,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:741,usability,user,user,741,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots. * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once  not per group. * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code. * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:255,deployability,scale,scale,255,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:255,energy efficiency,scale,scale,255,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:62,modifiability,scal,scaling,62,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:255,modifiability,scal,scale,255,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:255,performance,scale,scale,255,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:73,testability,plan,plans,73,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1757:281,usability,user,user-images,281,@ivirshup @gokceneraslan any tutorial on how to apply z-score scaling or plans to add this feature in pl.dotplot? I believe this is the default with Seurat and it would be great to have such option for cases where the genes have very different expression scale. . ![image](https://user-images.githubusercontent.com/9028967/187627865-ab2b7b83-7ec4-4bd1-a860-91040d4934d1.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757
https://github.com/scverse/scanpy/issues/1758:51,deployability,updat,updating,51,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:77,deployability,version,version,77,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:129,deployability,releas,release,129,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:137,deployability,build,builds,137,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:77,integrability,version,version,77,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:77,modifiability,version,version,77,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:153,reliability,doe,doesn,153,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:51,safety,updat,updating,51,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:51,security,updat,updating,51,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:191,usability,confirm,confirm,191,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:426,availability,error,error,426,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:401,deployability,fail,failed,401,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:458,deployability,updat,update,458,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:82,energy efficiency,current,currently,82,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:426,performance,error,error,426,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:401,reliability,fail,failed,401,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:426,safety,error,error,426,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:458,safety,updat,update,458,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:458,security,updat,update,458,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:426,usability,error,error,426,```. sc.__version__. '1.8.0.dev78+gc488909a'. ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. . In other words if adata.raw was missing the notation it failed for me (different error though). . I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:307,modifiability,variab,variables,307,"> What I noticed was that if I didn't have the same ID columns in my adata.var when setting adata.raw I couldn't use gene_symbols. Ah, if you're using the values in `raw` for differential expression the column used for `gene_symbols` should be in `raw.var` this is because `raw` can have a different set of variables than the main object. So this case, at least, is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:486,deployability,modul,module,486,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1357,deployability,log,log,1357,"---------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1782,deployability,log,log,1782,"return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2163,deployability,log,log,2163,"ixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2574,deployability,log,log,2574,"in, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:3809,deployability,Contain,Contains,3809,"--> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.Chr5.382', 'KY.Chr7.503', 'KY.Chr7.846', 'KY.Chr8.1168', 'KY.Chr8.473', 'KY.Chr9.813', 'KY.Chr9.912']' in columns of `adata.obs` or in adata.var['Uniq_Name']."". ```. The KeyError is odd. `adata.var['Uniq_Name']` Contains the actual gene identifiers. The ones in `KeyError` is in `adata.var_names`. So it looks like it searches for the var_names in Uniq_Name for some reason. I pulled from this repo just now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:486,modifiability,modul,module,486,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1501,modifiability,layer,layer,1501,"c.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1910,modifiability,layer,layer,1910,"scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_sy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2291,modifiability,layer,layer,2291,"atrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2595,modifiability,layer,layer,2595,"m, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.C",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2764,modifiability,layer,layer,2764,"upby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.Chr5.382', 'KY.Chr7.503', 'KY.Chr7.846', 'KY.Chr8.1168', 'KY.Chr8.473', 'KY.Chr9.813', 'KY.Chr9.912']' in columns of `adata.obs` or in adata.var['Uniq_Name']."". ```. The ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2770,modifiability,layer,layer,2770,"use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.Chr5.382', 'KY.Chr7.503', 'KY.Chr7.846', 'KY.Chr8.1168', 'KY.Chr8.473', 'KY.Chr9.813', 'KY.Chr9.912']' in columns of `adata.obs` or in adata.var['Uniq_Name']."". ```. The KeyErr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2899,modifiability,layer,layer,2899,"tation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.Chr5.382', 'KY.Chr7.503', 'KY.Chr7.846', 'KY.Chr8.1168', 'KY.Chr8.473', 'KY.Chr9.813', 'KY.Chr9.912']' in columns of `adata.obs` or in adata.var['Uniq_Name']."". ```. The KeyError is odd. `adata.var['Uniq_Name']` Contains the actual gene identifiers. The ones in `KeyError` is in `adata.var_names`. So it l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:82,reliability,doe,does,82,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:215,safety,test,test,215,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:459,safety,input,input-,459,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:486,safety,modul,module,486,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1357,safety,log,log,1357,"---------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1782,safety,log,log,1782,"return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2163,safety,log,log,2163,"ixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2574,safety,log,log,2574,"in, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1357,security,log,log,1357,"---------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1782,security,log,log,1782,"return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2163,security,log,log,2163,"ixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2574,security,log,log,2574,"in, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:3834,security,ident,identifiers,3834,"--> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246', 'KY.Chr4.468', 'KY.Chr5.382', 'KY.Chr7.503', 'KY.Chr7.846', 'KY.Chr8.1168', 'KY.Chr8.473', 'KY.Chr9.813', 'KY.Chr9.912']' in columns of `adata.obs` or in adata.var['Uniq_Name']."". ```. The KeyError is odd. `adata.var['Uniq_Name']` Contains the actual gene identifiers. The ones in `KeyError` is in `adata.var_names`. So it looks like it searches for the var_names in Uniq_Name for some reason. I pulled from this repo just now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:215,testability,test,test,215,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:415,testability,Trace,Traceback,415,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1357,testability,log,log,1357,"---------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:1782,testability,log,log,1782,"return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2163,testability,log,log,2163,"ixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:2574,testability,log,log,2574,"in, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110 ):. --> 111 BasePlot.__init__(. 112 self,. 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds). 109 self._update_var_groups(). 110 . --> 111 self.categories, self.obs_tidy = _prepare_dataframe(. 112 adata,. 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1864 groupby.remove(groupby_index). 1865 keys = list(groupby) + list(np.unique(var_names)). -> 1866 obs_tidy = get.obs_df(. 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols. 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 270 alias_index = None. 271 . --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.246'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:459,usability,input,input-,459,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data. ```. sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'). ```. ```. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-41-3b246f8b6bcc> in <module>. ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 823 """""". 824 . --> 825 return _rank_genes_groups_plot(. 826 adata,. 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds). 448 from .._matrixplot import matrixplot. 449 . --> 450 _pl = matrixplot(. 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds. 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds). 345 """""". 346 . --> 347 mp = MatrixPlot(. 348 adata,. 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds). 109 **kwds,. 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1758:57,interoperability,share,sharey,57,"Surprisingly `sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False, gene_symbols='Uniq_Name')` works just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758
https://github.com/scverse/scanpy/issues/1759:28,deployability,version,version,28,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:56,deployability,instal,installed,56,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:28,integrability,version,version,28,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:28,modifiability,version,version,28,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:42,security,session,sessioninfo,42,"Sorry,I forget to mention it , this is my sessioninfo:. ![image](https://user-images.githubusercontent.com/59059267/112479150-b6c3a380-8daf-11eb-9f4d-f073c19f7b42.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:73,usability,user,user-images,73,"Sorry,I forget to mention it , this is my sessioninfo:. ![image](https://user-images.githubusercontent.com/59059267/112479150-b6c3a380-8daf-11eb-9f4d-f073c19f7b42.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:32,deployability,automat,automatically,32,"Hey, I found the issue. AnnData automatically casts your input data from a float64 to a float32 format. If you generate it using:. `adata = sc.AnnData(X, dtype='float64')`. your results will be reproducible",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:96,interoperability,format,format,96,"Hey, I found the issue. AnnData automatically casts your input data from a float64 to a float32 format. If you generate it using:. `adata = sc.AnnData(X, dtype='float64')`. your results will be reproducible",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:57,safety,input,input,57,"Hey, I found the issue. AnnData automatically casts your input data from a float64 to a float32 format. If you generate it using:. `adata = sc.AnnData(X, dtype='float64')`. your results will be reproducible",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:32,testability,automat,automatically,32,"Hey, I found the issue. AnnData automatically casts your input data from a float64 to a float32 format. If you generate it using:. `adata = sc.AnnData(X, dtype='float64')`. your results will be reproducible",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1759:57,usability,input,input,57,"Hey, I found the issue. AnnData automatically casts your input data from a float64 to a float32 format. If you generate it using:. `adata = sc.AnnData(X, dtype='float64')`. your results will be reproducible",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759
https://github.com/scverse/scanpy/issues/1760:680,energy efficiency,Load,Loading,680,"Ah, I think I see what you're asking now. At the moment, I don't think we have a function for that. But this should be fairly straightforward to work around. Something like this should work:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from sklearn.metrics import pairwise_distances. import seaborn as sns. def groupby_mean(adata, groupby):. grouped = adata.obs.groupby(groupby). results = np.zeros((grouped.ngroups, adata.n_vars), dtype=np.float64). for idx, indices in enumerate(grouped.indices.values()):. results[idx] = np.ravel(adata.X[indices].mean(axis=0)). return pd.DataFrame(results, columns=adata.var_names, index=grouped.groups.keys()). # Loading data. pbmc_full = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc_small = sc.datasets.pbmc68k_reduced().raw.to_adata(). var_intersect = pbmc_full.var_names.intersection(pbmc_small.var_names). # Calculate mean expression per cell type. full_means = groupby_mean(pbmc_full[:, var_intersect], ""louvain""). small_means = groupby_mean(pbmc_small[:, var_intersect], ""louvain""). # Correlation distance between celltypes. corr_mtx = pd.DataFrame(. pairwise_distances(full_means, small_means, metric=""correlation""),. index= full_means.index,. columns=small_means.index,. ). ```. Is this more of what you were thinking?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760
https://github.com/scverse/scanpy/issues/1760:680,performance,Load,Loading,680,"Ah, I think I see what you're asking now. At the moment, I don't think we have a function for that. But this should be fairly straightforward to work around. Something like this should work:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from sklearn.metrics import pairwise_distances. import seaborn as sns. def groupby_mean(adata, groupby):. grouped = adata.obs.groupby(groupby). results = np.zeros((grouped.ngroups, adata.n_vars), dtype=np.float64). for idx, indices in enumerate(grouped.indices.values()):. results[idx] = np.ravel(adata.X[indices].mean(axis=0)). return pd.DataFrame(results, columns=adata.var_names, index=grouped.groups.keys()). # Loading data. pbmc_full = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc_small = sc.datasets.pbmc68k_reduced().raw.to_adata(). var_intersect = pbmc_full.var_names.intersection(pbmc_small.var_names). # Calculate mean expression per cell type. full_means = groupby_mean(pbmc_full[:, var_intersect], ""louvain""). small_means = groupby_mean(pbmc_small[:, var_intersect], ""louvain""). # Correlation distance between celltypes. corr_mtx = pd.DataFrame(. pairwise_distances(full_means, small_means, metric=""correlation""),. index= full_means.index,. columns=small_means.index,. ). ```. Is this more of what you were thinking?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760
https://github.com/scverse/scanpy/issues/1760:39,energy efficiency,heat,heatmap,39,"Exactly, could you add how to plot the heatmap of corr_mtx? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760
https://github.com/scverse/scanpy/issues/1761:174,deployability,observ,observations,174,"You can transpose the anndata object with `adata.T` or `adata.transpose()`. The issue is the difference in convention between `R` defaulting to fortran order arrays (putting observations on the columns), while python defaults to `C` order arrays (observations on rows).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761
https://github.com/scverse/scanpy/issues/1761:247,deployability,observ,observations,247,"You can transpose the anndata object with `adata.T` or `adata.transpose()`. The issue is the difference in convention between `R` defaulting to fortran order arrays (putting observations on the columns), while python defaults to `C` order arrays (observations on rows).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761
https://github.com/scverse/scanpy/issues/1761:174,testability,observ,observations,174,"You can transpose the anndata object with `adata.T` or `adata.transpose()`. The issue is the difference in convention between `R` defaulting to fortran order arrays (putting observations on the columns), while python defaults to `C` order arrays (observations on rows).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761
https://github.com/scverse/scanpy/issues/1761:247,testability,observ,observations,247,"You can transpose the anndata object with `adata.T` or `adata.transpose()`. The issue is the difference in convention between `R` defaulting to fortran order arrays (putting observations on the columns), while python defaults to `C` order arrays (observations on rows).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761
https://github.com/scverse/scanpy/issues/1762:32,modifiability,extens,extensively,32,"So, that dataset is used pretty extensively in the tests, and especially around plotting (plus it's actually shipped with the library). I don't think we're likely to modify it, given that it's used so heavily as a reference. What do you need it for, and could you use `pbmc3k_processed` for that purpose?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:51,safety,test,tests,51,"So, that dataset is used pretty extensively in the tests, and especially around plotting (plus it's actually shipped with the library). I don't think we're likely to modify it, given that it's used so heavily as a reference. What do you need it for, and could you use `pbmc3k_processed` for that purpose?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:166,security,modif,modify,166,"So, that dataset is used pretty extensively in the tests, and especially around plotting (plus it's actually shipped with the library). I don't think we're likely to modify it, given that it's used so heavily as a reference. What do you need it for, and could you use `pbmc3k_processed` for that purpose?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:51,testability,test,tests,51,"So, that dataset is used pretty extensively in the tests, and especially around plotting (plus it's actually shipped with the library). I don't think we're likely to modify it, given that it's used so heavily as a reference. What do you need it for, and could you use `pbmc3k_processed` for that purpose?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,deployability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,integrability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,interoperability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,modifiability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,reliability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,security,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:18,testability,integr,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:203,energy efficiency,current,current,203,"I'm not sure if we're going to be able to find a full, analyzed copy of that dataset (I personally do not have a copy). It also would have been analyzed a few years ago, so would not have been following current best practice. If you just want the full transcriptome, I believe this is the raw data: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:216,reliability,pra,practice,216,"I'm not sure if we're going to be able to find a full, analyzed copy of that dataset (I personally do not have a copy). It also would have been analyzed a few years ago, so would not have been following current best practice. If you just want the full transcriptome, I believe this is the raw data: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:88,usability,person,personally,88,"I'm not sure if we're going to be able to find a full, analyzed copy of that dataset (I personally do not have a copy). It also would have been analyzed a few years ago, so would not have been following current best practice. If you just want the full transcriptome, I believe this is the raw data: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1762:307,usability,support,support,307,"I'm not sure if we're going to be able to find a full, analyzed copy of that dataset (I personally do not have a copy). It also would have been analyzed a few years ago, so would not have been following current best practice. If you just want the full transcriptome, I believe this is the raw data: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762
https://github.com/scverse/scanpy/issues/1764:232,performance,perform,performing,232,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells? Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764
https://github.com/scverse/scanpy/issues/1764:255,safety,test,tests,255,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells? Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764
https://github.com/scverse/scanpy/issues/1764:255,testability,test,tests,255,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells? Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764
https://github.com/scverse/scanpy/issues/1764:232,usability,perform,performing,232,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells? Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764
https://github.com/scverse/scanpy/issues/1764:252,performance,throughput,throughput,252,"We knocked out a gene, then wanna to reveal the difference between KO embro cells and wild type embryo cells and illustrate the function of the gene in development process. The single cell data was generated using Smart-seq2 technology, which is a low throughput technology. And besides, there were only a few cells in eary embros because they are very small.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764
https://github.com/scverse/scanpy/pull/1765:347,availability,down,downstream,347,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:194,modifiability,concern,concern,194,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:505,modifiability,variab,variable,505,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:562,modifiability,variab,variable,562,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:645,reliability,pra,practice,645,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:394,security,ident,identified,394,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:34,testability,simpl,simple,34,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:194,testability,concern,concern,194,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:34,usability,simpl,simple,34,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:662,usability,paus,pause,662,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1765:711,usability,user,user-images,711,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this. * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause? <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765
https://github.com/scverse/scanpy/pull/1767:39,usability,tool,tools,39,"Thanks for the PR! I think ""functional tools"" may be a little generic/ ambiguous with the programming concept. Is there another term that could be used here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1767:61,reliability,doe,does,61,"I changed the name to ""Footprint-based enrichment analysis"", does it fit better?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1767:126,availability,down,downstream,126,"It is an enrichment analysis but foot-print based: we don't just look at the elements of a pathway/TF but also the biological downstream effects that occur when said biological process is active. . ""Annotation/ Enrichment Analysis"" fits but it would be good to also mention somewhere that they are foot-print based. Would this be okay?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1767:101,usability,help,helpful,101,"Would it be sufficient to put the ""footprint-based"" in the function description? I guess it would be helpful to have a category of methods that will be easily recognizable to people if they're looking for a tool. That would also mean having a category that tools from multiple labs could fall under.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1767:207,usability,tool,tool,207,"Would it be sufficient to put the ""footprint-based"" in the function description? I guess it would be helpful to have a category of methods that will be easily recognizable to people if they're looking for a tool. That would also mean having a category that tools from multiple labs could fall under.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1767:257,usability,tool,tools,257,"Would it be sufficient to put the ""footprint-based"" in the function description? I guess it would be helpful to have a category of methods that will be easily recognizable to people if they're looking for a tool. That would also mean having a category that tools from multiple labs could fall under.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767
https://github.com/scverse/scanpy/pull/1771:64,availability,failur,failure,64,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:64,deployability,fail,failure,64,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:64,performance,failur,failure,64,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:64,reliability,fail,failure,64,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:147,reliability,doe,does,147,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:34,safety,test,test,34,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1771:34,testability,test,test,34,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771
https://github.com/scverse/scanpy/pull/1772:49,deployability,API,API,49,"This is already useful without the final fixture API design, so lets just merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1772
https://github.com/scverse/scanpy/pull/1772:49,integrability,API,API,49,"This is already useful without the final fixture API design, so lets just merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1772
https://github.com/scverse/scanpy/pull/1772:49,interoperability,API,API,49,"This is already useful without the final fixture API design, so lets just merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1772
https://github.com/scverse/scanpy/pull/1775:198,deployability,API,API,198,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:245,deployability,API,API,245,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:198,integrability,API,API,198,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:245,integrability,API,API,245,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:198,interoperability,API,API,198,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:245,interoperability,API,API,245,"@jacobkimmel thanks for the PR and the kind words! Question: why do you want this to be in `external` and not `ecosystem`? Generally, I think of `external` as a place to put provide a ""scanpy like"" API, but `scNym` already provides this kind of API.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:174,deployability,API,API,174,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:219,deployability,API,API,219,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:349,deployability,API,API,349,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:174,integrability,API,API,174,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:219,integrability,API,API,219,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:349,integrability,API,API,349,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:394,integrability,endpoint,endpoint,394,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:444,integrability,discover,discoverability,444,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:174,interoperability,API,API,174,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:219,interoperability,API,API,219,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:349,interoperability,API,API,349,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:444,interoperability,discover,discoverability,444,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:570,security,team,team,570,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:31,usability,feedback,feedback,31,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:444,usability,discov,discoverability,444,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:464,usability,user,users,464,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:552,usability,prefer,preferable,552,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it. 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,. Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:199,integrability,discover,discoverable,199,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:199,interoperability,discover,discoverable,199,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:34,security,team,team,34,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:199,usability,discov,discoverable,199,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:346,usability,help,helpful,346,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:366,usability,user,users,366,"So, we discussed this at the last team meeting and are in agreement that this would be a better fit for the Ecosystem page. We're going to try to increase the visibility of this page to make it more discoverable (#1801). For the entry, I think including a link to a tutorial as well as the main project page under the entry in ecosystem would be helpful for getting users to check it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:177,performance,time,time,177,"> this would be a better fit for the Ecosystem page. Understood, I'll go ahead and close this PR and open a clean one that modifies `ecosystem.rst` alone. Thanks for taking the time to find the right place for this! Best,. jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:123,security,modif,modifies,123,"> this would be a better fit for the Ecosystem page. Understood, I'll go ahead and close this PR and open a clean one that modifies `ecosystem.rst` alone. Thanks for taking the time to find the right place for this! Best,. jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/pull/1775:83,usability,close,close,83,"> this would be a better fit for the Ecosystem page. Understood, I'll go ahead and close this PR and open a clean one that modifies `ecosystem.rst` alone. Thanks for taking the time to find the right place for this! Best,. jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775
https://github.com/scverse/scanpy/issues/1776:146,interoperability,format,format,146,"The Flit PR says that support is experimental, so maybe we should wait until its no longer marked as such? Flit itself switched to that metadata format though and we should be able to verify if the packages are created correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776
https://github.com/scverse/scanpy/issues/1776:199,modifiability,pac,packages,199,"The Flit PR says that support is experimental, so maybe we should wait until its no longer marked as such? Flit itself switched to that metadata format though and we should be able to verify if the packages are created correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776
https://github.com/scverse/scanpy/issues/1776:185,testability,verif,verify,185,"The Flit PR says that support is experimental, so maybe we should wait until its no longer marked as such? Flit itself switched to that metadata format though and we should be able to verify if the packages are created correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776
https://github.com/scverse/scanpy/issues/1776:22,usability,support,support,22,"The Flit PR says that support is experimental, so maybe we should wait until its no longer marked as such? Flit itself switched to that metadata format though and we should be able to verify if the packages are created correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776
https://github.com/scverse/scanpy/issues/1776:49,deployability,instal,installs,49,"Also, consider setting `flit >=3.4` for editable installs via [PEP 660](https://www.python.org/dev/peps/pep-0660/) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776
https://github.com/scverse/scanpy/issues/1778:317,deployability,log,logfoldchanges,317,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:346,deployability,log,logfoldchanges,346,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:384,deployability,log,logfoldchanges,384,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:414,deployability,log,logfoldchanges,414,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:468,deployability,log,logfoldchanges,468,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:508,deployability,log,logfoldchanges,508,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:531,deployability,log,logfoldchanges,531,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:578,deployability,log,logfoldchanges,578,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:601,deployability,log,logfoldchanges,601,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:793,deployability,log,logy,793,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:317,safety,log,logfoldchanges,317,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:346,safety,log,logfoldchanges,346,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:384,safety,log,logfoldchanges,384,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:414,safety,log,logfoldchanges,414,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:468,safety,log,logfoldchanges,468,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:508,safety,log,logfoldchanges,508,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:531,safety,log,logfoldchanges,531,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:578,safety,log,logfoldchanges,578,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:601,safety,log,logfoldchanges,601,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:793,safety,log,logy,793,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:317,security,log,logfoldchanges,317,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:346,security,log,logfoldchanges,346,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:384,security,log,logfoldchanges,384,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:414,security,log,logfoldchanges,414,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:468,security,log,logfoldchanges,468,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:508,security,log,logfoldchanges,508,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:531,security,log,logfoldchanges,531,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:578,security,log,logfoldchanges,578,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:601,security,log,logfoldchanges,601,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:793,security,log,logy,793,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:317,testability,log,logfoldchanges,317,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:346,testability,log,logfoldchanges,346,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:384,testability,log,logfoldchanges,384,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:414,testability,log,logfoldchanges,414,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:468,testability,log,logfoldchanges,468,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:508,testability,log,logfoldchanges,508,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:531,testability,log,logfoldchanges,531,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:578,testability,log,logfoldchanges,578,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:601,testability,log,logfoldchanges,601,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:793,testability,log,logy,793,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot? I've used something like this snipped (using `hvplots`) for these:. ```python. import hvplot.pandas. def plot_volcano(dedf):. dedf = dedf.copy(). dedf = dedf[dedf[""pvals""].notnull()]. dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10. dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10. return dedf.hvplot.scatter(. ""logfoldchanges"",. ""pvals"",. xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),. ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),. hover_cols=list(dedf.columns),. logy=True,. flip_yaxis=True. ). plot_volcano(sc.get.rank_genes_groups(adata, ...)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:75,reliability,doe,does,75,"Hello,. I actually wanted to know that in the plot that I have posted what does the numbers on right hand side signify. Like 5, 3, 3,.. and so on. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:111,security,sign,signify,111,"Hello,. I actually wanted to know that in the plot that I have posted what does the numbers on right hand side signify. Like 5, 3, 3,.. and so on. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/issues/1778:77,interoperability,format,formats,77,"Ah, I believe that should just be expression level per row. It looks like it formats pretty poorly due to the small area and fantasize though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778
https://github.com/scverse/scanpy/pull/1780:177,deployability,API,API,177,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:259,deployability,API,API,259,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:428,deployability,log,log,428,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:484,deployability,releas,release,484,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:177,integrability,API,API,177,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:259,integrability,API,API,259,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:177,interoperability,API,API,177,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:259,interoperability,API,API,259,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:497,reliability,doe,does,497,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:428,safety,log,log,428,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:428,security,log,log,428,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:428,testability,log,log,428,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:185,usability,tool,tools,185,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:229,usability,tool,tool,229,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:365,usability,user,users,365,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:405,usability,tool,tool,405,"Thanks for opening this PR! Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:632,availability,redund,redundancy,632,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:632,deployability,redundan,redundancy,632,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:782,deployability,releas,releases,782,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:29,energy efficiency,core,core,29,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:35,energy efficiency,reduc,reduce,35,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:377,energy efficiency,reduc,reduce,377,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:462,energy efficiency,current,current,462,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:600,energy efficiency,reduc,reduce,600,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:106,integrability,wrap,wrapper,106,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:351,integrability,wrap,wrapper,351,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:531,integrability,wrap,wrapper,531,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:588,integrability,wrap,wrapper,588,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:669,integrability,interfac,interfaces,669,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:106,interoperability,wrapper,wrapper,106,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:351,interoperability,wrapper,wrapper,351,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:531,interoperability,wrapper,wrapper,531,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:588,interoperability,wrapper,wrapper,588,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:669,interoperability,interfac,interfaces,669,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:669,modifiability,interfac,interfaces,669,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:692,modifiability,pac,package,692,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:632,reliability,redundan,redundancy,632,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:852,reliability,Doe,Does,852,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:412,safety,input,input,412,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:632,safety,redund,redundancy,632,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:261,security,access,access,261,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:99,testability,simpl,simple,99,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:99,usability,simpl,simple,99,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:166,usability,user,users,166,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:252,usability,user,users,252,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:412,usability,input,input,412,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:814,usability,user,user,814,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:393,deployability,manag,management,393,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:743,deployability,releas,release,743,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:106,energy efficiency,reduc,reduce,106,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:393,energy efficiency,manag,management,393,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:81,integrability,wrap,wrapper,81,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:234,integrability,wrap,wrapper,234,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:626,integrability,interfac,interface,626,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:81,interoperability,wrapper,wrapper,81,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:234,interoperability,wrapper,wrapper,234,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:626,interoperability,interfac,interface,626,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:626,modifiability,interfac,interface,626,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:832,modifiability,concern,concern,832,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:140,safety,input,input,140,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:393,safety,manag,management,393,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:776,safety,input,input,776,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:832,testability,concern,concern,832,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:140,usability,input,input,140,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:336,usability,document,documentation,336,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:425,usability,document,documentation,425,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:698,usability,document,documentation,698,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:776,usability,input,input,776,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:863,usability,prefer,prefer,863,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:883,usability,tool,tool,883,"Sorry about the late reply to this! > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:262,availability,down,downloading,262,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:645,availability,mainten,maintenance,645,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1439,availability,mainten,maintenance,1439,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1365,deployability,releas,releases,1365,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1509,deployability,modul,modules,1509,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:158,energy efficiency,current,current,158,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:383,energy efficiency,load,loaded,383,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1074,energy efficiency,current,currently,1074,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:765,integrability,wrap,wrapper,765,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:803,integrability,wrap,wrapper,803,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1798,integrability,repositor,repository,1798,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:548,interoperability,format,format,548,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:765,interoperability,wrapper,wrapper,765,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:803,interoperability,wrapper,wrapper,803,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1043,interoperability,architectur,architecture,1043,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1165,interoperability,specif,specific,1165,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1633,interoperability,format,format,1633,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1798,interoperability,repositor,repository,1798,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1204,modifiability,pac,package,1204,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1509,modifiability,modul,modules,1509,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1721,modifiability,maintain,maintained,1721,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:383,performance,load,loaded,383,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:645,reliability,mainten,maintenance,645,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1439,reliability,mainten,maintenance,1439,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:952,safety,input,inputs,952,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1509,safety,modul,modules,1509,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1721,safety,maintain,maintained,1721,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:933,security,control,controls,933,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:933,testability,control,controls,933,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1617,testability,simpl,simply,1617,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:290,usability,learn,learning,290,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:397,usability,user,user,397,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:534,usability,command,command,534,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:559,usability,user,user,559,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:952,usability,input,inputs,952,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1097,usability,document,documentation,1097,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1301,usability,workflow,workflow,1301,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1336,usability,close,close,1336,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1538,usability,document,documentation,1538,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1568,usability,document,documentation,1568,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1617,usability,simpl,simply,1617,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/pull/1780:1739,usability,tool,tool,1739,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780
https://github.com/scverse/scanpy/issues/1781:174,deployability,releas,release,174,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:81,integrability,wrap,wrapper,81,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:138,integrability,wrap,wrapper,138,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:81,interoperability,wrapper,wrapper,81,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:138,interoperability,wrapper,wrapper,138,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:124,modifiability,pac,package,124,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:39,usability,tool,tools,39,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:60,usability,tool,tools,60,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:89,usability,support,supports,89,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:209,usability,tool,tools,209,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:10,deployability,API,API,10,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:10,integrability,API,API,10,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:55,integrability,wrap,wrapper,55,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:10,interoperability,API,API,10,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1781:55,interoperability,wrapper,wrapper,55,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781
https://github.com/scverse/scanpy/issues/1782:179,availability,error,error,179,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:15,deployability,releas,release,15,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:206,deployability,releas,release,206,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:256,deployability,fail,failing,256,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:7,energy efficiency,current,current,7,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:179,performance,error,error,179,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:256,reliability,fail,failing,256,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:179,safety,error,error,179,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:179,usability,error,error,179,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1782:46,deployability,releas,released,46,"Should be fixed as of `1.7.2`, which was just released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782
https://github.com/scverse/scanpy/issues/1791:151,deployability,version,version,151,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:151,integrability,version,version,151,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:151,modifiability,version,version,151,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:335,testability,simpl,simply,335,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:61,usability,help,help,61,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:335,usability,simpl,simply,335,"Thanks very much for your reply. . Yes, I am aware dpi can help increase the resolution. However, I noticed that with the same dpi , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing dpi will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:159,deployability,observ,observing,159,I'm not able to see any obvious difference on my machine. Maybe you could produce a side by side example? It would be good to make sure any differences you're observing aren't coming from different versions of matplotlib or seaborn.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:198,deployability,version,versions,198,I'm not able to see any obvious difference on my machine. Maybe you could produce a side by side example? It would be good to make sure any differences you're observing aren't coming from different versions of matplotlib or seaborn.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:198,integrability,version,versions,198,I'm not able to see any obvious difference on my machine. Maybe you could produce a side by side example? It would be good to make sure any differences you're observing aren't coming from different versions of matplotlib or seaborn.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:198,modifiability,version,versions,198,I'm not able to see any obvious difference on my machine. Maybe you could produce a side by side example? It would be good to make sure any differences you're observing aren't coming from different versions of matplotlib or seaborn.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1791:159,testability,observ,observing,159,I'm not able to see any obvious difference on my machine. Maybe you could produce a side by side example? It would be good to make sure any differences you're observing aren't coming from different versions of matplotlib or seaborn.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791
https://github.com/scverse/scanpy/issues/1793:0,deployability,Resourc,Resources,0,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:0,energy efficiency,Resourc,Resources,0,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,energy efficiency,gpu,gpus,135,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:0,performance,Resourc,Resources,0,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,performance,gpu,gpus,135,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:0,safety,Resourc,Resources,0,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:0,testability,Resourc,Resources,0,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:19,usability,help,help,19,Resources that may help:. - https://www.youtube.com/watch?v=rVq-SCNyxVc. - https://dvc.org/blog/cml-self-hosted-runners-on-demand-with-gpus,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:116,deployability,deploy,deployed,116,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:213,deployability,updat,updated,213,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:287,deployability,version,versions,287,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:24,energy efficiency,gpu,gpu,24,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:199,energy efficiency,core,core,199,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:287,integrability,version,versions,287,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:86,modifiability,pac,packages,86,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:287,modifiability,version,versions,287,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:24,performance,gpu,gpu,24,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:213,safety,updat,updated,213,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:213,security,updat,updated,213,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:13,usability,help,helps,13,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:114,availability,servic,service,114,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:114,deployability,servic,service,114,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:683,deployability,log,logs,683,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:43,energy efficiency,GPU,GPU,43,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:201,energy efficiency,GPU,GPUs,201,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:297,energy efficiency,GPU,GPU,297,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:452,energy efficiency,cloud,cloud,452,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:594,energy efficiency,cloud,cloud,594,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:841,energy efficiency,cloud,cloud,841,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:114,integrability,servic,service,114,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:712,integrability,interfac,interface,712,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:82,interoperability,share,share,82,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:712,interoperability,interfac,interface,712,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:114,modifiability,servic,service,114,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:712,modifiability,interfac,interface,712,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:43,performance,GPU,GPU,43,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:201,performance,GPU,GPUs,201,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:297,performance,GPU,GPU,297,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:683,safety,log,logs,683,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:683,security,log,logs,683,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:417,testability,simpl,simple,417,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:485,testability,simpl,simple,485,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:683,testability,log,logs,683,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:174,usability,custom,custom,174,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:301,usability,support,support,301,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:417,usability,simpl,simple,417,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:485,usability,simpl,simple,485,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/. https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:52,usability,support,support,52,"Hi @aktech,. thank you for your comment. I only see support for AWS, GCP and DO. For us it would be much easier to stick with our Azure setup (payments etc much easier). Is there a way to use it with Azure?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,availability,avail,available,135,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,reliability,availab,available,135,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,safety,avail,available,135,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:135,security,availab,available,135,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:77,usability,support,support,77,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:98,usability,progress,progress,98,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:145,availability,avail,available,145,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:145,reliability,availab,available,145,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:145,safety,avail,available,145,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:145,security,availab,available,145,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:87,usability,support,support,87,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:108,usability,progress,progress,108,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:298,usability,close,closely,298,"> . > . > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this? No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:52,deployability,updat,update,52,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:100,energy efficiency,GPU,GPU-accelerated,100,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:100,performance,GPU,GPU-accelerated,100,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:52,safety,updat,update,52,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:52,security,updat,update,52,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:130,testability,understand,understand,130,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:45,usability,statu,status,45,"Hi @Zethson,. I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:132,availability,ping,ping,132,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:121,deployability,log,login,121,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:162,interoperability,convers,conversation,162,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:121,safety,log,login,121,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:121,security,log,login,121,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:121,testability,log,login,121,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:84,availability,avail,available,84,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:71,energy efficiency,GPU,GPU,71,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:75,integrability,wrap,wrappers,75,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:75,interoperability,wrapper,wrappers,75,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:29,performance,time,time,29,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:71,performance,GPU,GPU,71,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:84,reliability,availab,available,84,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:84,safety,avail,available,84,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:84,security,availab,available,84,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:. https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1793:48,usability,help,helpful,48,"Thank you very much, @LouisFaure. This is super helpful!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793
https://github.com/scverse/scanpy/issues/1795:75,availability,error,error,75,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:81,integrability,messag,message,81,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:81,interoperability,messag,message,81,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:75,performance,error,error,75,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:75,safety,error,error,75,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:75,usability,error,error,75,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:86,availability,error,error,86,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:92,integrability,messag,message,92,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:92,interoperability,messag,message,92,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:86,performance,error,error,86,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:130,reliability,doe,does,130,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:86,safety,error,error,86,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:86,usability,error,error,86,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:115,usability,command,command,115,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:63,availability,error,error,63,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:69,integrability,messag,message,69,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:69,interoperability,messag,message,69,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:27,modifiability,variab,variable,27,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:12,performance,content,content,12,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:63,performance,error,error,63,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:63,safety,error,error,63,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:63,usability,error,error,63,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:226,availability,error,error,226,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:232,integrability,messag,message,232,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:42,interoperability,format,format,42,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:232,interoperability,messag,message,232,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:4,modifiability,variab,variable,4,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:118,performance,time,time,118,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:226,performance,error,error,226,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:52,safety,input,input,52,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:226,safety,error,error,226,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:52,usability,input,input,52,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:134,usability,command,command,134,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:169,usability,command,commands,169,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:226,usability,error,error,226,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:24,performance,time,time,24,"After spending a lot of time, I find out that the directory was changed somehow and the object was being saved in some other folder. So I changed the directory again to the destinated folder where I want to save these objects. But I would like to thank you for the help. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:265,usability,help,help,265,"After spending a lot of time, I find out that the directory was changed somehow and the object was being saved in some other folder. So I changed the directory again to the destinated folder where I want to save these objects. But I would like to thank you for the help. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:209,availability,error,error,209,"> I find out the solution. Thank You. Although I think that this is not an issue with Scanpy, it is usually common courtesy to post the solution to the corresponding issue. If other people search for the same error they can find your solution :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:209,performance,error,error,209,"> I find out the solution. Thank You. Although I think that this is not an issue with Scanpy, it is usually common courtesy to post the solution to the corresponding issue. If other people search for the same error they can find your solution :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:209,safety,error,error,209,"> I find out the solution. Thank You. Although I think that this is not an issue with Scanpy, it is usually common courtesy to post the solution to the corresponding issue. If other people search for the same error they can find your solution :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1795:209,usability,error,error,209,"> I find out the solution. Thank You. Although I think that this is not an issue with Scanpy, it is usually common courtesy to post the solution to the corresponding issue. If other people search for the same error they can find your solution :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795
https://github.com/scverse/scanpy/issues/1798:83,deployability,log,log-normalized,83,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:313,integrability,filter,filtered,313,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:265,modifiability,layer,layers,265,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:129,reliability,pra,practices,129,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:83,safety,log,log-normalized,83,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:83,security,log,log-normalized,83,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:83,testability,log,log-normalized,83,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:139,usability,workflow,workflow,139,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:182,energy efficiency,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:182,integrability,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:182,interoperability,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:182,modifiability,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:16,usability,user,user,16,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:205,usability,workflow,workflows,205,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:122,integrability,filter,filtered,122,"I actually meant recreate the counts by reloading the data object ;). I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. I haven't come across the need to query these... and we would normally regard this as background noise anyway, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,deployability,depend,depends,659,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:561,energy efficiency,load,load,561,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:389,integrability,pub,publish,389,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,integrability,depend,depends,659,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:678,integrability,filter,filtering,678,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:881,integrability,filter,filtered,881,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:209,interoperability,distribut,distribute,209,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,modifiability,depend,depends,659,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:561,performance,load,load,561,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,safety,depend,depends,659,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:180,testability,simpl,simply,180,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,testability,depend,depends,659,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:180,usability,simpl,simply,180,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:459,usability,user,users,459,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no? This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:857,availability,mask,mask,857,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:896,availability,mask,mask,896,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1284,availability,avail,available,1284,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,deployability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1274,energy efficiency,current,currently,1274,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:413,integrability,filter,filtered,413,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,integrability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:659,integrability,filter,filtering,659,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:804,integrability,sub,subset,804,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1030,integrability,transform,transform,1030,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,interoperability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1030,interoperability,transform,transform,1030,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,modifiability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:984,performance,memor,memory,984,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,reliability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:970,reliability,doe,does,970,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1227,reliability,pra,practically,1227,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1284,reliability,availab,available,1284,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:599,safety,safe,safely,599,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1284,safety,avail,available,1284,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,security,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1284,security,availab,available,1284,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:497,testability,integr,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:334,usability,behavi,behavior,334,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:984,usability,memor,memory,984,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1239,usability,usab,usable,1239,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1096,availability,mask,masks,1096,"file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is qui",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1166,availability,mask,mask,1166,"t case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1209,availability,slo,slower,1209,"raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:316,deployability,version,version,316,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,deployability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,deployability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1471,deployability,pipelin,pipeline,1471,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1564,deployability,contain,container,1564,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1661,deployability,pipelin,pipeline,1661,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2013,deployability,version,version,2013,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2157,deployability,version,version,2157,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1300,energy efficiency,current,currently,1300,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1646,energy efficiency,current,current,1646,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:316,integrability,version,version,316,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,integrability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,integrability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:942,integrability,transform,transform,942,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1064,integrability,sub,subset,1064," can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1471,integrability,pipelin,pipeline,1471,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1661,integrability,pipelin,pipeline,1661,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2013,integrability,version,version,2013,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2157,integrability,version,version,2157,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:104,interoperability,distribut,distribute,104,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:272,interoperability,distribut,distributing,272,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:378,interoperability,distribut,distributing,378,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,interoperability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,interoperability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:942,interoperability,transform,transform,942,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:316,modifiability,version,version,316,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:352,modifiability,layer,layer,352,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,modifiability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,modifiability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1748,modifiability,layer,layer,1748,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1798,modifiability,layer,layer,1798,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2013,modifiability,version,version,2013,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2157,modifiability,version,version,2157,"ne, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:896,performance,memor,memory,896,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,reliability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,reliability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:882,reliability,doe,does,882,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1209,reliability,slo,slower,1209,"raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,security,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,security,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:846,security,ident,identity,846,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:75,testability,simpl,simply,75,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:531,testability,integr,integration,531,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:544,testability,Integr,Integration,544,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:971,testability,understand,understand,971,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:75,usability,simpl,simply,75,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:434,usability,clear,clear,434,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:896,usability,memor,memory,896,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no? > Might be important for integration? Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment? I would be a bit hesitant to not have a replacement for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3426,availability,mask,masks,3426,". . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3496,availability,mask,mask,3496,". . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where jus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4104,availability,mask,masked,4104,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:384,deployability,observ,observations,384,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,deployability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,deployability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1720,deployability,scale,scale,1720,"y allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1956,deployability,scale,scale,1956,"ly. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3600,deployability,scale,scale,3600,". . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3795,deployability,pipelin,pipeline,3795,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3888,deployability,contain,container,3888,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1409,energy efficiency,measur,measured,1409,"on for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1720,energy efficiency,scale,scale,1720,"y allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1956,energy efficiency,scale,scale,1956,"ly. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3600,energy efficiency,scale,scale,3600,". . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,integrability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,integrability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1595,integrability,transform,transform,1595,"There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2078,integrability,sub,subset,2078,"le gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3394,integrability,sub,subset,3394,". . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variable",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3653,integrability,filter,filtered,3653,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3795,integrability,pipelin,pipeline,3795,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4308,integrability,sub,subset,4308,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:120,interoperability,distribut,distribute,120,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,interoperability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,interoperability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1595,interoperability,transform,transform,1595,"There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:285,modifiability,variab,variables,285,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:337,modifiability,variab,variables,337,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,modifiability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,modifiability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1075,modifiability,variab,variable,1075,"t it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1720,modifiability,scal,scale,1720,"y allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1956,modifiability,scal,scale,1956,"ly. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:2006,modifiability,variab,variables,2006,"sets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3600,modifiability,scal,scale,3600,". . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4200,modifiability,layer,layers,4200,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4277,modifiability,variab,variables,4277,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4389,modifiability,variab,variables,4389,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1549,performance,memor,memory,1549," think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1720,performance,scale,scale,1720,"y allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1956,performance,scale,scale,1956,"ly. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:3600,performance,scale,scale,3600,". . . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,reliability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,reliability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1535,reliability,doe,does,1535,"ything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:4232,reliability,doe,doesn,4232,". . . . . ```. While this is one with blocks along the diagonal:. ```. . . . . . . . . . . . . . . ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment? I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:194,safety,compl,completley,194,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:406,safety,detect,detection,406,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:194,security,compl,completley,194,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:406,security,detect,detection,406,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,security,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,security,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:91,testability,simpl,simply,91,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:384,testability,observ,observations,384,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:917,testability,Integr,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1047,testability,integr,integrate,1047,"solutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1626,testability,understand,understand,1626,"alyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:91,usability,simpl,simply,91,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1381,usability,help,helpful,1381,"bservations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1549,usability,memor,memory,1549," think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python. with h5py.File(""analysis.h5"") as f:. processed = ad.read_h5ad(f[""processed""]). raw = ad.read_h5ad(f[""raw""]). ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1917,usability,workflow,workflow,1917,"gration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data. > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```. . . . . . . . . . . . . . . ```. While this is one with blocks along the diagonal:. ```. . . ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:201,availability,mask,masks,201,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:216,availability,operat,operation,216,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:547,availability,mask,masking,547,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:972,integrability,filter,filtered,972,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:267,modifiability,layer,layers,267,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:622,modifiability,variab,variable,622,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:504,safety,test,testing,504,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1480,security,sign,signal,1480,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:121,testability,understand,understand,121,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:504,testability,test,testing,504,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no? > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset? I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:85,availability,mask,masks,85,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:100,availability,operat,operation,100,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:261,availability,mask,masking,261,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:536,availability,mainten,maintenance,536,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:595,deployability,log,logic,595,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:710,integrability,filter,filtered,710,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:150,modifiability,layer,layers,150,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1523,performance,time,time,1523,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:536,reliability,mainten,maintenance,536,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:381,safety,except,except,381,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:595,safety,log,logic,595,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:595,security,log,logic,595,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:1394,security,sign,signal,1394,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:7,testability,understand,understand,7,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:595,testability,log,logic,595,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:498,usability,support,supporting,498,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers? Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no? I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level  but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union? > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:23,availability,mask,mask,23,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:117,availability,cluster,clusters,117,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:126,availability,mask,masked,126,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1798:117,deployability,cluster,clusters,117,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798
https://github.com/scverse/scanpy/issues/1799:47,interoperability,share,share,47,"@dawe thanks for letting us know. Can you also share the corresponding numba (or umap) github issues here, if any? It'd make it easier to keep track.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:199,deployability,version,version,199,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:277,deployability,version,version,277,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:199,integrability,version,version,199,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:277,integrability,version,version,277,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:191,modifiability,pac,package,191,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:199,modifiability,version,version,199,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:277,modifiability,version,version,277,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:170,testability,verif,verifications,170,"Hi, I have opened an issue on the [UMAP tracker](https://github.com/lmcinnes/umap/issues/651#issue-859916330), where it seems everything works fine. I'm waiting for some verifications on the package version (I don't have the computer here right now), it may be related to UMAP version < 0.5, required by `scvelo`, whereas UMAP 0.5.1 may work. I'll keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:17,deployability,version,versions,17,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:68,deployability,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:93,deployability,Version,Versions,93,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:17,integrability,version,versions,17,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:68,integrability,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:93,integrability,Version,Versions,93,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:17,modifiability,version,versions,17,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:68,modifiability,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:93,modifiability,Version,Versions,93,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:68,safety,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:68,testability,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:14,deployability,updat,updated,14,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:64,deployability,version,version,64,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:64,integrability,version,version,64,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:64,modifiability,version,version,64,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:14,safety,updat,updated,14,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:14,security,updat,updated,14,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:180,availability,error,error,180,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:230,availability,error,error,230,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:305,availability,error,error,305,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:312,availability,ERROR,ERROR,312,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:327,availability,error,errored,327,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1905,availability,error,error,1905,"xec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_exte",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1994,availability,ERROR,ERROR,1994,"te_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:42,deployability,updat,updating,42,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:103,deployability,version,version,103,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:262,deployability,Build,Building,262,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:491,deployability,instal,install-,491,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:588,deployability,instal,install-,588,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:974,deployability,instal,install-,974,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1139,deployability,instal,install-,1139,"fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1202,deployability,build,build,1202,":. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1217,deployability,version,version,1217,"ummary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1289,deployability,instal,install-,1289,"tup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1352,deployability,build,build,1352," 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1377,deployability,modul,module,1377,"ko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1409,deployability,instal,install-,1409," -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1472,deployability,build,build,1472,"= '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1545,deployability,instal,install-,1545,"/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1608,deployability,build,build,1608,"te_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1683,deployability,Build,Building,1683,"'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1884,deployability,build,building,1884,"de, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <modul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1967,deployability,fail,failed,1967,"pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2001,deployability,Fail,Failed,2001,"580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2008,deployability,build,building,2008,"46f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_ex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2145,deployability,instal,installation,2145,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2177,deployability,version,version,2177,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2289,deployability,modul,module,2289,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2399,deployability,modul,module,2399,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2437,deployability,log,logging,2437,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2577,deployability,modul,module,2577,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2743,deployability,modul,module,2743,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2883,deployability,modul,module,2883,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2931,deployability,Modul,ModuleNotFoundError,2931,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2955,deployability,modul,module,2955,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2995,deployability,instal,install,2995,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2598,energy efficiency,core,core,2598,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2712,energy efficiency,core,core,2712,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2852,energy efficiency,core,core,2852,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:103,integrability,version,version,103,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,integrability,messag,message,236,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1217,integrability,version,version,1217,"ummary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2177,integrability,version,version,2177,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,interoperability,messag,message,236,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:103,modifiability,version,version,103,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1217,modifiability,version,version,1217,"ummary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1377,modifiability,modul,module,1377,"ko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2177,modifiability,version,version,2177,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2289,modifiability,modul,module,2289,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2357,modifiability,pac,packages,2357,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2399,modifiability,modul,module,2399,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2534,modifiability,pac,packages,2534,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2577,modifiability,modul,module,2577,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2696,modifiability,pac,packages,2696,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2743,modifiability,modul,module,2743,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2836,modifiability,pac,packages,2836,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2883,modifiability,modul,module,2883,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2931,modifiability,Modul,ModuleNotFoundError,2931,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2955,modifiability,modul,module,2955,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:180,performance,error,error,180,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:230,performance,error,error,230,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:305,performance,error,error,305,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:312,performance,ERROR,ERROR,312,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:327,performance,error,errored,327,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1905,performance,error,error,1905,"xec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_exte",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1994,performance,ERROR,ERROR,1994,"te_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1967,reliability,fail,failed,1967,"pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2001,reliability,Fail,Failed,2001,"580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:42,safety,updat,updating,42,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:180,safety,error,error,180,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:230,safety,error,error,230,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:305,safety,error,error,305,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:312,safety,ERROR,ERROR,312,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:327,safety,error,errored,327,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:383,safety,test,test,383,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1035,safety,Compl,Complete,1035,"or updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1099,safety,test,test,1099,"p version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1377,safety,modul,module,1377,"ko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1905,safety,error,error,1905,"xec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_exte",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1935,safety,test,test,1935,"mp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. Module",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1994,safety,ERROR,ERROR,1994,"te_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2289,safety,modul,module,2289,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2318,safety,test,test,2318,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2399,safety,modul,module,2399,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2437,safety,log,logging,2437,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2495,safety,test,test,2495,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2577,safety,modul,module,2577,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2657,safety,test,test,2657,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2743,safety,modul,module,2743,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2797,safety,test,test,2797,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2883,safety,modul,module,2883,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2931,safety,Modul,ModuleNotFoundError,2931,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2955,safety,modul,module,2955,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:42,security,updat,updating,42,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:453,security,token,tokenize,453,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:673,security,token,tokenize,673,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1035,security,Compl,Complete,1035,"or updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2437,security,log,logging,2437,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:383,testability,test,test,383,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1099,testability,test,test,1099,"p version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1238,testability,Trace,Traceback,1238,". </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1935,testability,test,test,1935,"mp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. Module",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2225,testability,Trace,Traceback,2225,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2318,testability,test,test,2318,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2437,testability,log,logging,2437,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2495,testability,test,test,2495,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2657,testability,test,test,2657,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:2797,testability,test,test,2797,"st/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:180,usability,error,error,180,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:230,usability,error,error,230,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:305,usability,error,error,305,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:312,usability,ERROR,ERROR,312,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:319,usability,Command,Command,319,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:327,usability,error,errored,327,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:349,usability,statu,status,349,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:360,usability,command,command,360,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:865,usability,close,close,865,"Thanks for reporting @dawe and thanks for updating @WeilerP . I ran into the same problem with the pip version. When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:. <details>. <summary>. error message. </summary>. ```. Building wheel for llvmlite (setup.py) ... error. ERROR: Command errored out with exit status 1:. command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1812,usability,document,documentation,1812,"ead().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1905,usability,error,error,1905,"xec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_exte",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1912,usability,command,command,1912,"))' bdist_wheel -d /tmp/pip-wheel-rb92hbao. cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:1994,usability,ERROR,ERROR,1994,"te_860b580657d846f1993072c1a58436b0/. Complete output (15 lines):. running bdist_wheel. /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py. LLVM version... 11.1.0. . Traceback (most recent call last):. File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>. main(). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main. main_posix('linux', '.so'). File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix. raise RuntimeError(msg). RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path. Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite. . error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1. . ERROR: Failed building wheel for llvmlite. ```. </details>. Any ideas about that? When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>. from scvelo import datasets, logging, pl, pp, settings, tl, utils. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>. from scvelo.core import cleanup, SplicingDynamics. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>. from ._anndata import (. File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>. from typing_extensions import Literal. ModuleNotFoundError: No module named 'typing_extensions'`. `pip in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,availability,error,error,236,"@mihem, thanks for pointing this out. I opened [this](https://github.com/theislab/scvelo/issues/443) issue on the `scvelo` repo to resolve the `typing_extensions` problem. You may want to open a new issue on `scanpy` for the `llvmlite` error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,performance,error,error,236,"@mihem, thanks for pointing this out. I opened [this](https://github.com/theislab/scvelo/issues/443) issue on the `scvelo` repo to resolve the `typing_extensions` problem. You may want to open a new issue on `scanpy` for the `llvmlite` error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,safety,error,error,236,"@mihem, thanks for pointing this out. I opened [this](https://github.com/theislab/scvelo/issues/443) issue on the `scvelo` repo to resolve the `typing_extensions` problem. You may want to open a new issue on `scanpy` for the `llvmlite` error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:236,usability,error,error,236,"@mihem, thanks for pointing this out. I opened [this](https://github.com/theislab/scvelo/issues/443) issue on the `scvelo` repo to resolve the `typing_extensions` problem. You may want to open a new issue on `scanpy` for the `llvmlite` error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:22,availability,error,error,22,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:163,deployability,instal,install,163,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:251,deployability,updat,updating,251,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:297,deployability,instal,install,297,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:22,performance,error,error,22,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:22,safety,error,error,22,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:251,safety,updat,updating,251,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:251,security,updat,updating,251,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:61,testability,Assert,AssertionError,61,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:22,usability,error,error,22,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:234,usability,learn,learn,234,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1799:325,usability,learn,learn,325,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:. conda install -c conda-forge umap-learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799
https://github.com/scverse/scanpy/issues/1800:38,deployability,log,log,38,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:375,deployability,log,logfoldchanges,375,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:38,safety,log,log,38,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:118,safety,test,test,118,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:375,safety,log,logfoldchanges,375,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:38,security,log,log,38,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:375,security,log,logfoldchanges,375,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:38,testability,log,log,38,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:118,testability,test,test,118,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1800:375,testability,log,logfoldchanges,375,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'). print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()). ```. | | names | scores | logfoldchanges | pvals | pvals_adj |. |------:|:--------|---------:|-----------------:|-------------:|------------:|. | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |. | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |. | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |. | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |. | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800
https://github.com/scverse/scanpy/issues/1802:78,deployability,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:78,integrability,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:109,integrability,sub,subplots,109,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:78,modifiability,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:78,safety,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:192,security,control,control,192,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:78,testability,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:192,testability,control,control,192,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1802:52,usability,command,commands,52,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802
https://github.com/scverse/scanpy/issues/1803:143,deployability,api,api,143,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:143,integrability,api,api,143,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:143,interoperability,api,api,143,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:293,interoperability,standard,standard,293,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:267,modifiability,scal,scaling,267,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:452,modifiability,variab,variable,452,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:212,usability,clear,clear,212,"@moqri bit difficult to answer this question. What are you referring to? if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: . ```. The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:33,energy efficiency,measur,measure,33,Thanks @giovp! . I am using this measure to get the genes with most varied expression in my sample. I was wondering if the exact formula used to calculate dispersions_norm is documented in the reference? I believe it is based on dividing std by mean expression and then normalizing it somehow but an exact definition or formula would be greatly helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:175,usability,document,documented,175,Thanks @giovp! . I am using this measure to get the genes with most varied expression in my sample. I was wondering if the exact formula used to calculate dispersions_norm is documented in the reference? I believe it is based on dividing std by mean expression and then normalizing it somehow but an exact definition or formula would be greatly helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:345,usability,help,helpful,345,Thanks @giovp! . I am using this measure to get the genes with most varied expression in my sample. I was wondering if the exact formula used to calculate dispersions_norm is documented in the reference? I believe it is based on dividing std by mean expression and then normalizing it somehow but an exact definition or formula would be greatly helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:491,availability,sli,slightly,491,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:75,deployability,api,api,75,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:75,integrability,api,api,75,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:75,interoperability,api,api,75,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:293,interoperability,standard,standard,293,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:267,modifiability,scal,scaling,267,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:452,modifiability,variab,variable,452,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:491,reliability,sli,slightly,491,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:595,reliability,doe,does,595,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1803:605,usability,help,help,605,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:. ```. For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected. ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803
https://github.com/scverse/scanpy/issues/1804:80,modifiability,pac,package,80,@mbuttner any thoughts on this? Perhaps this would fit better with the `scCODA` package?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1804:31,deployability,compos,composition,31,"@ivirshup changes in cell type composition can be analysed using [`scCODA`](https://sccoda.readthedocs.io/en/latest/), which builds upon `scanpy`. Please note that in order to analyse changes in composition, you will need at least three different cell types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1804:125,deployability,build,builds,125,"@ivirshup changes in cell type composition can be analysed using [`scCODA`](https://sccoda.readthedocs.io/en/latest/), which builds upon `scanpy`. Please note that in order to analyse changes in composition, you will need at least three different cell types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1804:195,deployability,compos,composition,195,"@ivirshup changes in cell type composition can be analysed using [`scCODA`](https://sccoda.readthedocs.io/en/latest/), which builds upon `scanpy`. Please note that in order to analyse changes in composition, you will need at least three different cell types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1804:31,modifiability,compos,composition,31,"@ivirshup changes in cell type composition can be analysed using [`scCODA`](https://sccoda.readthedocs.io/en/latest/), which builds upon `scanpy`. Please note that in order to analyse changes in composition, you will need at least three different cell types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1804:195,modifiability,compos,composition,195,"@ivirshup changes in cell type composition can be analysed using [`scCODA`](https://sccoda.readthedocs.io/en/latest/), which builds upon `scanpy`. Please note that in order to analyse changes in composition, you will need at least three different cell types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1804
https://github.com/scverse/scanpy/issues/1806:96,deployability,scale,scale,96,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806
https://github.com/scverse/scanpy/issues/1806:96,energy efficiency,scale,scale,96,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806
https://github.com/scverse/scanpy/issues/1806:96,modifiability,scal,scale,96,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806
https://github.com/scverse/scanpy/issues/1806:96,performance,scale,scale,96,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806
https://github.com/scverse/scanpy/issues/1806:34,usability,efficien,efficient,34,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806
https://github.com/scverse/scanpy/pull/1811:38,deployability,fail,failing,38,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:63,deployability,build,build,63,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:86,deployability,log,logs,86,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:102,deployability,fail,fails,102,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:173,deployability,version,version,173,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:173,integrability,version,version,173,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:173,modifiability,version,version,173,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:38,reliability,fail,failing,38,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:69,reliability,doe,doesn,69,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:102,reliability,fail,fails,102,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:86,safety,log,logs,86,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:125,safety,compl,complaining,125,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:86,security,log,logs,86,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:125,security,compl,complaining,125,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:86,testability,log,logs,86,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:150,usability,tool,tools,150,"Hm, I am a bit lost wrt to the checks failing @giovp . the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:55,deployability,instal,installed,55,I don't think `igraph` or `louvain` are actually being installed on readthedocs. . I think you'll need to modify the `.readthedocs.yml` for this. But not calling `louvain` could also work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1811:106,security,modif,modify,106,I don't think `igraph` or `louvain` are actually being installed on readthedocs. . I think you'll need to modify the `.readthedocs.yml` for this. But not calling `louvain` could also work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811
https://github.com/scverse/scanpy/pull/1814:47,modifiability,extens,extensive,47,"looks really nice @Hrovatin , really liked how extensive the examples are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1814
https://github.com/scverse/scanpy/issues/1818:20,performance,multiplex,multiplex,20,"Sorry, I didn't use multiplex community detection. I focused more on preprocessing and briefly on graph construction. I stored the CITE-Seq data in `adata.obsm` and the gene-protein mapping as a column in `adata.var` - if this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:40,safety,detect,detection,40,"Sorry, I didn't use multiplex community detection. I focused more on preprocessing and briefly on graph construction. I stored the CITE-Seq data in `adata.obsm` and the gene-protein mapping as a column in `adata.var` - if this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:40,security,detect,detection,40,"Sorry, I didn't use multiplex community detection. I focused more on preprocessing and briefly on graph construction. I stored the CITE-Seq data in `adata.obsm` and the gene-protein mapping as a column in `adata.var` - if this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:227,usability,help,helps,227,"Sorry, I didn't use multiplex community detection. I focused more on preprocessing and briefly on graph construction. I stored the CITE-Seq data in `adata.obsm` and the gene-protein mapping as a column in `adata.var` - if this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,deployability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,integrability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,interoperability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:200,modifiability,layer,layer,200,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,modifiability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:343,modifiability,paramet,parameter,343,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:17,performance,multiplex,multiplex,17,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:408,performance,tune,tuned,408,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,reliability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,security,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:294,testability,integr,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:363,testability,understand,understand,363,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:277,usability,multi-mod,multi-modal,277,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,deployability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,integrability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,interoperability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:158,modifiability,layer,layer,158,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,modifiability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:570,performance,tune,tune,570,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,reliability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,security,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:484,security,empow,empowering,484,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:254,testability,integr,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:20,usability,feedback,feedback,20,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:498,usability,user,users,498,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:618,usability,experien,experience,618,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree. > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:292,availability,cluster,clustering,292,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:618,availability,cluster,clustering,618,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:292,deployability,cluster,clustering,292,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:618,deployability,cluster,clustering,618,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:344,energy efficiency,core,core,344,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:520,energy efficiency,core,core,520,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:723,energy efficiency,core,core,723,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:182,integrability,pub,published,182,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:282,performance,multiplex,multiplex,282,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:608,performance,multiplex,multiplex,608,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:25,security,polic,policy,25,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:121,usability,tool,tools,121,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:192,usability,tool,tools,192,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:361,usability,tool,tools,361,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:550,usability,clear,clearly,550,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:662,usability,User,Users,662,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:708,usability,tool,tool,708,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:395,availability,replic,replicate,395,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:416,availability,cluster,clustering,416,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:416,deployability,cluster,clustering,416,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:522,modifiability,pac,package,522,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:466,performance,network,network,466,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:129,safety,risk,risky,129,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:209,safety,test,test,209,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:129,security,risk,risky,129,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:466,security,network,network,466,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:209,testability,test,test,209,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:579,usability,help,help,579,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky. however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. . happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:292,modifiability,pac,package,292,"@bio-la thanks for the interest. If you are keen to take a stab at implementation I'd be very happy to support. . re: multiplex partition being a ""new algorithm"" or too experimental and therefore not suitable for Scanpy, I still disagree. FWIW it's already implemented in the great muon-data package (both for louvain and leiden):. - function call: https://github.com/PMBio/muon/blob/a0c81c28f26314341967943aa0c139a1ce80bcdd/muon/_core/tools.py#L847. - tutorial: https://github.com/PMBio/muon-tutorials/blob/master/cite-seq/1-CITE-seq-PBMC-5k.ipynb.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:118,performance,multiplex,multiplex,118,"@bio-la thanks for the interest. If you are keen to take a stab at implementation I'd be very happy to support. . re: multiplex partition being a ""new algorithm"" or too experimental and therefore not suitable for Scanpy, I still disagree. FWIW it's already implemented in the great muon-data package (both for louvain and leiden):. - function call: https://github.com/PMBio/muon/blob/a0c81c28f26314341967943aa0c139a1ce80bcdd/muon/_core/tools.py#L847. - tutorial: https://github.com/PMBio/muon-tutorials/blob/master/cite-seq/1-CITE-seq-PBMC-5k.ipynb.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:103,usability,support,support,103,"@bio-la thanks for the interest. If you are keen to take a stab at implementation I'd be very happy to support. . re: multiplex partition being a ""new algorithm"" or too experimental and therefore not suitable for Scanpy, I still disagree. FWIW it's already implemented in the great muon-data package (both for louvain and leiden):. - function call: https://github.com/PMBio/muon/blob/a0c81c28f26314341967943aa0c139a1ce80bcdd/muon/_core/tools.py#L847. - tutorial: https://github.com/PMBio/muon-tutorials/blob/master/cite-seq/1-CITE-seq-PBMC-5k.ipynb.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:436,usability,tool,tools,436,"@bio-la thanks for the interest. If you are keen to take a stab at implementation I'd be very happy to support. . re: multiplex partition being a ""new algorithm"" or too experimental and therefore not suitable for Scanpy, I still disagree. FWIW it's already implemented in the great muon-data package (both for louvain and leiden):. - function call: https://github.com/PMBio/muon/blob/a0c81c28f26314341967943aa0c139a1ce80bcdd/muon/_core/tools.py#L847. - tutorial: https://github.com/PMBio/muon-tutorials/blob/master/cite-seq/1-CITE-seq-PBMC-5k.ipynb.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:7,energy efficiency,Cool,Cool,7,"@giovp Cool! I hadn't seen this. If this is referenced in their paper, then multiplex leiden would fit into the category of ""used in sc analysis"" that I was arguing before, and I would be happy with it being in here. I do think that some testing should ideally happen on our side, so it would be great if you want to take this on, @bio-la !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:76,performance,multiplex,multiplex,76,"@giovp Cool! I hadn't seen this. If this is referenced in their paper, then multiplex leiden would fit into the category of ""used in sc analysis"" that I was arguing before, and I would be happy with it being in here. I do think that some testing should ideally happen on our side, so it would be great if you want to take this on, @bio-la !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:238,safety,test,testing,238,"@giovp Cool! I hadn't seen this. If this is referenced in their paper, then multiplex leiden would fit into the category of ""used in sc analysis"" that I was arguing before, and I would be happy with it being in here. I do think that some testing should ideally happen on our side, so it would be great if you want to take this on, @bio-la !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:238,testability,test,testing,238,"@giovp Cool! I hadn't seen this. If this is referenced in their paper, then multiplex leiden would fit into the category of ""used in sc analysis"" that I was arguing before, and I would be happy with it being in here. I do think that some testing should ideally happen on our side, so it would be great if you want to take this on, @bio-la !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:210,availability,replic,replicate,210,"Hi,. What should be the current workaround to run the following line of code? `sc.tl.leiden_multiplex(rna, [""rna_connectivities"", ""protein_connectivities""]) # Adds key ""leiden_multiplex"" by default`. I want to replicate [this](https://scanpy-tutorials.readthedocs.io/en/multiomics/cite-seq/pbmc5k.html#Reading) tutorial. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:24,energy efficiency,current,current,24,"Hi,. What should be the current workaround to run the following line of code? `sc.tl.leiden_multiplex(rna, [""rna_connectivities"", ""protein_connectivities""]) # Adds key ""leiden_multiplex"" by default`. I want to replicate [this](https://scanpy-tutorials.readthedocs.io/en/multiomics/cite-seq/pbmc5k.html#Reading) tutorial. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:22,availability,avail,available,22,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:238,availability,down,down,238,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:86,deployability,api,api,86,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:86,integrability,api,api,86,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:86,interoperability,api,api,86,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:22,reliability,availab,available,22,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:22,safety,avail,available,22,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:22,security,availab,available,22,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/issues/1818:262,usability,close,close,262,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818
https://github.com/scverse/scanpy/pull/1821:19,reliability,Doe,Does,19,"Thanks for the PR! Does `legend_loc` ever interact with colorbars otherwise? At least for the embedding plots, I think it only refers to categorical legends. Should colorbar positioning be separate from this argument, especially since options like `""on data""` don't really apply?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:42,usability,interact,interact,42,"Thanks for the PR! Does `legend_loc` ever interact with colorbars otherwise? At least for the embedding plots, I think it only refers to categorical legends. Should colorbar positioning be separate from this argument, especially since options like `""on data""` don't really apply?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:85,deployability,continu,continuous,85,"would also make sense to have this as `colorbar_loc` as this only really applies for continuous coloring, right? I can definetly see the benefit of removing colorbar from figure if this is wanted by the user",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:203,usability,user,user,203,"would also make sense to have this as `colorbar_loc` as this only really applies for continuous coloring, right? I can definetly see the benefit of removing colorbar from figure if this is wanted by the user",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:140,deployability,continu,continuous,140,"Sorry for the late reply, thought I responded to this! > would also make sense to have this as colorbar_loc as this only really applies for continuous coloring. I think that would make sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:158,deployability,continu,continuous,158,Hi! Could you finally solve the issue of removing colorbar from the figure? I have tried with the legend_loc=None and legend_loc='none' and they don't remove continuous colorbars. I have tried to fix this issue myself but couldn't find the exact place in the code to do that. Any help would be more than welcome!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:280,usability,help,help,280,Hi! Could you finally solve the issue of removing colorbar from the figure? I have tried with the legend_loc=None and legend_loc='none' and they don't remove continuous colorbars. I have tried to fix this issue myself but couldn't find the exact place in the code to do that. Any help would be more than welcome!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:64,deployability,continu,continuous,64,"Hi! Could you finally solve this issue? I am trying to remove a continuous colorbar from a umap and I cannot make it. Thanks,. Ldia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:47,safety,test,test,47,Just changed the argument a little and added a test. Will be out soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1821:47,testability,test,test,47,Just changed the argument a little and added a test. Will be out soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821
https://github.com/scverse/scanpy/pull/1822:77,modifiability,paramet,parameter,77,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:129,modifiability,concern,concerned,129,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:201,modifiability,paramet,parameters,201,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:112,safety,test,tests,112,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:112,testability,test,tests,112,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:129,testability,concern,concerned,129,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:58,modifiability,paramet,parameter,58,would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. . will check behaviour for this wrt groupby and palette.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:146,reliability,doe,does,146,would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. . will check behaviour for this wrt groupby and palette.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:189,usability,behavi,behaviour,189,would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. . will check behaviour for this wrt groupby and palette.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:315,deployability,observ,observations,315,"> would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. I'm not sure. I don't know off the top of my head how well we can make current functions work with `hue`, since seaborn uses it to group observations in many cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:249,energy efficiency,current,current,249,"> would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. I'm not sure. I don't know off the top of my head how well we can make current functions work with `hue`, since seaborn uses it to group observations in many cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:60,modifiability,paramet,parameter,60,"> would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. I'm not sure. I don't know off the top of my head how well we can make current functions work with `hue`, since seaborn uses it to group observations in many cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:148,reliability,doe,does,148,"> would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. I'm not sure. I don't know off the top of my head how well we can make current functions work with `hue`, since seaborn uses it to group observations in many cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:315,testability,observ,observations,315,"> would it then make generally sense to add an optional hue parameter in seaborn based plotting functions? was checking if hue is anywhere used but does not seem to be the case. I'm not sure. I don't know off the top of my head how well we can make current functions work with `hue`, since seaborn uses it to group observations in many cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:26,energy efficiency,current,current,26,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:331,energy efficiency,model,model,331,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:542,energy efficiency,current,currently,542,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:631,energy efficiency,current,currently,631,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:132,integrability,translat,translate,132,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:132,interoperability,translat,translate,132,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:331,security,model,model,331,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/pull/1822:359,usability,support,supports,359,"Looking at this again, my current reading is that. ```python. sc.pl.violin(adata, keys=keys, groupby=groupby, hue=hue). ```. should translate into something like:. ```python. df = sc.get.obs_df(adata, [groupby, hue] + keys). for k in keys:. sns.violinplot(data=df, x=groupby, y=k, hue=hue). ```. `sns.catplot` could also be a good model, but I don't think it supports the same range of attributes. While also passing through the right color palette to use for hue. * We would need to address where the legend goes, since I believe this would currently repeat the same legend for each plot. * `palette` handling, which I believe is currently resulting in a `KeyError`. * Axis labeling, we would probably want it per groupby, and leave `hue` to the legend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822
https://github.com/scverse/scanpy/issues/1823:56,deployability,version,versions,56,"I have also encountered this bug with the same software versions (pyton 3.9.4, scanpy 1.7.2).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:56,integrability,version,versions,56,"I have also encountered this bug with the same software versions (pyton 3.9.4, scanpy 1.7.2).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:56,modifiability,version,versions,56,"I have also encountered this bug with the same software versions (pyton 3.9.4, scanpy 1.7.2).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:148,deployability,releas,release,148,"Closing as:. * The original issue looks like a numba related problem, which I believe didn't work with python 3.9 at the time. * We're past the 1.7 release series and aren't supporting it anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:121,performance,time,time,121,"Closing as:. * The original issue looks like a numba related problem, which I believe didn't work with python 3.9 at the time. * We're past the 1.7 release series and aren't supporting it anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:174,usability,support,supporting,174,"Closing as:. * The original issue looks like a numba related problem, which I believe didn't work with python 3.9 at the time. * We're past the 1.7 release series and aren't supporting it anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:46,availability,error,error,46,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,availability,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:204,deployability,api,apic,204,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,deployability,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:21,energy efficiency,current,currently,21,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:114,energy efficiency,CPU,CPU,114,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:354,energy efficiency,cpu,cpuid,354,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,energy efficiency,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:204,integrability,api,apic,204,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:204,interoperability,api,apic,204,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:725,modifiability,deco,decodeassists,725,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:46,performance,error,error,46,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:114,performance,CPU,CPU,114,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:354,performance,cpu,cpuid,354,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:305,reliability,rdt,rdtscp,305,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,reliability,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:46,safety,error,error,46,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,safety,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:397,testability,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:46,usability,error,error,46,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:739,usability,paus,pausefilter,739,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:93,deployability,version,versions,93,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:109,deployability,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:143,deployability,log,logging,143,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:93,integrability,version,versions,93,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:109,integrability,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:93,modifiability,version,versions,93,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:109,modifiability,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:1,reliability,pra,pranzatelli,1,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:109,safety,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:143,safety,log,logging,143,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:143,security,log,logging,143,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:109,testability,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1823:143,testability,log,logging,143,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823
https://github.com/scverse/scanpy/issues/1824:241,deployability,api,api,241,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:241,integrability,api,api,241,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:241,interoperability,api,api,241,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:17,usability,user,user-images,17,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:166,usability,tool,toolbox,166,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:186,usability,tool,toolbox,186,![image](https://user-images.githubusercontent.com/21954664/117554339-4919c100-b057-11eb-8684-e32e74ea9e14.png). Would something like this suit? You could use the sc-toolbox: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.plot.cluster_composition_stacked_barplot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:158,deployability,api,api,158,@FADHLyemen Think I forgot tell you how to calculate the relative frequencies. Here you go: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.calc.relative_frequencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:66,energy efficiency,frequenc,frequencies,66,@FADHLyemen Think I forgot tell you how to calculate the relative frequencies. Here you go: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.calc.relative_frequencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:158,integrability,api,api,158,@FADHLyemen Think I forgot tell you how to calculate the relative frequencies. Here you go: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.calc.relative_frequencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:158,interoperability,api,api,158,@FADHLyemen Think I forgot tell you how to calculate the relative frequencies. Here you go: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.calc.relative_frequencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:103,usability,tool,toolbox,103,@FADHLyemen Think I forgot tell you how to calculate the relative frequencies. Here you go: https://sc-toolbox.readthedocs.io/en/latest/usage.html#sc_toolbox.api.calc.relative_frequencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:74,safety,test,test,74,"Thank you, do you have sankey plot also. another point, do you know which test to use to see which cell types are significant between groups?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:114,security,sign,significant,114,"Thank you, do you have sankey plot also. another point, do you know which test to use to see which cell types are significant between groups?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:74,testability,test,test,74,"Thank you, do you have sankey plot also. another point, do you know which test to use to see which cell types are significant between groups?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:43,security,sign,significance,43,"No, I don't have Sankey plots. . Regarding significance we usually use https://github.com/theislab/diffxpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:85,deployability,compos,composition,85,Hey! You can check out [scCODA](https://github.com/theislab/scCODA) for differential composition analysis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:85,modifiability,compos,composition,85,Hey! You can check out [scCODA](https://github.com/theislab/scCODA) for differential composition analysis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1157,deployability,modul,module,1157,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1670,deployability,api,api,1670,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2045,deployability,patch,patches,2045,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2069,deployability,Patch,Patch,2069,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1670,integrability,api,api,1670,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2139,integrability,sub,subscriptable,2139,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1670,interoperability,api,api,1670,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1157,modifiability,modul,module,1157,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1650,modifiability,pac,packages,1650,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1130,safety,input,input-,1130,"ter(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1157,safety,modul,module,1157,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2045,safety,patch,patches,2045,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2069,safety,Patch,Patch,2069,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:155,security,Auth,Author,155,"do you mind if I pigback on your issue? is this usage example correct? ```. relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:306,security,Auth,Author,306,"do you mind if I pigback on your issue? is this usage example correct? ```. relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:774,security,Auth,Author,774,"do you mind if I pigback on your issue? is this usage example correct? ```. relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1247,security,Auth,Author,1247,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1408,security,Auth,Author,1408,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2045,security,patch,patches,2045,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:2069,security,Patch,Patch,2069,"thor', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1086,testability,Trace,Traceback,1086,"ncies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:1130,usability,input,input-,1130,"ter(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). print(relative_frequencies). sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). ```. I am getting this output. ```. 0 1 2 3 4 5 \. Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 . Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 . Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 . Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author . Benitez 0.111111 0.133191 0.021368 0.000000 Benitez . Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari . Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari . Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-44-a5b07a2bfb6d> in <module>. 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None). 7 print(relative_frequencies). ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save). 835 for i, typ in enumerate(reversed(cell_types)):. 836 fig = sb.barplot(. --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize. 838 ). 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:76,usability,tool,toolbox,76,@dm8000 could you please open an issue at https://github.com/schillerlab/sc-toolbox. I will check it out then. It's a sc-toolbox issue and not directly Scanpy related :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:121,usability,tool,toolbox,121,@dm8000 could you please open an issue at https://github.com/schillerlab/sc-toolbox. I will check it out then. It's a sc-toolbox issue and not directly Scanpy related :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:78,usability,tool,toolbox,78,"> @dm8000 could you please open an issue at https://github.com/schillerlab/sc-toolbox. > . > I will check it out then. It's a sc-toolbox issue and not directly Scanpy related :). sure! thank you, I will post there!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:129,usability,tool,toolbox,129,"> @dm8000 could you please open an issue at https://github.com/schillerlab/sc-toolbox. > . > I will check it out then. It's a sc-toolbox issue and not directly Scanpy related :). sure! thank you, I will post there!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:62,availability,avail,available,62,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:92,deployability,integr,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:92,integrability,integr,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:92,interoperability,integr,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:109,interoperability,specif,specific,109,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:92,modifiability,integr,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:62,reliability,availab,available,62,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:92,reliability,integr,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
https://github.com/scverse/scanpy/issues/1824:62,safety,avail,available,62,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824
