id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/2369:5387,testability,Trace,Traceback,5387,"lders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5462,testability,test,test,5462,": /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/op",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5648,testability,test,test,5648,"on-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5839,testability,test,test,5839,"ption information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6022,testability,test,test,6022,"ne 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6228,testability,test,test,6228,"ternal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6483,testability,test,test,6483,"e-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6709,testability,test,test,6709,".py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/pyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6949,testability,test,test,6949,"er.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File """,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7160,testability,test,test,7160,"solve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7332,testability,test,test,7332,"lution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCand",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7515,testability,test,test,7515,"pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7698,testability,test,test,7698,"a3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7984,testability,test,test,7984,"/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/pyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8168,testability,test,test,8168,"ython3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8397,testability,test,test,8397,""", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. ret",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8564,testability,test,test,8564,"olution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8758,testability,test,test,8758,"olvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/minic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8973,testability,test,test,8973,"a3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9283,testability,test,test,9283,"link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = genera",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9554,testability,test,test,9554,"3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9772,testability,test,test,9772,".11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9969,testability,test,test,9969,"2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:10153,testability,test,test,10153,"_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:10372,testability,test,test,10372,"_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:188,usability,error,error,188,"The problem is that it does not install at all. . When I run. ```. conda create -n test. conda activate test. conda install python=3.11. conda install -c conda-forge scanpy. ```. I get an error output for the last line, which is:. ```. Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0. - feature:|@/osx-64::__osx==10.16=0. - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16. ```. Repeating this with python=3.10 does not give an error. Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```. Collecting numba>=0.41.0. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod. Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache. Current age based on date: 1302943. Ignoring unknown cache-control directive: immutable. Freshness lifetime from max-age: 365000000. The response is ""fresh"", returning cached response. 365000000 > 1302943. Using cached numba-0.56.4.tar.gz (2.4 MB). Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:817,usability,error,error,817,"The problem is that it does not install at all. . When I run. ```. conda create -n test. conda activate test. conda install python=3.11. conda install -c conda-forge scanpy. ```. I get an error output for the last line, which is:. ```. Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0. - feature:|@/osx-64::__osx==10.16=0. - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16. ```. Repeating this with python=3.10 does not give an error. Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```. Collecting numba>=0.41.0. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod. Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache. Current age based on date: 1302943. Ignoring unknown cache-control directive: immutable. Freshness lifetime from max-age: 365000000. The response is ""fresh"", returning cached response. 365000000 > 1302943. Using cached numba-0.56.4.tar.gz (2.4 MB). Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:904,usability,error,error,904,"The problem is that it does not install at all. . When I run. ```. conda create -n test. conda activate test. conda install python=3.11. conda install -c conda-forge scanpy. ```. I get an error output for the last line, which is:. ```. Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0. - feature:|@/osx-64::__osx==10.16=0. - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16. ```. Repeating this with python=3.10 does not give an error. Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```. Collecting numba>=0.41.0. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod. Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache. Current age based on date: 1302943. Ignoring unknown cache-control directive: immutable. Freshness lifetime from max-age: 365000000. The response is ""fresh"", returning cached response. 365000000 > 1302943. Using cached numba-0.56.4.tar.gz (2.4 MB). Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2034,usability,command,command,2034,"/T/pip-unpack-9g89heod. Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache. Current age based on date: 1302943. Ignoring unknown cache-control directive: immutable. Freshness lifetime from max-age: 365000000. The response is ""fresh"", returning cached response. 365000000 > 1302943. Using cached numba-0.56.4.tar.gz (2.4 MB). Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2671,usability,support,supported,2671,"yvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2682,usability,error,error,2682,"mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportErr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2712,usability,error,error,2712,"-tracker-740xp5sy'. Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2829,usability,error,error,2829,"jnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba. Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2908,usability,command,command,2908," Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:2918,usability,User,Users,2918,"temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. Running command python setup.py egg_info. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:3261,usability,clear,clear,3261,"nstall-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:3267,usability,error,error,3267,"-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>. _guard_py_ver(). File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver. raise RuntimeError(msg.format(cur_py, min_py, max_py)). RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported. error: subprocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:3691,usability,error,error,3691,"ocess-exited-with-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:3708,usability,ERROR,ERROR,3708,"-error. . × python setup.py egg_info did not run successfully. │ exit code: 1. ╰─> See above for output. . note: This error originates from a subprocess, and is likely not a problem with pip. full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '. exec(compile('""'""''""'""''""'""'. # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py. #. # - It imports setuptools before invoking setup.py, to enable projects that directly. # import from `distutils.core` to work with newer packaging standards. # - It provides a clear error message when setuptools is not installed. # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so. # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:. # manifest_maker: standard file '""'""'-c'""'""' not found"". # - It generates a shim setup.py, for handling setup.cfg-only projects. import os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:4619,usability,error,error,4619," os, sys, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:4626,usability,error,error,4626,"s, tokenize. . try:. import setuptools. except ImportError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/op",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:4675,usability,error,error,4675,"portError as error:. print(. ""ERROR: Can not execute `setup.py` since setuptools is not available in "". ""the build environment."",. file=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:4808,usability,hint,hint,4808,"ile=sys.stderr,. ). sys.exit(1). . __file__ = %r. sys.argv[0] = __file__. . if os.path.exists(__file__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:4904,usability,User,Users,4904,"ile__):. filename = __file__. with tokenize.open(__file__) as f:. setup_py_code = f.read(). else:. filename = ""<auto-generated setuptools caller>"". setup_py_code = ""from setuptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5078,usability,User,Users,5078,"ptools import setup; setup()"". . exec(compile(setup_py_code, filename, ""exec"")). '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5216,usability,error,error,5216,"kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/minicond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5430,usability,User,Users,5430,"/pip-pip-egg-info-tlduu_0q. cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5566,usability,statu,status,5566,"6f16ee1b89979/. Preparing metadata (setup.py) ... error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5616,usability,User,Users,5616,"error. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_crit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5807,usability,User,Users,5807,"nt: See above for details. Exception information:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5887,usability,command,commands,5887,"st):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:5990,usability,User,Users,5990,"s/build/metadata_legacy.py"", line 64, in generate_metadata. call_subprocess(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6196,usability,User,Users,6196,"ubprocess. raise error. pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6451,usability,User,Users,6451,"3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper. status = run_func(*args). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6677,usability,User,Users,6677,"s/pip/_internal/cli/req_command.py"", line 247, in wrapper. return func(self, options, args). ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:6917,usability,User,Users,6917,"n run. requirement_set = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve. result = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7128,usability,User,Users,7128,"lt = self._result = resolver.resolve(. ^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7300,usability,User,Users,7300,"e 481, in resolve. state = resolution.resolve(requirements, max_rounds=max_rounds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7483,usability,User,Users,7483,"t/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve. failure_causes = self._attempt_to_pin_criterion(name). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7666,usability,User,Users,7666," File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion. criteria = self._get_updated_criteria(candidate). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:7952,usability,User,Users,7952,"ib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria. self._add_to_criteria(criteria, requirement, parent=candidate). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8136,usability,User,Users,8136,"opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria. if not criterion.candidates:. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8365,usability,User,Users,8365,"p/_vendor/resolvelib/structs.py"", line 151, in __bool__. return bool(self._sequence). ^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8532,usability,User,Users,8532,"site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__. return any(self). ^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/op",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8726,usability,User,Users,8726,"es/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>. return (c for c in iterator if id(c) not in self._incompatible_ids). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:8941,usability,User,Users,8941," File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built. candidate = func(). ^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9251,usability,User,Users,9251,"e 206, in _make_candidate_from_link. self._link_candidate_cache[link] = LinkCandidate(. ^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__. super().__init__(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9522,usability,User,Users,9522,"File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__. self.dist = self._prepare(). ^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9740,usability,User,Users,9740,"niconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare. dist = self._prepare_distribution(). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:9937,usability,User,Users,9937,"/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution. return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:10121,usability,User,Users,10121,"_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:10340,usability,User,Users,10340,"_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:10550,usability,error,error,10550,"_linked_requirement(self._ireq, parallel_builds=True). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement. return self._prepare_linked_requirement(req, parallel_builds). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement. dist = _get_prepared_distribution(. ^^^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution. abstract_dist.prepare_distribution_metadata(. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata. self.req.prepare_metadata(). File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata. self.metadata_directory = generate_metadata_legacy(. ^^^^^^^^^^^^^^^^^^^^^^^^^. File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata. raise MetadataGenerationFailed(package_details=details) from error. pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed. Remote version of pip: 22.3.1. Local version of pip: 22.3.1. Was pip installed by pip? False. Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:22,deployability,instal,install,22,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:40,deployability,version,version,40,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:61,deployability,version,versions,61,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:40,integrability,version,version,40,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:61,integrability,version,versions,61,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:40,modifiability,version,version,40,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:61,modifiability,version,versions,61,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:86,usability,support,supported,86,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:94,availability,error,error,94,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:539,availability,error,error,539,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:21,deployability,instal,install,21,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:53,deployability,instal,install,53,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:223,deployability,contain,containing,223,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:377,deployability,version,versions,377,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:490,deployability,instal,install,490,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:701,deployability,instal,install,701,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:713,deployability,depend,dependent,713,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:377,integrability,version,versions,377,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:713,integrability,depend,dependent,713,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:134,interoperability,conflict,conflicts,134,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:210,interoperability,conflict,conflicting,210,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:367,interoperability,specif,specified,367,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:393,interoperability,conflict,conflict,393,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:200,modifiability,pac,packages,200,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:377,modifiability,version,versions,377,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:713,modifiability,depend,dependent,713,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:94,performance,error,error,94,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:539,performance,error,error,539,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:94,safety,error,error,94,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:179,safety,compl,completely,179,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:539,safety,error,error,539,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:713,safety,depend,dependent,713,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:179,security,compl,completely,179,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:713,testability,depend,dependent,713,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:94,usability,error,error,94,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:362,usability,user,user-specified,362,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:539,usability,error,error,539,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2369:661,usability,support,supports,661,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369
https://github.com/scverse/scanpy/issues/2371:39,availability,slo,slow,39,"Just came across this, sorry for being slow here and thanks for your interest in scanpy! Is this still an issue for you? If yes, is there a way you could supply an example I could check for such an Excel file, e.g. via a Link for a small public dataset? Just tried with an own dummy file (scanpy 1.9.5) and it seemed to work",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371
https://github.com/scverse/scanpy/issues/2371:238,integrability,pub,public,238,"Just came across this, sorry for being slow here and thanks for your interest in scanpy! Is this still an issue for you? If yes, is there a way you could supply an example I could check for such an Excel file, e.g. via a Link for a small public dataset? Just tried with an own dummy file (scanpy 1.9.5) and it seemed to work",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371
https://github.com/scverse/scanpy/issues/2371:39,reliability,slo,slow,39,"Just came across this, sorry for being slow here and thanks for your interest in scanpy! Is this still an issue for you? If yes, is there a way you could supply an example I could check for such an Excel file, e.g. via a Link for a small public dataset? Just tried with an own dummy file (scanpy 1.9.5) and it seemed to work",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371
https://github.com/scverse/scanpy/issues/2376:58,deployability,Version,Versions,58,"You didn’t fill out the **Minimal Code Sample**, or the **Versions** please try again with both there",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376
https://github.com/scverse/scanpy/issues/2376:58,integrability,Version,Versions,58,"You didn’t fill out the **Minimal Code Sample**, or the **Versions** please try again with both there",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376
https://github.com/scverse/scanpy/issues/2376:58,modifiability,Version,Versions,58,"You didn’t fill out the **Minimal Code Sample**, or the **Versions** please try again with both there",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376
https://github.com/scverse/scanpy/issues/2376:26,usability,Minim,Minimal,26,"You didn’t fill out the **Minimal Code Sample**, or the **Versions** please try again with both there",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376
https://github.com/scverse/scanpy/issues/2381:321,performance,time,times,321,"Ok I understand what's happening now. No need to answer. For anyone else following at home:. The distance matrix would be symmetric if not for the kNN constraint, which sets the non-zero values to 20 for each row. The columns don't have to have 20 non-zero entries because each cell can contribute an arbitrary number of times to another cell's local neighborhood so any individual distance in a column is preserved as long as it contributes to the neighborhood of the cell in the corresponding row. . Sorry for the unecessary post!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381
https://github.com/scverse/scanpy/issues/2381:5,testability,understand,understand,5,"Ok I understand what's happening now. No need to answer. For anyone else following at home:. The distance matrix would be symmetric if not for the kNN constraint, which sets the non-zero values to 20 for each row. The columns don't have to have 20 non-zero entries because each cell can contribute an arbitrary number of times to another cell's local neighborhood so any individual distance in a column is preserved as long as it contributes to the neighborhood of the cell in the corresponding row. . Sorry for the unecessary post!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381
https://github.com/scverse/scanpy/issues/2383:59,availability,servic,service,59,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:27,deployability,automat,automatic,27,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:59,deployability,servic,service,59,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:59,integrability,servic,service,59,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:59,modifiability,servic,service,59,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:27,testability,automat,automatic,27,"@Zethson I suspect it's an automatic response via an email service in China. ""Received, thanks""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:111,availability,servic,service,111,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:428,availability,error,errors,428,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:520,availability,error,error,520,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:80,deployability,automat,automatic,80,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:111,deployability,servic,service,111,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:111,integrability,servic,service,111,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:695,interoperability,convers,conversion,695,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:111,modifiability,servic,service,111,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:428,performance,error,errors,428,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:520,performance,error,error,520,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:428,safety,error,errors,428,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:520,safety,error,error,520,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:80,testability,automat,automatic,80,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:428,usability,error,errors,428,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:520,usability,error,error,520,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2383:616,usability,User,Users,616,"> @brainfo I'm afraid that we only speak English. Hi sorry and indeed that's an automatic reply from the email service I just changed the email address so this would not be a problem later. To add one more problem encountered when writing out anndata object: . When the anndata has 'predicted_doublet' in obs annotation from sc.external.pp.scrublet, the boolean values could not be implicitly converted to strings so that gives errors: . ```{py}. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'predicted_doublet' of <class 'h5py._hl.group.Group'> to /. ```. Users can map the values like this but would it be better to have the implicit conversion while read/write anndatas? ```{py}. anndata.obs['predicted_doublet'] = anndata.obs['predicted_doublet'].map({True: 'True', False: 'False'}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383
https://github.com/scverse/scanpy/issues/2386:88,availability,replic,replicate,88,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:142,availability,cluster,clustering,142,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:142,deployability,cluster,clustering,142,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:352,deployability,log,log-transforming,352,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:117,integrability,pub,public,117,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:356,integrability,transform,transforming,356,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:356,interoperability,transform,transforming,356,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:352,safety,log,log-transforming,352,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:352,security,log,log-transforming,352,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:245,testability,context,context,245,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:352,testability,log,log-transforming,352,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:103,usability,behavi,behavior,103,"Hi danli249, . what dataset are you using? If this is your own dataset, can you instead replicate this behavior on a public dataset? e.g. the clustering tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html? without much needed context, this looks like some oversight in the preprocessing, for example not normalizing your data or not log-transforming it. Michael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:86,usability,learn,learn,86,"Hi Michael:. I compare the `umap` in `scanpy` with the original [`umap`](https://umap-learn.readthedocs.io/en/latest/plotting.html) (https://umap-learn.readthedocs.io/en/latest/plotting.html) using the same dataset, the original `umap` works well, I think the problem is in `scanpy`. I edited my question to include this. Do you have some suggestions? Thanks. Dan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:146,usability,learn,learn,146,"Hi Michael:. I compare the `umap` in `scanpy` with the original [`umap`](https://umap-learn.readthedocs.io/en/latest/plotting.html) (https://umap-learn.readthedocs.io/en/latest/plotting.html) using the same dataset, the original `umap` works well, I think the problem is in `scanpy`. I edited my question to include this. Do you have some suggestions? Thanks. Dan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:621,deployability,instal,installation,621,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:613,energy efficiency,current,current,613,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:19,performance,perform,perform,19,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:19,usability,perform,perform,19,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:459,usability,behavi,behaving,459,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:292,availability,down,down,292,"Hi Michael:. Thanks for your suggestion, I run the tutorial without changing anything: https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb, . ![image](https://user-images.githubusercontent.com/33963919/209399067-2287268f-a77c-4f12-ba5b-d56e4370b2f2.png). My `scanpy` can't do down dimension correctly in the `pbmc3k` data. ![image](https://user-images.githubusercontent.com/33963919/209398837-d2f77dfa-5855-4bf7-9c7d-33625909af09.png). Can you please let me know what is wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:175,usability,user,user-images,175,"Hi Michael:. Thanks for your suggestion, I run the tutorial without changing anything: https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb, . ![image](https://user-images.githubusercontent.com/33963919/209399067-2287268f-a77c-4f12-ba5b-d56e4370b2f2.png). My `scanpy` can't do down dimension correctly in the `pbmc3k` data. ![image](https://user-images.githubusercontent.com/33963919/209398837-d2f77dfa-5855-4bf7-9c7d-33625909af09.png). Can you please let me know what is wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:356,usability,user,user-images,356,"Hi Michael:. Thanks for your suggestion, I run the tutorial without changing anything: https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb, . ![image](https://user-images.githubusercontent.com/33963919/209399067-2287268f-a77c-4f12-ba5b-d56e4370b2f2.png). My `scanpy` can't do down dimension correctly in the `pbmc3k` data. ![image](https://user-images.githubusercontent.com/33963919/209398837-d2f77dfa-5855-4bf7-9c7d-33625909af09.png). Can you please let me know what is wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:72,deployability,version,version,72,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:99,deployability,updat,update,99,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:124,deployability,version,version,124,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:116,energy efficiency,current,current,116,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:72,integrability,version,version,72,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:124,integrability,version,version,124,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:198,interoperability,distribut,distribution,198,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:72,modifiability,version,version,72,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:124,modifiability,version,version,124,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:5,reliability,doe,does,5,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:99,safety,updat,update,99,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:99,security,updat,update,99,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:158,testability,understand,understand,158,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:2,deployability,upgrad,upgrade,2,"I upgrade `scanpy` to `1.9.1`, and now `sc.tl.umap` works as expected:. ![image](https://user-images.githubusercontent.com/33963919/209405851-2798000b-0e90-4b3f-982c-f0de196489c5.png). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:2,modifiability,upgrad,upgrade,2,"I upgrade `scanpy` to `1.9.1`, and now `sc.tl.umap` works as expected:. ![image](https://user-images.githubusercontent.com/33963919/209405851-2798000b-0e90-4b3f-982c-f0de196489c5.png). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:89,usability,user,user-images,89,"I upgrade `scanpy` to `1.9.1`, and now `sc.tl.umap` works as expected:. ![image](https://user-images.githubusercontent.com/33963919/209405851-2798000b-0e90-4b3f-982c-f0de196489c5.png). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:36,usability,close,close,36,great! good luck out there. can you close the issue? best,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1857,availability,cluster,clusters,1857,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1304,deployability,scale,scale,1304,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1857,deployability,cluster,clusters,1857,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:941,energy efficiency,predict,predict,941,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1304,energy efficiency,scale,scale,1304,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:230,integrability,batch,batch,230,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:455,integrability,batch,batch,455,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1099,integrability,filter,filter,1099,"` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1483,integrability,batch,batch,1483,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1304,modifiability,scal,scale,1304,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:2045,modifiability,paramet,parameter,2045,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:230,performance,batch,batch,230,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:455,performance,batch,batch,455,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1304,performance,scale,scale,1304,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1483,performance,batch,batch,1483,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:941,safety,predict,predict,941,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:262,usability,user,user-images,262,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:487,usability,user,user-images,487,"Hi Michael. For my own data, . I compared `scanpy` `umap` with the original `umap`. The original `umap` output is:. ```. import umap. import umap.plot. mapper = umap.UMAP().fit(adata.X). umap.plot.points(mapper, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1552,usability,user,user-images,1552,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:1721,usability,user,user-images,1721,"//user-images.githubusercontent.com/33963919/209409453-578ce08c-3eed-4b42-a0c9-e3165d5859cc.png). ```. mapper2 = umap.UMAP().fit(adata.obsm['X_pca']). umap.plot.points(mapper2, labels=adata.obs['batch']). ```. ![image](https://user-images.githubusercontent.com/33963919/209409492-9559fc2e-817e-4003-b8e2-560a5687b241.png). The `scanpy` output is:. ```. sc.pp.filter_cells(adata, min_counts = 100). adata = adata[adata.obs['mt_frac'] < 0.2]. sc.pp.filter_cells(adata, min_genes = 1). sc.pp.filter_genes(adata, min_cells=1). import doubletdetection. clf = doubletdetection.BoostClassifier(. n_iters=10, . use_phenograph=False, . standard_scaling=True. ). doublets = clf.fit(adata.X).predict(p_thresh=1e-16, voter_thresh=0.5). doublet_score = clf.doublet_score(). adata.obs[""doublet""] = doublets. adata = adata[adata.obs[""doublet""] == 0.0] # filter out doublets . sc.pp.normalize_total(adata, target_sum=1e6). sc.pp.log1p(adata, base=2). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, . flavor='cell_ranger'). sc.pp.scale(adata, max_value=10). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata, min_dist=0.5, spread = 1.0). sc.pl.umap(adata, color='batch', use_raw=False, save='batch_umap.pdf'). ```. ![image](https://user-images.githubusercontent.com/33963919/209409864-0c0c0207-c3af-40f3-8af4-c3448f9ca0f1.png). If I do not run pca, then run `sc.tl.umap` I will get:. ![image](https://user-images.githubusercontent.com/33963919/209410266-efca8967-c8d0-4253-b273-3a048ae27653.png). 1. What should I do to get more compact clusters? 2. Why the output so different between `scanpy.tl.umap` and the orignal `umap`? 3. Should I use PCA for umap or not? 4. Should I use less highly_variable_genes in `n_top_genes ` parameter when I run `scanpy.pp.highly_variable_genes` to get higher explained variance in `scanpy.pp.pca` and `adata.uns['pca']['variance_ratio']` ? If yes, how much percent explained variance is good? Thanks. Dan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:90,usability,dialog,dialogue,90,". Hi Dan, . These are good questions. I'm not sure this is the appropriate venue for this dialogue. Issues are meant to reporting bugs or discuss feature requests for scanpy. Maybe we can move this to a forum like this instead https://discourse.scverse.org/c/help/27 ? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2386:259,usability,help,help,259,". Hi Dan, . These are good questions. I'm not sure this is the appropriate venue for this dialogue. Issues are meant to reporting bugs or discuss feature requests for scanpy. Maybe we can move this to a forum like this instead https://discourse.scverse.org/c/help/27 ? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386
https://github.com/scverse/scanpy/issues/2389:16,availability,operat,operates,16,"normalize total operates inplace. in the code example above `adata.layers[""counts""]` and `adata.X` are referencing the same object. You should add `.copy()` if you want independent behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389
https://github.com/scverse/scanpy/issues/2389:67,modifiability,layer,layers,67,"normalize total operates inplace. in the code example above `adata.layers[""counts""]` and `adata.X` are referencing the same object. You should add `.copy()` if you want independent behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389
https://github.com/scverse/scanpy/issues/2389:181,usability,behavi,behavior,181,"normalize total operates inplace. in the code example above `adata.layers[""counts""]` and `adata.X` are referencing the same object. You should add `.copy()` if you want independent behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389
https://github.com/scverse/scanpy/issues/2389:51,modifiability,layer,layers,51,So one should always use .copy() when creating new layers from .X? .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389
https://github.com/scverse/scanpy/issues/2390:187,availability,avail,available,187,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:385,interoperability,specif,specifically,385,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:187,reliability,availab,available,187,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:187,safety,avail,available,187,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:187,security,availab,available,187,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:120,testability,context,context,120,"cc: @Zethson @grst. Hey, . In principle this sounds good, but I'd like to hear a little bit more about the usecase. For context on our side, there are some other paths for speeding up DE available (probably some form of calculating statistics via https://github.com/scverse/anndata/pull/564). There're also increased momentum on more featureful DE in the scverse ecosystem. If you are specifically looking for faster scanpy DE, this makes sense, though there may be some easier paths forward (at least to me). If you need anything fancier or even just different, it could be good to check in with other efforts. E.g. . * https://github.com/theislab/pertpy/issues/189",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:853,availability,slo,slow,853,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:286,deployability,automat,automatically,286,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:300,reliability,doe,does,300,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:853,reliability,slo,slow,853,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:936,reliability,doe,does,936,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:286,testability,automat,automatically,286,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:706,testability,simpl,simple,706,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:30,usability,help,help,30,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:77,usability,user,user,77,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:107,usability,interact,interactive,107,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:706,usability,simpl,simple,706,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:805,usability,interact,interactive,805,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy? Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:904,availability,consist,consistent,904,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:778,deployability,releas,released,778,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:736,energy efficiency,current,currently,736,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:915,integrability,interfac,interface,915,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:915,interoperability,interfac,interface,915,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:915,modifiability,interfac,interface,915,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:246,performance,parallel,parallelized,246,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:205,reliability,doe,doesn,205,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:439,reliability,pra,practice,439,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:475,reliability,doe,doesn,475,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:587,reliability,pra,practices,587,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:675,reliability,doe,does,675,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:128,testability,simpl,simple,128,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:128,usability,simpl,simple,128,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:904,usability,consist,consistent,904,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). . What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy. >. > Could does this relate to @davidsebfischer and diffxpy? Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:598,availability,mask,mask,598,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:617,availability,mask,mask,617,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1702,availability,slo,slow,1702,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:219,deployability,automat,automatically,219,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:338,energy efficiency,current,current,338,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:452,energy efficiency,alloc,allocated,452,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:524,energy efficiency,alloc,allocated,524,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1333,energy efficiency,power,power,1333,"e current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1423,energy efficiency,reduc,reduce,1423,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1921,energy efficiency,reduc,reduce,1921,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:2315,energy efficiency,current,currently,2315,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:555,integrability,sub,subsetting,555,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1241,integrability,sub,subprocess,1241," each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1720,integrability,interfac,interface,1720,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1796,integrability,interfac,interfaces,1796,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1162,interoperability,share,share,1162,"tween compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Parti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1720,interoperability,interfac,interface,1720,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1796,interoperability,interfac,interfaces,1796,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:285,modifiability,concern,concern,285,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:993,modifiability,concern,concern,993,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1720,modifiability,interfac,interface,1720,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1796,modifiability,interfac,interfaces,1796,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:364,performance,parallel,parallelize,364,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:438,performance,memor,memory,438,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:514,performance,memor,memory,514,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:632,performance,parallel,parallelize,632,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:669,performance,memor,memory,669,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:836,performance,memor,memory,836,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:978,performance,memor,memory,978,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1168,performance,memor,memory,1168,"compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1466,performance,parallel,parallelized,1466,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1609,performance,parallel,parallelized,1609,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1867,performance,time,time,1867,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:2380,performance,time,timeline,2380,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:233,reliability,doe,does,233,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1702,reliability,slo,slow,1702,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:2080,reliability,pra,practice,2080,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:2116,reliability,doe,doesn,2116,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:219,testability,automat,automatically,219,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:285,testability,concern,concern,285,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:993,testability,concern,concern,993,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:10,usability,user,user,10,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:40,usability,interact,interactive,40,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:438,usability,memor,memory,438,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:514,usability,memor,memory,514,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:669,usability,memor,memory,669,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:836,usability,memor,memory,836,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:931,usability,tool,tools,931,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:978,usability,memor,memory,978,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1168,usability,memor,memory,1168,"compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2390:1654,usability,interact,interactive,1654,"ithin that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute. * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390
https://github.com/scverse/scanpy/issues/2391:19,interoperability,compatib,compatibility,19,"This seems to be a compatibility issue with outputs from SpaceRanger 2.0.0. Following this [issue](https://github.com/satijalab/seurat/pull/6208#issue-1308410878) on Seurat, I was finally able to solve it. Please consider an official fix for this. I am new to github and still trying to figure out how to do a pull request. Thanks! @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:108,interoperability,compatib,compatible,108,"Hi @cf98 what was the fix for this? Did you revert to SpaceRanger 1.0, or did you find a way to make scanPy compatible with SpaceRanger 2.0? I didn't see anything directly applicable to this in the Seurat thread. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:388,deployability,version,versions,388,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:330,energy efficiency,load,load,330,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:388,integrability,version,versions,388,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:237,modifiability,maintain,maintaining,237,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:388,modifiability,version,versions,388,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:330,performance,load,load,330,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:237,safety,maintain,maintaining,237,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2391:225,security,auth,authors,225,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,. Changfeng.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391
https://github.com/scverse/scanpy/issues/2392:3,usability,prefer,prefer,3,We prefer usage questions over here: https://discourse.scverse.org/. Could you please repost there?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2392
https://github.com/scverse/scanpy/issues/2393:78,availability,state,statement,78,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:333,deployability,pipelin,pipeline,333,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:78,integrability,state,statement,78,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:333,integrability,pipelin,pipeline,333,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:353,integrability,filter,filtering,353,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:55,testability,understand,understand,55,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:126,testability,understand,understand,126,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:426,usability,user,user-images,426,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:568,usability,user,user-images,568,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps. ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png). ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:40,testability,understand,understand,40,I'm glad you found the issue then! I do understand why this is prefixed though... it would be quite confusing having a data object with mixed genes from mice and humans and this not immediately being obvious. I often catch myself looking up which specie was capitalized and which wasn't ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:23,usability,close,closed,23,Can this issue then be closed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2393:48,usability,help,helping,48,"Glad you figured it out and thanks @LuckyMD for helping out. In any case, please ask usage questions here: https://discourse.scverse.org/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393
https://github.com/scverse/scanpy/issues/2398:4,reliability,doe,does,4,How does scanpy/muon know that these are separate modalities? Isn't it a bug to discard certain rows from a file?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:57,energy efficiency,load,load,57,"Hey @kchl5 and @vitkl,. Muon (`mu.read_10x_h5()`) should load it correctly if the `feature_types` value for the gRNAs is different from the one for the genes. As they are missing, I assume it is. Moreover, just in case you're interested, splitting by `feature_types` is even [a feature](https://github.com/scverse/mudata/blob/4d3b5f4e6039b4a31519584db5461a5809741dce/mudata/_core/mudata.py#L88) of the `MuData` initialiser, so running . ```py. adata = sc.read_10x_h5(h5file, gex_only=False). mdata = MuData(adata). ```. should also work, and this is roughly what muon does. (Thanks for tagging me, @adamgayoso!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:57,performance,load,load,57,"Hey @kchl5 and @vitkl,. Muon (`mu.read_10x_h5()`) should load it correctly if the `feature_types` value for the gRNAs is different from the one for the genes. As they are missing, I assume it is. Moreover, just in case you're interested, splitting by `feature_types` is even [a feature](https://github.com/scverse/mudata/blob/4d3b5f4e6039b4a31519584db5461a5809741dce/mudata/_core/mudata.py#L88) of the `MuData` initialiser, so running . ```py. adata = sc.read_10x_h5(h5file, gex_only=False). mdata = MuData(adata). ```. should also work, and this is roughly what muon does. (Thanks for tagging me, @adamgayoso!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:568,reliability,doe,does,568,"Hey @kchl5 and @vitkl,. Muon (`mu.read_10x_h5()`) should load it correctly if the `feature_types` value for the gRNAs is different from the one for the genes. As they are missing, I assume it is. Moreover, just in case you're interested, splitting by `feature_types` is even [a feature](https://github.com/scverse/mudata/blob/4d3b5f4e6039b4a31519584db5461a5809741dce/mudata/_core/mudata.py#L88) of the `MuData` initialiser, so running . ```py. adata = sc.read_10x_h5(h5file, gex_only=False). mdata = MuData(adata). ```. should also work, and this is roughly what muon does. (Thanks for tagging me, @adamgayoso!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:87,energy efficiency,current,current,87,"For your second question, @vitkl, I guess you could suggest a new issue to improve the current documentation so that this is specified in the description of [`sc.read_10x_h5`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_10x_h5.html). It is documented at the level of function parameters at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:125,interoperability,specif,specified,125,"For your second question, @vitkl, I guess you could suggest a new issue to improve the current documentation so that this is specified in the description of [`sc.read_10x_h5`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_10x_h5.html). It is documented at the level of function parameters at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:294,modifiability,paramet,parameters,294,"For your second question, @vitkl, I guess you could suggest a new issue to improve the current documentation so that this is specified in the description of [`sc.read_10x_h5`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_10x_h5.html). It is documented at the level of function parameters at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:95,usability,document,documentation,95,"For your second question, @vitkl, I guess you could suggest a new issue to improve the current documentation so that this is specified in the description of [`sc.read_10x_h5`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_10x_h5.html). It is documented at the level of function parameters at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2398:258,usability,document,documented,258,"For your second question, @vitkl, I guess you could suggest a new issue to improve the current documentation so that this is specified in the description of [`sc.read_10x_h5`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_10x_h5.html). It is documented at the level of function parameters at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398
https://github.com/scverse/scanpy/issues/2402:56,availability,error,error,56,@mmarwaosman what was the fix. I am getting the similar error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402
https://github.com/scverse/scanpy/issues/2402:56,performance,error,error,56,@mmarwaosman what was the fix. I am getting the similar error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402
https://github.com/scverse/scanpy/issues/2402:56,safety,error,error,56,@mmarwaosman what was the fix. I am getting the similar error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402
https://github.com/scverse/scanpy/issues/2402:56,usability,error,error,56,@mmarwaosman what was the fix. I am getting the similar error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402
https://github.com/scverse/scanpy/issues/2404:92,performance,time,time,92,Duplicate https://github.com/scverse/scanpy/issues/2359. Please search existing issues next time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2404
https://github.com/scverse/scanpy/pull/2409:503,availability,error,error,503,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:509,availability,state,states,509,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:85,deployability,pipelin,pipeline,85,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:85,integrability,pipelin,pipeline,85,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:509,integrability,state,states,509,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:503,performance,error,error,503,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:503,safety,error,error,503,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:551,safety,test,tests,551,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:551,testability,test,tests,551,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:503,usability,error,error,503,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:668,usability,help,help,668,"Hi @ivirshup, . I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) . We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:101,testability,plan,planned,101,Please ignore the scrublet issue. It's unrelated and we'll fix it asap. Thank you very much for the (planned) contribution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:76,deployability,updat,updates,76,"Hi @Zethson,. Thank you so much for the response. Please let us know if any updates/changes are required to be done from our side in the above contribution, we will do them at the earliest. Also, please let us know if we need to do anything else for fixing the issues in the PR at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:76,safety,updat,updates,76,"Hi @Zethson,. Thank you so much for the response. Please let us know if any updates/changes are required to be done from our side in the above contribution, we will do them at the earliest. Also, please let us know if we need to do anything else for fixing the issues in the PR at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:76,security,updat,updates,76,"Hi @Zethson,. Thank you so much for the response. Please let us know if any updates/changes are required to be done from our side in the above contribution, we will do them at the earliest. Also, please let us know if we need to do anything else for fixing the issues in the PR at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:101,performance,time,time,101,@Sakshi-2797 please have some patience since we're all busy at the moment. It might take us a bit of time to get back to you and your teams PRs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/pull/2409:134,security,team,teams,134,@Sakshi-2797 please have some patience since we're all busy at the moment. It might take us a bit of time to get back to you and your teams PRs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409
https://github.com/scverse/scanpy/issues/2411:59,deployability,instal,installing,59,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:83,deployability,version,version,83,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:136,deployability,instal,install,136,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:191,deployability,version,version,191,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:83,integrability,version,version,83,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:191,integrability,version,version,191,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:199,interoperability,specif,specification,199,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:83,modifiability,version,version,83,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:191,modifiability,version,version,191,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2411:3,usability,user,users,3,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```. $ pip install 'matplotlib<3.7'. ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411
https://github.com/scverse/scanpy/issues/2413:143,availability,mainten,maintenance,143,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:24,deployability,version,version,24,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:95,deployability,version,version,95,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:250,deployability,infrastructur,infrastructure,250,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:287,deployability,version,versions,287,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:331,deployability,infrastructur,infrastructure,331,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:571,deployability,version,version,571,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:155,energy efficiency,schedul,schedule,155,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:24,integrability,version,version,24,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:95,integrability,version,version,95,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:287,integrability,version,versions,287,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:571,integrability,version,version,571,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:24,modifiability,version,version,24,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:95,modifiability,version,version,95,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:287,modifiability,version,versions,287,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:571,modifiability,version,version,571,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:155,performance,schedul,schedule,155,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:143,reliability,mainten,maintenance,143,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:265,reliability,doe,doesn,265,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:16,usability,minim,minimum,16,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:87,usability,minim,minimum,87,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:170,usability,support,support,170,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/issues/2413:622,usability,user,user,622,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit. 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413
https://github.com/scverse/scanpy/pull/2414:70,integrability,abstract,abstract,70,"considering that AxesSubplot defines no methods at all (much less any abstract methods), this does seem like the right solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2414
https://github.com/scverse/scanpy/pull/2414:70,modifiability,abstract,abstract,70,"considering that AxesSubplot defines no methods at all (much less any abstract methods), this does seem like the right solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2414
https://github.com/scverse/scanpy/pull/2414:94,reliability,doe,does,94,"considering that AxesSubplot defines no methods at all (much less any abstract methods), this does seem like the right solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2414
https://github.com/scverse/scanpy/issues/2418:558,interoperability,specif,specific,558,"* I've wrote a solution, but this causes trouble if the dendogram is used afterwards, because the dendogram will be built only using the relevant categories, and a later function, lets say `sc.pl.stacked_violin` is expecting all of the categories, including the unused/. Concrete possible solution (starting line 122 of `_dendogram.py`:. ```. categorical.name = ""_"".join(groupby). #suggested edit:. categorical = categorical.cat.remove_unused_categories(). rep_df.set_index(categorical, inplace=True). ```. This will also not solve the issue for cases where specific var_names are used, cause then `AnnData._prepare_dataframe` is used. I think there might be a need to a change of approach towards categorical columns generally in the ecosystem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2418
https://github.com/scverse/scanpy/issues/2423:151,modifiability,pac,package,151,@alam-shahul. Thank you very much! I did that as abbreviation of single cell to distinguish from single nuclei dataset and completely forgot about the package thing... . Thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2423
https://github.com/scverse/scanpy/issues/2423:123,safety,compl,completely,123,@alam-shahul. Thank you very much! I did that as abbreviation of single cell to distinguish from single nuclei dataset and completely forgot about the package thing... . Thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2423
https://github.com/scverse/scanpy/issues/2423:123,security,compl,completely,123,@alam-shahul. Thank you very much! I did that as abbreviation of single cell to distinguish from single nuclei dataset and completely forgot about the package thing... . Thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2423
https://github.com/scverse/scanpy/pull/2424:214,deployability,resourc,resources,214,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:512,deployability,resourc,resources,512,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:214,energy efficiency,resourc,resources,214,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:512,energy efficiency,resourc,resources,512,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:130,interoperability,coordinat,coordinates,130,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:263,interoperability,standard,standard-,263,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:560,interoperability,standard,standard-,560,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:214,performance,resourc,resources,214,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:512,performance,resourc,resources,512,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:214,safety,resourc,resources,214,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:512,safety,resourc,resources,512,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:214,testability,resourc,resources,214,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:512,testability,resourc,resources,512,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:233,usability,mous,mouse-brain-section-coronal-,233,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:362,usability,user,user-images,362,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:537,usability,mous,mouse-olfactory-bulb-,537,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:655,usability,user,user-images,655,10x Visium data from both space ranger 1.0.0 and 2.0.0 can be read in using ```sc.read_visium()``` and plotted using squidpy with coordinates in correct order. space ranger 1.0.0 [data](https://www.10xgenomics.com/resources/datasets/mouse-brain-section-coronal-1-standard-1-0-0) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_1](https://user-images.githubusercontent.com/64135338/220176480-bdbc5392-debb-43ca-a8b3-fb0b47eff00a.png). space ranger 2.0.0 [data](https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1) from 10x plotted with squidpy's ```spatial_scatter()```:. ![spaceranger_2](https://user-images.githubusercontent.com/64135338/220176572-7d00bb30-e99a-440f-aa87-5ed8fae17187.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:82,deployability,releas,release,82,"> @giovp feel free to approve and merge. One request first:. > . > Can this get a release note? for sure, @LLehner could you? think he's is on holiday until next week",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:38,reliability,Doe,Doesn,38,"hi @LLehner, why is this `header=1`?. Doesn’t that mean “the second line of data”? Shouldn’t it be `header=0`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:402,interoperability,coordinat,coordinates,402,"Folks. Forgive my ignorance if I am completely wrong but it seems read_visium still reads it wrong. ~https://github.com/scverse/scanpy/blob/edd613026cd5991baf92c8308b5ee2375089adc8/scanpy/readwrite.py#L466C1-L473~. another edit:. https://github.com/scverse/scanpy/blob/89804c26bcbdde4449dfe98b3193a9d28c2a0e2f/scanpy/readwrite.py#L466-L482. Basically line 476 is fixes the switch and makes the barcode coordinates in (x,y). I apologise for the confusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:36,safety,compl,completely,36,"Folks. Forgive my ignorance if I am completely wrong but it seems read_visium still reads it wrong. ~https://github.com/scverse/scanpy/blob/edd613026cd5991baf92c8308b5ee2375089adc8/scanpy/readwrite.py#L466C1-L473~. another edit:. https://github.com/scverse/scanpy/blob/89804c26bcbdde4449dfe98b3193a9d28c2a0e2f/scanpy/readwrite.py#L466-L482. Basically line 476 is fixes the switch and makes the barcode coordinates in (x,y). I apologise for the confusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:36,security,compl,completely,36,"Folks. Forgive my ignorance if I am completely wrong but it seems read_visium still reads it wrong. ~https://github.com/scverse/scanpy/blob/edd613026cd5991baf92c8308b5ee2375089adc8/scanpy/readwrite.py#L466C1-L473~. another edit:. https://github.com/scverse/scanpy/blob/89804c26bcbdde4449dfe98b3193a9d28c2a0e2f/scanpy/readwrite.py#L466-L482. Basically line 476 is fixes the switch and makes the barcode coordinates in (x,y). I apologise for the confusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:263,modifiability,maintain,maintaing,263,"With https://github.com/scverse/spatialdata-io/pull/102 we could consider replacing `scanpy.read_visium` with. ```python. def read_visium(*args, **kwargs): . import spatialdata_io. return spatialdata_io.visium(*args, **kwargs).to_legacy_anndata(). ```. and avoid maintaing the annoying spaceranger output parsing in multiple locations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:257,safety,avoid,avoid,257,"With https://github.com/scverse/spatialdata-io/pull/102 we could consider replacing `scanpy.read_visium` with. ```python. def read_visium(*args, **kwargs): . import spatialdata_io. return spatialdata_io.visium(*args, **kwargs).to_legacy_anndata(). ```. and avoid maintaing the annoying spaceranger output parsing in multiple locations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/pull/2424:263,safety,maintain,maintaing,263,"With https://github.com/scverse/spatialdata-io/pull/102 we could consider replacing `scanpy.read_visium` with. ```python. def read_visium(*args, **kwargs): . import spatialdata_io. return spatialdata_io.visium(*args, **kwargs).to_legacy_anndata(). ```. and avoid maintaing the annoying spaceranger output parsing in multiple locations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424
https://github.com/scverse/scanpy/issues/2425:158,deployability,automat,automation-rules,158,"ok, thanks! it should be relatively easy. The admin of the readthedocs project just needs to add an [_automation rule_](https://docs.readthedocs.io/en/stable/automation-rules.html) in the settings for the project",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425
https://github.com/scverse/scanpy/issues/2425:158,testability,automat,automation-rules,158,"ok, thanks! it should be relatively easy. The admin of the readthedocs project just needs to add an [_automation rule_](https://docs.readthedocs.io/en/stable/automation-rules.html) in the settings for the project",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425
https://github.com/scverse/scanpy/issues/2425:13,usability,close,close,13,I'm going to close this now because it seems like the issue has been addressed. Thanks for doing that! ![image](https://github.com/user-attachments/assets/bb53ffac-48b2-4f48-80b0-c4d8ea6627f8),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425
https://github.com/scverse/scanpy/issues/2425:131,usability,user,user-attachments,131,I'm going to close this now because it seems like the issue has been addressed. Thanks for doing that! ![image](https://github.com/user-attachments/assets/bb53ffac-48b2-4f48-80b0-c4d8ea6627f8),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425
https://github.com/scverse/scanpy/issues/2427:71,deployability,releas,released,71,Thanks! you can just specify `n_top_genes=2000` or so until the fix is released,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427
https://github.com/scverse/scanpy/issues/2427:21,interoperability,specif,specify,21,Thanks! you can just specify `n_top_genes=2000` or so until the fix is released,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427
https://github.com/scverse/scanpy/issues/2430:75,usability,user,user-images,75,"when I run . sc.pl.pca(adata). its ok ,show me a grey graph. ![下载](https://user-images.githubusercontent.com/125791298/221105677-e56599f5-68e2-4e52-a92c-13bdb1414acc.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2430:63,availability,error,error,63,"but when i run . sc.pl.pca(adata, color='CST3'). it return the error,how could i fix it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2430:63,performance,error,error,63,"but when i run . sc.pl.pca(adata, color='CST3'). it return the error,how could i fix it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2430:63,safety,error,error,63,"but when i run . sc.pl.pca(adata, color='CST3'). it return the error,how could i fix it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2430:63,usability,error,error,63,"but when i run . sc.pl.pca(adata, color='CST3'). it return the error,how could i fix it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2430:7,usability,help,help,7,anyone help？please,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430
https://github.com/scverse/scanpy/issues/2431:79,usability,tool,tools,79,No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:233,availability,error,error,233,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:467,availability,cluster,clustering,467,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:467,deployability,cluster,clustering,467,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:233,performance,error,error,233,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:233,safety,error,error,233,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:81,usability,tool,tools,81,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:233,usability,error,error,233,"> No worries. I meant here: https://github.com/scverse/scanpy/blob/master/scanpy/tools/_leiden.py. Sorry I still don't really know what to do. After the RandomState object is passed to leiden, it passes it to igraph, which raises an error because igraph wants an int to initialize [it's own rng system](https://igraph.org/c/doc/igraph-Random.html). It seems to me like there's no easy way to fix this, no? Maybe I should just give it an int instead? I'd just like my clustering to be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2431:34,usability,person,personally,34,Closing this as it appears (to me personally) that not much can actually be done from the scanpy side and this is more of a leidenalg/igraph issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2431
https://github.com/scverse/scanpy/issues/2432:56,availability,slo,slow,56,"Hi, thanks for your interest in scanpy! Sorry for being slow here! Would it be possible that you can add a minimal reproducible example so someone could generate adata objects (with some dummy data) in the style you are using them so we could check this? Sidenote: from a first impression you are using `adata.obs_names_make_unique`: you might want to double check to _call_ the function here by adding `()` at the end of the line: `adata.obs_names_make_unique()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432
https://github.com/scverse/scanpy/issues/2432:56,reliability,slo,slow,56,"Hi, thanks for your interest in scanpy! Sorry for being slow here! Would it be possible that you can add a minimal reproducible example so someone could generate adata objects (with some dummy data) in the style you are using them so we could check this? Sidenote: from a first impression you are using `adata.obs_names_make_unique`: you might want to double check to _call_ the function here by adding `()` at the end of the line: `adata.obs_names_make_unique()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432
https://github.com/scverse/scanpy/issues/2432:107,usability,minim,minimal,107,"Hi, thanks for your interest in scanpy! Sorry for being slow here! Would it be possible that you can add a minimal reproducible example so someone could generate adata objects (with some dummy data) in the style you are using them so we could check this? Sidenote: from a first impression you are using `adata.obs_names_make_unique`: you might want to double check to _call_ the function here by adding `()` at the end of the line: `adata.obs_names_make_unique()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432
https://github.com/scverse/scanpy/issues/2436:499,integrability,Messag,Message,499,"Hi Lukas,. I am sorry, but what is a PR? Best. Jin. ᐧ. On Fri, Mar 10, 2023 at 5:02 AM Lukas Heumos ***@***.***>. wrote:. > Would you be willing to file a PR for this? Thank you! >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:499,interoperability,Messag,Message,499,"Hi Lukas,. I am sorry, but what is a PR? Best. Jin. ᐧ. On Fri, Mar 10, 2023 at 5:02 AM Lukas Heumos ***@***.***>. wrote:. > Would you be willing to file a PR for this? Thank you! >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:380,security,auth,auth,380,"Hi Lukas,. I am sorry, but what is a PR? Best. Jin. ᐧ. On Fri, Mar 10, 2023 at 5:02 AM Lukas Heumos ***@***.***>. wrote:. > Would you be willing to file a PR for this? Thank you! >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:479,security,auth,authored,479,"Hi Lukas,. I am sorry, but what is a PR? Best. Jin. ᐧ. On Fri, Mar 10, 2023 at 5:02 AM Lukas Heumos ***@***.***>. wrote:. > Would you be willing to file a PR for this? Thank you! >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:668,integrability,Messag,Message,668,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since. I am not really using it. ᐧ. On Mon, Mar 13, 2023 at 5:10 AM Lukas Heumos ***@***.***>. wrote:. > A pull request that fixes this :). >. > See https://scanpy.readthedocs.io/en/stable/dev/index.html. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:668,interoperability,Messag,Message,668,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since. I am not really using it. ᐧ. On Mon, Mar 13, 2023 at 5:10 AM Lukas Heumos ***@***.***>. wrote:. > A pull request that fixes this :). >. > See https://scanpy.readthedocs.io/en/stable/dev/index.html. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:549,security,auth,auth,549,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since. I am not really using it. ᐧ. On Mon, Mar 13, 2023 at 5:10 AM Lukas Heumos ***@***.***>. wrote:. > A pull request that fixes this :). >. > See https://scanpy.readthedocs.io/en/stable/dev/index.html. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2436:648,security,auth,authored,648,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since. I am not really using it. ᐧ. On Mon, Mar 13, 2023 at 5:10 AM Lukas Heumos ***@***.***>. wrote:. > A pull request that fixes this :). >. > See https://scanpy.readthedocs.io/en/stable/dev/index.html. >. > —. > Reply to this email directly, view it on GitHub. > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436
https://github.com/scverse/scanpy/issues/2438:23,deployability,instal,installed,23,I uninstalled and then installed scanpy. This resolves the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2438
https://github.com/scverse/scanpy/issues/2440:29,availability,error,error,29,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:201,availability,error,error,201,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:458,availability,error,error,458,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1386,deployability,log,logreg,1386,".pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1400,deployability,log,logg,1400,"s_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1493,deployability,log,logarithmize,1493,"ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1946,deployability,log,log,1946,"raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_ev",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2046,deployability,log,logging,2046,"ayer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2144,deployability,version,version,2144,"pby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2160,deployability,version,version,2160,"3 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2220,deployability,instal,install,2220," 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:3519,deployability,updat,updated,3519,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2144,integrability,version,version,2144,"pby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2160,integrability,version,version,2160,"3 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:40,interoperability,specif,specifically,40,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:256,interoperability,specif,specifically,256,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2866,interoperability,platform,platformdirs,2866,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:858,modifiability,pac,packages,858,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1049,modifiability,layer,layer,1049," in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.loggin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1312,modifiability,layer,layer,1312,"roups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made fr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1603,modifiability,pac,packages,1603,"ck (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1724,modifiability,layer,layer,1724,"e='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2144,modifiability,version,version,2144,"pby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2160,modifiability,version,version,2160,"3 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2534,modifiability,deco,decorator,2534,"es_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2771,modifiability,pac,packaging,2771,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:29,performance,error,error,29,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:201,performance,error,error,201,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:458,performance,error,error,458,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:29,safety,error,error,29,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:201,safety,error,error,201,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:458,safety,error,error,458,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1386,safety,log,logreg,1386,".pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1400,safety,log,logg,1400,"s_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1493,safety,log,logarithmize,1493,"ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1946,safety,log,log,1946,"raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_ev",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2046,safety,log,logging,2046,"ayer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:3519,safety,updat,updated,3519,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1386,security,log,logreg,1386,".pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1400,security,log,logg,1400,"s_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1493,security,log,logarithmize,1493,"ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1946,security,log,log,1946,"raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_ev",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2046,security,log,logging,2046,"ayer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:3499,security,Session,Session,3499,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:3519,security,updat,updated,3519,"~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.2.0. tornado 6.3.2. traitlets 5.9.0. wcwidth 0.2.6. zmq 25.1.0. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.5-arm64-i386-64bit. -----. Session information updated at 2023-07-26 10:47. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:600,testability,Trace,Traceback,600,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1386,testability,log,logreg,1386,".pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1400,testability,log,logg,1400,"s_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1493,testability,log,logarithmize,1493,"ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1946,testability,log,log,1946,"raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_ev",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2046,testability,log,logging,2046,"ayer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:2116,testability,verif,verification,2116,"params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18.2. joblib 1.3.1. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.9.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:29,usability,error,error,29,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:201,usability,error,error,201,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:458,usability,error,error,458,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:874,usability,tool,tools,874,"I have also encountered this error, but specifically in the scanpy tutorial outlined [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). That should make reproducibility easier. The error occurs under the _Finding Marker Genes_ heading, specifically the following line:. ```py. sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. With an error output of the following:. ```pytb. ranking genes. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2440:1619,usability,tool,tools,1619," call last). Cell In[49], line 1. ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File ~/Desktop/data/env/lib/python3.11/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. I've pasted the output of scanpy.logging.print_versions() details below as requested, which includes a verification of used scanpy version (latest version, 1.9.3). It may not be important, but I also had to install `leidenalg` manually in the middle of the tutorial. That's the only deviation I made from the original tutorial. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.9.0. igraph 0.10.6. ipykernel 6.25.0. jedi 0.18",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2440
https://github.com/scverse/scanpy/issues/2442:59,deployability,updat,updates,59,I have the same issue. I am eager to hear if there are any updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2442
https://github.com/scverse/scanpy/issues/2442:59,safety,updat,updates,59,I have the same issue. I am eager to hear if there are any updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2442
https://github.com/scverse/scanpy/issues/2442:59,security,updat,updates,59,I have the same issue. I am eager to hear if there are any updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2442
https://github.com/scverse/scanpy/issues/2443:7,integrability,sub,submit,7,Please submit this issue at scvelo,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2443
https://github.com/scverse/scanpy/issues/2444:40,integrability,sub,submit,40,This sounds great! Please go for it and submit a minimal draft PR. Then we can iterate over it,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:49,usability,minim,minimal,49,This sounds great! Please go for it and submit a minimal draft PR. Then we can iterate over it,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:305,integrability,sub,subclasses,305,"Example of creating dot plot, which to my mind was a main missing feature. This should go great with `sc.get.aggregate`: https://github.com/Marsilea-viz/marsilea/issues/27#issuecomment-2022353426. Ideally, I think we could switch to marsilea for many plots for scanpy 2.0, replacing our existing BasePlot subclasses. This definitely requires more exploration first though. @Mr-Milk, wdyt about including a [how-to example](https://scanpy.readthedocs.io/en/stable/how-to/index.html) in scanpy for Marsilea? cc: @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,deployability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,integrability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,interoperability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,modifiability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,reliability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,security,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:224,testability,integr,integrating,224,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:441,deployability,continu,continuity,441,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:522,deployability,API,API,522,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:522,integrability,API,API,522,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:522,interoperability,API,API,522,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:381,modifiability,maintain,maintained,381,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:423,modifiability,pac,package,423,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:494,modifiability,maintain,maintained,494,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:229,performance,perform,performance,229,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:381,safety,maintain,maintained,381,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:494,safety,maintain,maintained,494,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:229,usability,perform,performance,229,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:261,usability,tool,tools,261,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:333,usability,tool,tools,333,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:471,usability,tool,tool,471,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:572,availability,mainten,maintenance,572,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:183,deployability,API,APIs,183,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,deployability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,deployability,API,APIs,303,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:615,deployability,API,API,615,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:685,deployability,API,API,685,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:510,energy efficiency,reduc,reduce,510,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:130,integrability,compon,components,130,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:183,integrability,API,APIs,183,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,integrability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,integrability,API,APIs,303,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:615,integrability,API,API,615,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:685,integrability,API,API,685,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:757,integrability,transform,transformation,757,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:130,interoperability,compon,components,130,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:183,interoperability,API,APIs,183,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,interoperability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,interoperability,API,APIs,303,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:615,interoperability,API,API,615,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:685,interoperability,API,API,685,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:705,interoperability,specif,specifically,705,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:757,interoperability,transform,transformation,757,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:130,modifiability,compon,components,130,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,modifiability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,reliability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:572,reliability,mainten,maintenance,572,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:871,reliability,doe,doesn,871,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:531,safety,compl,complexities,531,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,security,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:347,security,expos,expose,347,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:496,security,sign,significantly,496,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:531,security,compl,complexities,531,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:593,security,Expos,Exposing,593,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:247,testability,integr,integration,247,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:662,testability,simul,simulates,662,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:169,usability,visual,visualization,169,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:289,usability,visual,visualization,289,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:370,usability,user,user,370,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:636,usability,visual,visualization,636,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:807,usability,user,user,807,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:840,usability,visual,visualization,840,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,deployability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:458,deployability,API,API,458,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,deployability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,deployability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,integrability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:458,integrability,API,API,458,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,integrability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,integrability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,interoperability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:458,interoperability,API,API,458,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,interoperability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,interoperability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,modifiability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,modifiability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,modifiability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,reliability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:345,reliability,doe,doesn,345,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,reliability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,reliability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,security,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,security,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,security,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:303,testability,integr,integrating,303,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:466,testability,integr,integration,466,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:516,testability,integr,integration,516,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since. > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,deployability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,integrability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,interoperability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,modifiability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,reliability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,security,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2444:75,testability,integr,integration,75,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444
https://github.com/scverse/scanpy/issues/2445:19,deployability,instal,install,19,resolved via . pip install 'matplotlib<3.7',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445
https://github.com/scverse/scanpy/issues/2445:28,deployability,releas,releases,28,This is also fixed by newer releases of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445
https://github.com/scverse/scanpy/pull/2447:132,deployability,version,version,132,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:132,integrability,version,version,132,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:132,modifiability,version,version,132,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:2,usability,person,personally,2,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:39,usability,user,users,39,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:151,usability,support,supporting,151,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:206,usability,minim,minimum,206,I personally think that we have enough users that are still on 3.8 so I'd keep it at 3.8? Are you now trying to say that the lowest version that we're supporting is 3.7 or 3.9? If we agree on 3.8 being the minimum I'd just keep the PR as it is and merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,deployability,version,version,49,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:90,deployability,version,version,90,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,integrability,version,version,49,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:90,integrability,version,version,90,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,modifiability,version,version,49,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:90,modifiability,version,version,90,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:75,usability,minim,minimum,75,I'm saying either:. * Don't change the CI python version. * Or: Change the minimum python version for the project to 3.8 as well. https://github.com/scverse/scanpy/blob/0692ef9ea30335b95f7e7f9aab7be856469d9f35/pyproject.toml#L13,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,deployability,version,version,49,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,integrability,version,version,49,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:87,interoperability,compatib,compatibility,87,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:49,modifiability,version,version,49,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:148,safety,review,review,148,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:148,testability,review,review,148,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/pull/2447:34,usability,minim,minimum,34,"Got it, thanks. I'll increase the minimum Python version to 3.8. I'll also check which compatibility code we can remove later. I'll ask for another review then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2447
https://github.com/scverse/scanpy/issues/2449:15,availability,error,error,15,That's an HTTP error. You need to report this to the data hoster aka EBI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449
https://github.com/scverse/scanpy/issues/2449:15,performance,error,error,15,That's an HTTP error. You need to report this to the data hoster aka EBI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449
https://github.com/scverse/scanpy/issues/2449:15,safety,error,error,15,That's an HTTP error. You need to report this to the data hoster aka EBI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449
https://github.com/scverse/scanpy/issues/2449:15,usability,error,error,15,That's an HTTP error. You need to report this to the data hoster aka EBI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449
https://github.com/scverse/scanpy/pull/2457:230,availability,avail,available,230,"Hi @Zethson ,. The reason why we introduced flavors here is that we wanted the traditional implementation to be present in case anyone wanted to use it. We just introduced our implementation as a faster alternative to the already available one. In case replacing the code is required, we can do that as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:230,reliability,availab,available,230,"Hi @Zethson ,. The reason why we introduced flavors here is that we wanted the traditional implementation to be present in case anyone wanted to use it. We just introduced our implementation as a faster alternative to the already available one. In case replacing the code is required, we can do that as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:230,safety,avail,available,230,"Hi @Zethson ,. The reason why we introduced flavors here is that we wanted the traditional implementation to be present in case anyone wanted to use it. We just introduced our implementation as a faster alternative to the already available one. In case replacing the code is required, we can do that as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:230,security,availab,available,230,"Hi @Zethson ,. The reason why we introduced flavors here is that we wanted the traditional implementation to be present in case anyone wanted to use it. We just introduced our implementation as a faster alternative to the already available one. In case replacing the code is required, we can do that as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1838,availability,error,errors,1838,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2017,availability,error,errors,2017,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2413,availability,error,error,2413,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:589,deployability,scale,scale,589,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1421,deployability,Fail,Failed,1421," Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1445,deployability,pipelin,pipeline,1445," https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2037,deployability,Fail,Failed,2037,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2061,deployability,pipelin,pipeline,2061,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:589,energy efficiency,scale,scale,589,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1139,energy efficiency,core,core,1139,"________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2012,energy efficiency,core,core,2012,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2621,energy efficiency,core,core,2621,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:911,integrability,wrap,wrapper,911,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1445,integrability,pipelin,pipeline,1445," https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2061,integrability,pipelin,pipeline,2061,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:911,interoperability,wrapper,wrapper,911,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1778,interoperability,specif,specified,1778,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:259,modifiability,paramet,parametrize,259,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:589,modifiability,scal,scale,589,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1124,modifiability,pac,packages,1124,"e_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2606,modifiability,pac,packages,2606,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:589,performance,scale,scale,589,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1838,performance,error,errors,1838,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2017,performance,error,errors,2017,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2347,performance,time,time,2347,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2352,performance,time,time,2352,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2413,performance,error,error,2413,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1421,reliability,Fail,Failed,1421," Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2037,reliability,Fail,Failed,2037,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:621,safety,test,tests,621,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1731,safety,Except,Exception,1731,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1838,safety,error,errors,1838,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2017,safety,error,errors,2017,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2413,safety,error,error,2413,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2715,safety,test,test,2715,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:520,testability,assert,assert,520,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:621,testability,test,tests,621,"Unfortunately, I run into. ```. __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2715,testability,test,test,2715,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1750,usability,help,help,1750,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1838,usability,error,errors,1838,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2017,usability,error,errors,2017,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2413,usability,error,error,2413,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. return dispatch(args[0].__class__)(*args, **kw). scanpy/preprocessing/_simple.py:888: in scale_anndata. X, adata.var[""mean""], adata.var[""std""] = do_scale(. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. error_rewrite(e, 'typing'). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). issue_type = 'typing'. def error_rewrite(e, issue_type):. """""". Rewrite and raise Exception `e` with help supplied based on the. specified issue_type. """""". if config.SHOW_HELP:. help_msg = errors.error_extras[issue_type]. e.patch_message('\n'.join((str(e).rstrip(), help_msg))). if config.FULL_TRACEBACKS:. raise e. else:. > raise e.with_traceback(None). E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). E non-precise type pyobject. E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). E . E File ""scanpy/preprocessing/_simple.py"", line 763:. E def do_scale(X, maxv, nthr):. E <source elided>. E # t0= time.time(). E s = np.zeros((nthr, X.shape[1])). E ^ . E . E This error may have been caused by the following argument(s):. E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1924,availability,error,errors,1924,"/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2115,availability,error,errors,2115,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2533,availability,error,error,2533,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:627,deployability,scale,scale,627,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1487,deployability,Fail,Failed,1487,"github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1511,deployability,pipelin,pipeline,1511,"a/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2135,deployability,Fail,Failed,2135,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2159,deployability,pipelin,pipeline,2159,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2977,deployability,scale,scale,2977,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:627,energy efficiency,scale,scale,627,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1195,energy efficiency,core,core,1195,"__________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2110,energy efficiency,core,core,2110,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2749,energy efficiency,core,core,2749,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2977,energy efficiency,scale,scale,2977,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:959,integrability,wrap,wrapper,959,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1511,integrability,pipelin,pipeline,1511,"a/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2159,integrability,pipelin,pipeline,2159,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:959,interoperability,wrapper,wrapper,959,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1858,interoperability,specif,specified,1858," _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Ze",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:281,modifiability,paramet,parametrize,281,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:627,modifiability,scal,scale,627,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1180,modifiability,pac,packages,1180,"_______________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2734,modifiability,pac,packages,2734,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2977,modifiability,scal,scale,2977,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:627,performance,scale,scale,627,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1924,performance,error,errors,1924,"/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2115,performance,error,errors,2115,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2459,performance,time,time,2459,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2464,performance,time,time,2464,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2533,performance,error,error,2533,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2977,performance,scale,scale,2977,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1487,reliability,Fail,Failed,1487,"github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2135,reliability,Fail,Failed,2135,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:665,safety,test,tests,665,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1809,safety,Except,Exception,1809,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1924,safety,error,errors,1924,"/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2115,safety,error,errors,2115,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2533,safety,error,error,2533,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2851,safety,test,test,2851,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:554,testability,assert,assert,554,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:665,testability,test,tests,665,"> Unfortunately, I run into. > . > ```. > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. > . > flavor = 'use_fastpp'. > . > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). > def test_scale(flavor):. > adata = pbmc68k_reduced(). > adata.X = adata.raw.X. > v = adata[:, 0 : adata.shape[1] // 2]. > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. > assert v.is_view. > with pytest.warns(Warning, match=""view""):. > > sc.pp.scale(v, flavor=flavor). > . > scanpy/tests/test_preprocessing.py:127: . > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2851,testability,test,test,2851,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1828,usability,help,help,1828,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flav",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:1924,usability,error,errors,1924,"/python3.9/functools.py:888: in wrapper. > return dispatch(args[0].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2115,usability,error,errors,2115,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:2533,usability,error,error,2533,"].__class__)(*args, **kw). > scanpy/preprocessing/_simple.py:888: in scale_anndata. > X, adata.var[""mean""], adata.var[""std""] = do_scale(. > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args. > error_rewrite(e, 'typing'). > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. > . > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'). > issue_type = 'typing'. > . > def error_rewrite(e, issue_type):. > """""". > Rewrite and raise Exception `e` with help supplied based on the. > specified issue_type. > """""". > if config.SHOW_HELP:. > help_msg = errors.error_extras[issue_type]. > e.patch_message('\n'.join((str(e).rstrip(), help_msg))). > if config.FULL_TRACEBACKS:. > raise e. > else:. > > raise e.with_traceback(None). > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). > E non-precise type pyobject. > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763). > E . > E File ""scanpy/preprocessing/_simple.py"", line 763:. > E def do_scale(X, maxv, nthr):. > E <source elided>. > E # t0= time.time(). > E s = np.zeros((nthr, X.shape[1])). > E ^ . > E . > E This error may have been caused by the following argument(s):. > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. > . > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError. > ```. > . > When trying to use the new flavor with the existing test. Hi @Zethson ,. We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:677,availability,error,errors,677,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:387,deployability,scale,scale,387,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:579,deployability,fail,fails,579,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:598,deployability,FAIL,FAILED,598,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:697,deployability,Fail,Failed,697,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:721,deployability,pipelin,pipeline,721,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:7,energy efficiency,adapt,adapt,7,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:387,energy efficiency,scale,scale,387,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:672,energy efficiency,core,core,672,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:7,integrability,adapt,adapt,7,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:721,integrability,pipelin,pipeline,721,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:7,interoperability,adapt,adapt,7,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:7,modifiability,adapt,adapt,7,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:59,modifiability,paramet,parametrize,59,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:387,modifiability,scal,scale,387,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:387,performance,scale,scale,387,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:677,performance,error,errors,677,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:579,reliability,fail,fails,579,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:598,reliability,FAIL,FAILED,598,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:697,reliability,Fail,Failed,697,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:31,safety,test,test,31,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:612,safety,test,tests,612,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:677,safety,error,errors,677,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:31,testability,test,test,31,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:320,testability,assert,assert,320,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:412,testability,assert,assert,412,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:612,testability,test,tests,612,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:677,usability,error,errors,677,"Please adapt the corresponding test to:. ```. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). def test_scale(flavor):. adata = pbmc68k_reduced(). adata.X = adata.raw.X. v = adata[:, 0 : adata.shape[1] // 2]. # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965. assert v.is_view. with pytest.warns(Warning, match=""view""):. sc.pp.scale(v, flavor=flavor). assert not v.is_view. assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01). assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001). ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:28,deployability,fail,failed,28,I pushed to your branch. It failed yesterday while Github was having issues. The test should pass.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:28,reliability,fail,failed,28,I pushed to your branch. It failed yesterday while Github was having issues. The test should pass.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:81,safety,test,test,81,I pushed to your branch. It failed yesterday while Github was having issues. The test should pass.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:81,testability,test,test,81,I pushed to your branch. It failed yesterday while Github was having issues. The test should pass.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:155,deployability,fail,failing,155,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:238,deployability,scale,scale,238,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:334,deployability,fail,fail,334,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:238,energy efficiency,scale,scale,238,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:15,modifiability,paramet,parametrize,15,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:212,modifiability,paramet,parameter,212,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:238,modifiability,scal,scale,238,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:238,performance,scale,scale,238,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:155,reliability,fail,failing,155,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:334,reliability,fail,fail,334,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:109,safety,test,testcase,109,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:323,safety,test,tests,323,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:100,security,modif,modified,100,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:109,testability,test,testcase,109,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:323,testability,test,tests,323,"> @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]). Hi @Zethson ,. I tried running the modified testcase mentioned above , but it seems it is failing because sparse matrix is being passed in it as a parameter. As of now, our scale function is not implemented for the sparse matrices. It is expected that these tests will fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:44,deployability,resourc,resources,44,Sorry for the delay in responding here. Our resources got pulled into other projects. We are looking to make it work with sparse matrices.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:44,energy efficiency,resourc,resources,44,Sorry for the delay in responding here. Our resources got pulled into other projects. We are looking to make it work with sparse matrices.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:44,performance,resourc,resources,44,Sorry for the delay in responding here. Our resources got pulled into other projects. We are looking to make it work with sparse matrices.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:44,safety,resourc,resources,44,Sorry for the delay in responding here. Our resources got pulled into other projects. We are looking to make it work with sparse matrices.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2457:44,testability,resourc,resources,44,Sorry for the delay in responding here. Our resources got pulled into other projects. We are looking to make it work with sparse matrices.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457
https://github.com/scverse/scanpy/pull/2460:29,safety,test,test,29,@Zethson do we really need a test here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:29,testability,test,test,29,@Zethson do we really need a test here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:31,safety,test,test,31,> @Zethson do we really need a test here? Yes!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:31,testability,test,test,31,> @Zethson do we really need a test here? Yes!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:387,deployability,continu,continuous,387,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:468,deployability,fail,fail,468,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:491,deployability,version,version,491,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:512,deployability,fail,fail,512,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:316,energy efficiency,Current,Current,316,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:491,integrability,version,version,491,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:398,modifiability,variab,variables,398,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:491,modifiability,version,version,491,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:409,reliability,Doe,Does,409,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:468,reliability,fail,fail,468,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:512,reliability,fail,fail,512,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:462,safety,Test,Tests,462,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:241,security,modif,modified,241,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:462,testability,Test,Tests,462,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:324,usability,behavi,behaviour,324,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy? 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:42,safety,review,review,42,@flying-sheep Thank you very much for the review and finishing the PR!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:42,testability,review,review,42,@flying-sheep Thank you very much for the review and finishing the PR!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:59,deployability,releas,release,59,Thanks for the fix! Can we include this in the next scanpy release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/pull/2460:32,deployability,releas,release,32,"It will be included in the next release, don't worry :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460
https://github.com/scverse/scanpy/issues/2461:160,interoperability,format,format,160,"Hi,. thanks for your interest in scanpy and for raising your question! Could you provide a minimal reproducible example of how you arrive at a loom file in the format you are trying to read here? :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2461
https://github.com/scverse/scanpy/issues/2461:91,usability,minim,minimal,91,"Hi,. thanks for your interest in scanpy and for raising your question! Could you provide a minimal reproducible example of how you arrive at a loom file in the format you are trying to read here? :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2461
https://github.com/scverse/scanpy/issues/2462:30,deployability,releas,release,30,Yes! Please wait for the next release. Please also checkout squidpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2462
https://github.com/scverse/scanpy/pull/2467:29,safety,review,review,29,"@Zethson forgot to request a review from you, hence the comment. Ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2467
https://github.com/scverse/scanpy/pull/2467:29,testability,review,review,29,"@Zethson forgot to request a review from you, hence the comment. Ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2467
https://github.com/scverse/scanpy/issues/2471:15,deployability,version,versions,15,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:27,deployability,depend,dependencies,27,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:15,integrability,version,versions,15,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:27,integrability,depend,dependencies,27,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:15,modifiability,version,versions,15,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:27,modifiability,depend,dependencies,27,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:27,safety,depend,dependencies,27,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2471:27,testability,depend,dependencies,27,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471
https://github.com/scverse/scanpy/issues/2472:195,deployability,fail,fail,195,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:105,modifiability,exten,extend,105,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:40,reliability,Doe,Does,40,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:195,reliability,fail,fail,195,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:139,safety,test,test,139,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:139,testability,test,test,139,"Hi,. thanks for you interest in scanpy! Does this issue still persist for you? If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:8,usability,close,close,8,"We will close the issue for now, hope that you obtained the expected behaviour :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2472:69,usability,behavi,behaviour,69,"We will close the issue for now, hope that you obtained the expected behaviour :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472
https://github.com/scverse/scanpy/issues/2479:31,usability,command,command,31,I solved the problem with this command: `sudo ln -s /usr/local/bin/g++ /usr/local/bin/gcc++`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2479
https://github.com/scverse/scanpy/issues/2480:6,availability,recov,recover,6,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:105,availability,cluster,clusters,105,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:118,availability,down,downgrading,118,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6,deployability,recov,recover,6,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:105,deployability,cluster,clusters,105,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:140,deployability,version,version,140,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:174,deployability,version,version,174,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:248,deployability,instal,installed,248,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:266,deployability,version,versions,266,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:140,integrability,version,version,140,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:174,integrability,version,version,174,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:266,integrability,version,versions,266,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:140,modifiability,version,version,140,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:174,modifiability,version,version,174,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:258,modifiability,pac,package,258,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:266,modifiability,version,versions,266,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6,reliability,recov,recover,6,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6,safety,recov,recover,6,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6,security,recov,recover,6,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:79,security,ident,identical,79,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:27,usability,behavi,behavior,27,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:317,usability,learn,learn,317,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:64,deployability,Version,Versions,64,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1753,deployability,updat,updated,1753,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:64,integrability,Version,Versions,64,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:908,interoperability,platform,platformdirs,908,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:64,modifiability,Version,Versions,64,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:366,modifiability,deco,decorator,366,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:813,modifiability,pac,packaging,813,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1609,modifiability,pac,packaged,1609,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1753,safety,updat,updated,1753,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1733,security,Session,Session,1733,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1753,security,updat,updated,1753,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.10.0.dev57+g08be4e9a. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. google NA. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. setuptools_scm NA. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zmq 25.0.2. zoneinfo NA. -----. IPython 8.12.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 02:03. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:66,deployability,instal,installed,66,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:155,deployability,Instal,Installed,155,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:173,deployability,version,versions,173,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:365,deployability,instal,installed,365,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2025,deployability,updat,updated,2025,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:173,integrability,version,versions,173,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1236,interoperability,platform,platformdirs,1236,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:165,modifiability,pac,package,165,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:173,modifiability,version,versions,173,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:295,modifiability,pac,package,295,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:781,modifiability,deco,decorator,781,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1141,modifiability,pac,packaging,1141,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1881,modifiability,pac,packaged,1881,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2025,safety,updat,updated,2025,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2005,security,Session,Session,2005,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2025,security,updat,updated,2025,"alled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-03 21:49. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:419,usability,tool,tools,419,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway? <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:67,deployability,instal,installed,67,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:131,deployability,instal,install,131,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:175,deployability,instal,install,175,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:415,deployability,version,versions,415,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:516,deployability,updat,update,516,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2108,deployability,updat,updated,2108,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:445,energy efficiency,load,loading,445,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:415,integrability,version,versions,415,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1292,interoperability,platform,platformdirs,1292,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:153,modifiability,pac,packages,153,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:407,modifiability,pac,package,407,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:415,modifiability,version,versions,415,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:798,modifiability,deco,decorator,798,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1197,modifiability,pac,packaging,1197,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1964,modifiability,pac,packaged,1964,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:445,performance,load,loading,445,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9,safety,test,test,9,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:516,safety,updat,update,516,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2108,safety,updat,updated,2108,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:516,security,updat,update,516,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2088,security,Session,Session,2088,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2108,security,updat,updated,2108,"packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-5.19.0-41-generic-x86_64-with-glibc2.36. -----. Session information updated at 2023-05-04 01:16. </p>. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9,testability,test,test,9,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:470,usability,tool,tools,470,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>. <p>. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. gmpy2 2.1.2. h5py 3.8.0. hypergeom_ufunc NA. igraph 0.10.3. invgauss_ufunc NA. ipykernel 6.22.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.56.4. numpy 1.23.5. nvfuser NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. stack_data 0.6.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 2.0.0. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:874,availability,down,download,874,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5147,availability,checkpoint,checkpoint,5147,e. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:198,deployability,instal,install,198,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:278,deployability,instal,install,278,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:501,deployability,instal,installed,501,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:667,deployability,instal,installed,667,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:687,deployability,instal,install,687,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:776,deployability,instal,install,776,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:817,deployability,instal,install,817,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1084,deployability,Version,Version,1084,"le-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1092,deployability,Build,Build,1092,"ut/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2904,deployability,resourc,resources,2904, conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6005,deployability,api,api,6005,pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8418,deployability,instal,install,8418,"26a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8547,deployability,instal,install,8547,".1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8605,deployability,instal,install,8605,"ado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8774,deployability,Version,Version,8774,"hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8782,deployability,Build,Build,8782,"b_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:18140,deployability,api,api,18140,1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:18311,deployability,build,build,18311,inux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:18493,deployability,instal,installer,18493,.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py310hcb5633a_0 conda-forge. scanpy 1.9.3 pyhd8ed1ab_0 conda-forge. scikit-learn 1.3.0 py310hf7d194e_0 conda-forge. scipy 1.11.1 py310ha4c1d20_0 conda-forge. scvi-tools 1.0.2 pyhd8ed1ab_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:934,energy efficiency,GPU,GPU,934,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2904,energy efficiency,resourc,resources,2904, conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:4316,energy efficiency,cloud,cloud,4316,lient 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.0 he5830b7_0 conda-forge. liblapack 3.9.0 17_linux64_openblas conda-forge. libnsl 2.0.0 h7f98852_0 conda-forge. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8446,energy efficiency,GPU,GPU,8446,"0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:15224,energy efficiency,cloud,cloud,15224,ce57_2 conda-forge. libhwloc 2.9.1 nocuda_h7313eea_6 conda-forge. libiconv 1.17 h166bdaf_0 conda-forge. libjpeg-turbo 2.0.0 h9bf148f_0 pytorch. liblapack 3.9.0 16_linux64_mkl conda-forge. libleidenalg 0.11.1 h00ab1b0_0 conda-forge. libllvm14 14.0.6 hcd5def8_3 conda-forge. libnghttp2 1.52.0 h61bc06f_0 conda-forge. libnpp 11.8.0.86 0 nvidia. libnsl 2.0.0 h7f98852_0 conda-forge. libnvjpeg 11.9.0.86 0 nvidia. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libpng 1.6.39 h753d276_0 conda-forge. libprotobuf 3.21.12 h3eb15da_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.11.0 h0841786_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libtiff 4.5.0 h6adf6a1_2 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libwebp-base 1.3.1 hd590300_0 conda-forge. libxcb 1.13 h7f98852_1004 conda-forge. libxml2 2.11.4 h0d562d8_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.4 pyhd8ed1ab_0 conda-forge. lightning-cloud 0.5.37 pyhd8ed1ab_0 conda-forge. lightning-utilities 0.9.0 pyhd8ed1ab_0 conda-forge. llvm-openmp 16.0.6 h4dfa4b3_0 conda-forge. llvmlite 0.40.1 py310h1b8f574_0 conda-forge. lockfile 0.12.2 py_1 conda-forge. markdown-it-py 3.0.0 pyhd8ed1ab_0 conda-forge. markupsafe 2.1.3 py310h2372a71_0 conda-forge. matplotlib-base 3.7.1 py310he60537e_0 conda-forge. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.0 pyhd8ed1ab_0 conda-forge. metis 5.1.1 h59595ed_0 conda-forge. mkl 2022.2.1 h84fe81f_16997 conda-forge. ml-collections 0.1.1 pyhd8ed1ab_0 conda-forge. ml_dtypes 0.2.0 py310h7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17352,energy efficiency,core,core,17352,ge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17837,energy efficiency,core,core,17837,onda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1084,integrability,Version,Version,1084,"le-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6005,integrability,api,api,6005,pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8774,integrability,Version,Version,8774,"hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:18140,integrability,api,api,18140,1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:15,interoperability,convers,conversation,15,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5141,interoperability,orb,orbax-checkpoint,5141,orge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5509,interoperability,platform,platformdirs,5509,pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6005,interoperability,api,api,6005,pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:12754,interoperability,specif,specifications,12754,b_0 conda-forge. h5py 3.9.0 nompi_py310hcca72df_101 conda-forge. hdf5 1.14.1 nompi_h4f84152_100 conda-forge. html5lib 1.1 pyh9f0ad1d_0 conda-forge. icu 72.1 hcb278e6_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 h97b68dd_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. importlib_resources 6.0.0 pyhd8ed1ab_1 conda-forge. inquirer 3.1.3 pyhd8ed1ab_0 conda-forge. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pyhd8ed1ab_0 conda-forge. jaraco.classes 3.3.0 pyhd8ed1ab_0 conda-forge. jax 0.4.13 pyhd8ed1ab_0 conda-forge. jaxlib 0.4.12 cpu_py310hc0ddb09_1 conda-forge. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jeepney 0.8.0 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pyhd8ed1ab_1 conda-forge. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jpeg 9e h166bdaf_2 conda-forge. jsonschema 4.18.4 pyhd8ed1ab_0 conda-forge. jsonschema-specifications 2023.7.1 pyhd8ed1ab_0 conda-forge. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py310hff52083_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. keyring 23.13.1 py310hff52083_0 conda-forge. keyutils 1.6.1 h166bdaf_0 conda-forge. kiwisolver 1.4.4 py310hbf28c38_1 conda-forge. krb5 1.21.1 h659d440_0 conda-forge. lame 3.100 h166bdaf_1003 conda-forge. lcms2 2.15 hfd0df8a_0 conda-forge. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 py310hc6cd4ac_0 conda-forge. lerc 4.0.0 h27087fc_0 conda-forge. libabseil 20230125.3 cxx17_h59595ed_0 conda-forge. libaec 1.0.6 hcb278e6_1 conda-forge. libblas 3.9.0 16_linux64_mkl conda-forge. libbrotlicommon 1.0.9 h166bdaf_9 conda-forge. libbrotlidec 1.0.9 h166bdaf_9 conda-forge. libbrotlienc 1.0.9 h166bdaf_9 conda-forge. libcblas 3.9.0 16_linux64_mkl conda-forge. libcublas 11.11.3.6 0 nvidia. libcufft 10.9.0.58 0 nvidia. libcufile 1.7.0.149 0 nvidia. libcurand 10.3.3.53 0 nvidia. libcurl 8.2.0 hca2845,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17255,interoperability,platform,platformdirs,17255,d8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17396,interoperability,plug,plugin-export,17396,py 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. pytho,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17624,interoperability,stub,stubs,17624,_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:18140,interoperability,api,api,18140,1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:492,modifiability,pac,packages,492,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:699,modifiability,pac,packages,699,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1003,modifiability,pac,packages,1003,"n [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1052,modifiability,Pac,Packages,1052,"rg/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1084,modifiability,Version,Version,1084,"le-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2334,modifiability,deco,decorator,2334,i_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5208,modifiability,pac,packaging,5208, h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pyp,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:7761,modifiability,extens,extensions,7761,"0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8693,modifiability,pac,packages,8693,".9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8742,modifiability,Pac,Packages,8742,"0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8774,modifiability,Version,Version,8774,"hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:10859,modifiability,deco,decorator,10859, conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 py310heca2aa9_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pyhd8ed1ab_0 conda-forge. distlib 0.3.7 pyhd8ed1ab_0 conda-forge. dm-tree 0.1.7 py310h769672d_0 conda-forge. docrep 0.3.2 pyh44b312d_0 conda-forge. dulwich 0.21.5 py310h2372a71_0 conda-forge. et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge. exceptiongroup 1.1.2 pyhd8ed1ab_0 conda-forge. executing 1.2.0 pyhd8ed1ab_0 conda-forge. expat 2.5.0 hcb278e6_1 conda-forge. fastapi 0.100.0 pyhd8ed1ab_0 conda-forge. ffmpeg 4.3 hf484d3e_0 pytorch. filelock 3.12.2 pyhd8ed1ab_0 conda-forge. flax 0.6.1 pyhd8ed1ab_1 conda-forge. fonttools 4.41.1 py310h2372a71_0 conda-forge. freetype 2.12.1 hca18f0e_1 conda-forge. fsspec 2023.6.0 pyh1a96a4e_0 conda-forge. gettext 0.21.1 h27087fc_0 conda-forge. glpk 5.0 h445213a_0 conda-forge. gmp 6.2.1 h58526e2_0 conda-forge. gmpy2 2.1.2 py310h3ec546c_1 conda-forge. gnutls 3.6.13 h85f3911_1 conda-forge. h11 0.14.0 pyhd8ed1ab_0 conda-forge. h5py 3.9.0 nompi_py310hcca72df_101 conda-forge. hdf5 1.14.1 nompi_h4f84152_100 conda-f,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:16810,modifiability,pac,packaging,16810,7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:20947,modifiability,extens,extensions,20947,.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tomlkit 0.11.8 pyha770c72_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. torchaudio 2.0.2 py310_cu118 pytorch. torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge. torchtriton 2.0.0 py310 pytorch. torchvision 0.15.2 py310_cu118 pytorch. tornado 6.3.2 py310h2372a71_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge. typing 3.10.0.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py310hff52083_1 conda-forge. unicodedata2 15.0.0 py310h5764c6d_0 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. uvicorn 0.23.1 py310hff52083_0 conda-forge. virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. webencodings 0.5.1 py_1 conda-forge. websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge. websockets 11.0.3 py310h2372a71_0 conda-forge. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pyhd8ed1ab_0 conda-forge. xlrd 1.2.0 pyh9f0ad1d_1 conda-forge. xorg-libxau 1.0.11 hd590300_0 conda-forge. xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yaml 0.2.5 h7f98852_2 conda-forge. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.5.2 hfc55251_7 conda-forge. </p>. </,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:934,performance,GPU,GPU,934,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1447,performance,time,timeout,1447,"I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2904,performance,resourc,resources,2904, conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:4942,performance,network,networkx,4942,.9.0 17_linux64_openblas conda-forge. libnsl 2.0.0 h7f98852_0 conda-forge. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8446,performance,GPU,GPU,8446,"0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9295,performance,cach,cached-property,9295,"h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9769,performance,cach,cachecontrol,9769,ersion Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-fo,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9816,performance,cach,cachecontrol-with-filecache,9816,da-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9878,performance,cach,cached-property,9878,-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 py310heca2aa9_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:15403,performance,lock,lockfile,15403,-forge. libleidenalg 0.11.1 h00ab1b0_0 conda-forge. libllvm14 14.0.6 hcd5def8_3 conda-forge. libnghttp2 1.52.0 h61bc06f_0 conda-forge. libnpp 11.8.0.86 0 nvidia. libnsl 2.0.0 h7f98852_0 conda-forge. libnvjpeg 11.9.0.86 0 nvidia. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libpng 1.6.39 h753d276_0 conda-forge. libprotobuf 3.21.12 h3eb15da_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.11.0 h0841786_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libtiff 4.5.0 h6adf6a1_2 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libwebp-base 1.3.1 hd590300_0 conda-forge. libxcb 1.13 h7f98852_1004 conda-forge. libxml2 2.11.4 h0d562d8_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.4 pyhd8ed1ab_0 conda-forge. lightning-cloud 0.5.37 pyhd8ed1ab_0 conda-forge. lightning-utilities 0.9.0 pyhd8ed1ab_0 conda-forge. llvm-openmp 16.0.6 h4dfa4b3_0 conda-forge. llvmlite 0.40.1 py310h1b8f574_0 conda-forge. lockfile 0.12.2 py_1 conda-forge. markdown-it-py 3.0.0 pyhd8ed1ab_0 conda-forge. markupsafe 2.1.3 py310h2372a71_0 conda-forge. matplotlib-base 3.7.1 py310he60537e_0 conda-forge. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.0 pyhd8ed1ab_0 conda-forge. metis 5.1.1 h59595ed_0 conda-forge. mkl 2022.2.1 h84fe81f_16997 conda-forge. ml-collections 0.1.1 pyhd8ed1ab_0 conda-forge. ml_dtypes 0.2.0 py310h7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:16319,performance,network,networkx,16319,nmp 16.0.6 h4dfa4b3_0 conda-forge. llvmlite 0.40.1 py310h1b8f574_0 conda-forge. lockfile 0.12.2 py_1 conda-forge. markdown-it-py 3.0.0 pyhd8ed1ab_0 conda-forge. markupsafe 2.1.3 py310h2372a71_0 conda-forge. matplotlib-base 3.7.1 py310he60537e_0 conda-forge. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.0 pyhd8ed1ab_0 conda-forge. metis 5.1.1 h59595ed_0 conda-forge. mkl 2022.2.1 h84fe81f_16997 conda-forge. ml-collections 0.1.1 pyhd8ed1ab_0 conda-forge. ml_dtypes 0.2.0 py310h7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:177,reliability,doe,does,177,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:603,reliability,doe,does,603,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5147,reliability,checkpoint,checkpoint,5147,e. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1447,safety,timeout,timeout,1447,"I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2904,safety,resourc,resources,2904, conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:11151,safety,except,exceptiongroup,11151,yhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 py310heca2aa9_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pyhd8ed1ab_0 conda-forge. distlib 0.3.7 pyhd8ed1ab_0 conda-forge. dm-tree 0.1.7 py310h769672d_0 conda-forge. docrep 0.3.2 pyh44b312d_0 conda-forge. dulwich 0.21.5 py310h2372a71_0 conda-forge. et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge. exceptiongroup 1.1.2 pyhd8ed1ab_0 conda-forge. executing 1.2.0 pyhd8ed1ab_0 conda-forge. expat 2.5.0 hcb278e6_1 conda-forge. fastapi 0.100.0 pyhd8ed1ab_0 conda-forge. ffmpeg 4.3 hf484d3e_0 pytorch. filelock 3.12.2 pyhd8ed1ab_0 conda-forge. flax 0.6.1 pyhd8ed1ab_1 conda-forge. fonttools 4.41.1 py310h2372a71_0 conda-forge. freetype 2.12.1 hca18f0e_1 conda-forge. fsspec 2023.6.0 pyh1a96a4e_0 conda-forge. gettext 0.21.1 h27087fc_0 conda-forge. glpk 5.0 h445213a_0 conda-forge. gmp 6.2.1 h58526e2_0 conda-forge. gmpy2 2.1.2 py310h3ec546c_1 conda-forge. gnutls 3.6.13 h85f3911_1 conda-forge. h11 0.14.0 pyhd8ed1ab_0 conda-forge. h5py 3.9.0 nompi_py310hcca72df_101 conda-forge. hdf5 1.14.1 nompi_h4f84152_100 conda-forge. html5lib 1.1 pyh9f0ad1d_0 conda-forge. icu 72.1 hcb278e6_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 h97b68dd_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. importlib_resources 6.0.0 pyhd8ed1ab_1 cond,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1876,security,certif,certificates,1876,.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:1923,security,certif,certifi,1923,o have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:4942,security,network,networkx,4942,.9.0 17_linux64_openblas conda-forge. libnsl 2.0.0 h7f98852_0 conda-forge. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.5 pypi_0 pypi. lightning-cloud 0.5.37 pypi_0 pypi. lightning-utilities 0.9.0 pypi_0 pypi. lit 15.0.7 pypi_0 pypi. llvmlite 0.40.1 pypi_0 pypi. markdown-it-py 3.0.0 pypi_0 pypi. markupsafe 2.1.2 pypi_0 pypi. matplotlib 3.7.2 pypi_0 pypi. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.2 pypi_0 pypi. ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6858,security,session,session-info,6858,0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9722,security,certif,certificates,9722,ails><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:9972,security,certif,certifi,9972,s 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 py310heca2aa9_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pyhd8ed1ab_0 conda-forge. distlib 0.3.7 pyhd8ed1ab_0 conda-f,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:10478,security,cryptograph,cryptography,10478,ed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forge. brotlipy 0.7.0 py310h5764c6d_1005 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge. cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2023.7.22 pyhd8ed1ab_0 conda-forge. cffi 1.15.1 py310h255011f_3 conda-forge. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. chex 0.1.82 pyhd8ed1ab_0 conda-forge. cleo 2.0.1 pyhd8ed1ab_0 conda-forge. click 8.1.6 unix_pyh707e725_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. contourpy 1.1.0 py310hd41b1e2_0 conda-forge. crashtest 0.4.1 pyhd8ed1ab_0 conda-forge. croniter 1.3.15 pyhd8ed1ab_0 conda-forge. cryptography 41.0.2 py310h75e40e8_0 conda-forge. cuda-cudart 11.8.89 0 nvidia. cuda-cupti 11.8.87 0 nvidia. cuda-libraries 11.8.0 0 nvidia. cuda-nvrtc 11.8.89 0 nvidia. cuda-nvtx 11.8.86 0 nvidia. cuda-runtime 11.8.0 0 nvidia. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. dateutils 0.6.12 py_0 conda-forge. dbus 1.13.6 h5008d03_3 conda-forge. debugpy 1.6.7 py310heca2aa9_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pyhd8ed1ab_0 conda-forge. distlib 0.3.7 pyhd8ed1ab_0 conda-forge. dm-tree 0.1.7 py310h769672d_0 conda-forge. docrep 0.3.2 pyh44b312d_0 conda-forge. dulwich 0.21.5 py310h2372a71_0 conda-forge. et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge. exceptiongroup 1.1.2 pyhd8ed1ab_0 conda-forge. executing 1.2.0 pyhd8ed1ab_0 conda-forge. expat 2.5.0 hcb278e6_1 conda-forge. fastapi 0.100.0 pyhd8ed1ab_0 conda-forge. ffmpeg 4.3 hf484d3e_0 pytorch. filelock 3.12.2 pyhd8ed1ab_0 conda-forge. flax 0.6.1 pyhd8ed1ab_1 conda-forge. fonttools 4.41.1 py310h2372a71_0 conda-forge. freetype 2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:15403,security,lock,lockfile,15403,-forge. libleidenalg 0.11.1 h00ab1b0_0 conda-forge. libllvm14 14.0.6 hcd5def8_3 conda-forge. libnghttp2 1.52.0 h61bc06f_0 conda-forge. libnpp 11.8.0.86 0 nvidia. libnsl 2.0.0 h7f98852_0 conda-forge. libnvjpeg 11.9.0.86 0 nvidia. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libpng 1.6.39 h753d276_0 conda-forge. libprotobuf 3.21.12 h3eb15da_0 conda-forge. libsodium 1.0.18 h36c2ea0_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.11.0 h0841786_0 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libtiff 4.5.0 h6adf6a1_2 conda-forge. libuuid 2.38.1 h0b41bf4_0 conda-forge. libwebp-base 1.3.1 hd590300_0 conda-forge. libxcb 1.13 h7f98852_1004 conda-forge. libxml2 2.11.4 h0d562d8_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lightning 2.0.4 pyhd8ed1ab_0 conda-forge. lightning-cloud 0.5.37 pyhd8ed1ab_0 conda-forge. lightning-utilities 0.9.0 pyhd8ed1ab_0 conda-forge. llvm-openmp 16.0.6 h4dfa4b3_0 conda-forge. llvmlite 0.40.1 py310h1b8f574_0 conda-forge. lockfile 0.12.2 py_1 conda-forge. markdown-it-py 3.0.0 pyhd8ed1ab_0 conda-forge. markupsafe 2.1.3 py310h2372a71_0 conda-forge. matplotlib-base 3.7.1 py310he60537e_0 conda-forge. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.0 pyhd8ed1ab_0 conda-forge. metis 5.1.1 h59595ed_0 conda-forge. mkl 2022.2.1 h84fe81f_16997 conda-forge. ml-collections 0.1.1 pyhd8ed1ab_0 conda-forge. ml_dtypes 0.2.0 py310h7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:16319,security,network,networkx,16319,nmp 16.0.6 h4dfa4b3_0 conda-forge. llvmlite 0.40.1 py310h1b8f574_0 conda-forge. lockfile 0.12.2 py_1 conda-forge. markdown-it-py 3.0.0 pyhd8ed1ab_0 conda-forge. markupsafe 2.1.3 py310h2372a71_0 conda-forge. matplotlib-base 3.7.1 py310he60537e_0 conda-forge. matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge. mdurl 0.1.0 pyhd8ed1ab_0 conda-forge. metis 5.1.1 h59595ed_0 conda-forge. mkl 2022.2.1 h84fe81f_16997 conda-forge. ml-collections 0.1.1 pyhd8ed1ab_0 conda-forge. ml_dtypes 0.2.0 py310h7cbd5c2_1 conda-forge. more-itertools 9.1.0 pyhd8ed1ab_0 conda-forge. mpc 1.3.1 hfe3b2da_0 conda-forge. mpfr 4.2.0 hb012696_0 conda-forge. mpmath 1.3.0 pyhd8ed1ab_0 conda-forge. msgpack-python 1.0.5 py310hdf3cbec_0 conda-forge. mudata 0.2.3 pyhd8ed1ab_0 conda-forge. multipledispatch 0.6.0 py_0 conda-forge. munkres 1.1.4 pyh9f0ad1d_0 conda-forge. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. nettle 3.6 he412f7d_0 conda-forge. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py310h0f6aa51_0 conda-forge. numpy 1.24.4 py310ha4c1d20_0 conda-forge. numpyro 0.12.1 pyhd8ed1ab_0 conda-forge. openh264 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:19647,security,session,session-info,19647,0 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py310hcb5633a_0 conda-forge. scanpy 1.9.3 pyhd8ed1ab_0 conda-forge. scikit-learn 1.3.0 py310hf7d194e_0 conda-forge. scipy 1.11.1 py310ha4c1d20_0 conda-forge. scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. secretstorage 3.3.3 py310hff52083_1 conda-forge. session-info 1.0.0 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. shellingham 1.5.1 pyhd8ed1ab_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sleef 3.5.1 h9b69904_2 conda-forge. sniffio 1.3.0 pyhd8ed1ab_0 conda-forge. soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge. sparse 0.14.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tomlkit 0.11.8 pyha770c72_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. torchaudio 2.0.2 py310_cu118 pytorch. torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge. torc,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:2904,testability,resourc,resources,2904, conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1.7 pypi_0 pypi. click 8.1.6 pypi_0 pypi. cmake 3.25.0 pypi_0 pypi. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. comm 0.1.3 pyhd8ed1ab_0 conda-forge. contextlib2 21.6.0 pypi_0 pypi. contourpy 1.1.0 pypi_0 pypi. croniter 1.4.1 pypi_0 pypi. cycler 0.11.0 pypi_0 pypi. dateutils 0.6.12 pypi_0 pypi. debugpy 1.6.7 py311hcafe171_0 conda-forge. decorator 5.1.1 pyhd8ed1ab_0 conda-forge. deepdiff 6.3.1 pypi_0 pypi. dm-tree 0.1.8 pypi_0 pypi. docrep 0.3.2 pypi_0 pypi. etils 1.3.0 pypi_0 pypi. executing 1.2.0 pyhd8ed1ab_0 conda-forge. fa2 0.3.5 py311hd4cff14_2 conda-forge. fastapi 0.100.0 pypi_0 pypi. filelock 3.9.0 pypi_0 pypi. flax 0.7.0 pypi_0 pypi. fonttools 4.41.1 pypi_0 pypi. frozenlist 1.4.0 pypi_0 pypi. fsspec 2023.6.0 pypi_0 pypi. h11 0.14.0 pypi_0 pypi. h5py 3.9.0 pypi_0 pypi. idna 3.4 pyhd8ed1ab_0 conda-forge. igraph 0.10.6 pypi_0 pypi. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib-resources 6.0.0 pypi_0 pypi. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. inquirer 3.1.3 pypi_0 pypi. ipykernel 6.24.0 pyh71e2992_0 conda-forge. ipython 8.14.0 pyh41d4057_0 conda-forge. ipywidgets 8.0.7 pyhd8ed1ab_0 conda-forge. itsdangerous 2.1.2 pypi_0 pypi. jax 0.4.13 pypi_0 pypi. jaxlib 0.4.13 pypi_0 pypi. jedi 0.18.2 pyhd8ed1ab_0 conda-forge. jinja2 3.1.2 pypi_0 pypi. joblib 1.3.1 pypi_0 pypi. jupyter_client 8.3.0 pyhd8ed1ab_0 conda-forge. jupyter_core 5.3.1 py311h38be061_0 conda-forge. jupyterlab_widgets 3.0.8 pyhd8ed1ab_0 conda-forge. kiwisolver 1.4.4 pypi_0 pypi. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. leidenalg 0.10.1 pypi_0 pypi. levenshtein 0.21.1 pypi_0 pypi. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libexpat 2.5.0 hcb278e6_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17624,testability,stub,stubs,17624,_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:157,usability,behavi,behavior,157,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:438,usability,interact,interaction,438,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:549,usability,close,close,549,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:574,usability,behavi,behavior,574,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:806,usability,tool,tools,806,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:938,usability,support,support,938,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:956,usability,tool,tools,956,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```. conda create -n scanpy_test1 python. pip install scanpy leidenalg scvi-tools. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118. ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. absl-py 1.4.0 pypi_0 pypi. adjusttext 0.8 pypi_0 pypi. aiohttp 3.8.5 pypi_0 pypi. aiosignal 1.3.1 pypi_0 pypi. airr 1.4.1 pypi_0 pypi. anndata 0.9.1 pypi_0 pypi. anyio 3.7.1 pypi_0 pypi. arrow 1.2.3 pypi_0 pypi. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. async-timeout 4.0.2 pypi_0 pypi. attrs 23.1.0 pypi_0 pypi. awkward 2.3.1 pypi_0 pypi. awkward-cpp 21 pypi_0 pypi. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backoff 2.2.1 pypi_0 pypi. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pypi_0 pypi. blessed 1.20.0 pypi_0 pypi. brotli-python 1.0.9 py311ha362b79_9 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. ca-certificates 2023.7.22 hbcca054_0 conda-forge. certifi 2022.12.7 pypi_0 pypi. charset-normalizer 2.1.1 pypi_0 pypi. chex 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:5599,usability,tool,toolkit,5599,ml-collections 0.1.1 pypi_0 pypi. ml-dtypes 0.2.0 pypi_0 pypi. mpmath 1.2.1 pypi_0 pypi. msgpack 1.0.5 pypi_0 pypi. mudata 0.2.3 pypi_0 pypi. multidict 6.0.4 pypi_0 pypi. multipledispatch 1.0.0 pypi_0 pypi. muon 0.1.5 pypi_0 pypi. natsort 8.4.0 pypi_0 pypi. ncurses 6.4 hcb278e6_0 conda-forge. nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge. networkx 3.1 pypi_0 pypi. numba 0.57.1 pypi_0 pypi. numpy 1.24.4 pypi_0 pypi. numpyro 0.12.1 pypi_0 pypi. openssl 3.1.1 hd590300_1 conda-forge. opt-einsum 3.3.0 pypi_0 pypi. optax 0.1.5 pypi_0 pypi. orbax-checkpoint 0.2.7 pypi_0 pypi. ordered-set 4.1.0 pypi_0 pypi. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 pypi_0 pypi. parasail 1.3.4 pypi_0 pypi. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pypi_0 pypi. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 10.0.0 pypi_0 pypi. pip 23.2.1 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6680,usability,learn,learn,6680,e. protobuf 4.23.4 pypi_0 pypi. psutil 5.9.5 py311h2582759_0 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forg,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:6805,usability,tool,tools,6805,l 0.2.2 pyhd8ed1ab_0 conda-forge. pydantic 1.10.11 pypi_0 pypi. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pypi_0 pypi. pynndescent 0.5.10 pypi_0 pypi. pyparsing 3.0.9 pypi_0 pypi. pyro-api 0.1.2 pypi_0 pypi. pyro-ppl 1.8.5 pypi_0 pypi. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.11.4 hab00c5b_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 pypi_0 pypi. python-igraph 0.10.6 pypi_0 pypi. python-levenshtein 0.21.1 pypi_0 pypi. python-multipart 0.0.6 pypi_0 pypi. python_abi 3.11 3_cp311 conda-forge. pytorch-lightning 2.0.5 pypi_0 pypi. pytz 2023.3 pypi_0 pypi. pyyaml 6.0.1 pypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:7442,usability,tool,toolz,7442,ypi_0 pypi. pyzmq 25.1.0 py311h75c88c4_0 conda-forge. rapidfuzz 3.1.2 pypi_0 pypi. readchar 4.0.5 pypi_0 pypi. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pypi_0 pypi. rich 13.4.2 pypi_0 pypi. scanpy 1.9.3 pypi_0 pypi. scikit-learn 1.3.0 pypi_0 pypi. scipy 1.11.1 py311h64a7726_0 conda-forge. scirpy 0.13.0 pypi_0 pypi. scmisc 0.0.1 pypi_0 pypi. scvi-tools 1.0.2 pypi_0 pypi. seaborn 0.12.2 pypi_0 pypi. session-info 1.0.0 pypi_0 pypi. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:7884,usability,learn,learn,7884,"i. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. singlecellhaystack 0.0.5 pypi_0 pypi. six 1.16.0 pyh6c4a22f_0 conda-forge. sniffio 1.3.0 pypi_0 pypi. soupsieve 2.4.1 pypi_0 pypi. sparse 0.14.0 pypi_0 pypi. squarify 0.4.3 pypi_0 pypi. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8112,usability,widget,widgetsnbextension,8112,"tack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pypi_0 pypi. starsessions 1.3.0 pypi_0 pypi. statsmodels 0.14.0 pypi_0 pypi. stdlib-list 0.9.0 pypi_0 pypi. sympy 1.11.1 pypi_0 pypi. tensorstore 0.1.40 pypi_0 pypi. texttable 1.6.7 pypi_0 pypi. threadpoolctl 3.2.0 pypi_0 pypi. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pypi_0 pypi. torch 2.0.1+cu118 pypi_0 pypi. torchaudio 2.0.2+cu118 pypi_0 pypi. torchmetrics 1.0.1 pypi_0 pypi. torchvision 0.15.2+cu118 pypi_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:8592,usability,tool,tools,8592,"i_0 pypi. tornado 6.3.2 py311h459d7ec_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. triton 2.0.0 pypi_0 pypi. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023.3 pypi_0 pypi. umap-learn 0.5.3 pypi_0 pypi. urllib3 1.26.13 pypi_0 pypi. uvicorn 0.23.1 pypi_0 pypi. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. websocket-client 1.6.1 pypi_0 pypi. websockets 11.0.3 pypi_0 pypi. wheel 0.41.0 pyhd8ed1ab_0 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pypi_0 pypi. xz 5.2.6 h166bdaf_0 conda-forge. yamlordereddictloader 0.4.0 pypi_0 pypi. yarl 1.9.2 pypi_0 pypi. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>. </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:. ```. conda create -n scanpy_test2. conda install -c conda-forge scanpy leidenalg scvi-tools. conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia. ```. The packages in this environment:. <details><summary>Packages</summary>. <p>. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_kmp_llvm conda-forge. absl-py 1.4.0 pyhd8ed1ab_0 conda-forge. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge. anyio 3.7.1 pyhd8ed1ab_0 conda-forge. arpack 3.7.0 hdefa2d7_2 conda-forge. arrow 1.2.3 pyhd8ed1ab_0 conda-forge. asttokens 2.2.1 pyhd8ed1ab_0 conda-forge. attrs 23.1.0 pyh71513ae_1 conda-forge. backcall 0.2.0 pyh9f0ad1d_0 conda-forge. backports 1.0 pyhd8ed1ab_3 conda-forge. backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge. backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge. beautifulsoup4 4.12.2 pyha770c72_0 conda-forge. blas 1.0 mkl conda-forge. blessed 1.19.1 pyhe4f9e05_2 conda-forge. brotli 1.0.9 h166bdaf_9 conda-forge. brotli-bin 1.0.9 h166bdaf_9 conda-forg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:17487,usability,tool,toolkit,17487, 2.1.1 h780b84a_0 conda-forge. openjpeg 2.5.0 hfec8fc6_2 conda-forge. openpyxl 3.1.2 py310h2372a71_0 conda-forge. openssl 3.1.1 hd590300_1 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. optax 0.1.5 pyhd8ed1ab_0 conda-forge. ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge. orjson 3.9.2 py310h1e2579a_0 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py310h7cbd5c2_1 conda-forge. parso 0.8.3 pyhd8ed1ab_0 conda-forge. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hc3806b6_0 conda-forge. pexpect 4.8.0 pyh1a96a4e_2 conda-forge. pickleshare 0.7.5 py_1003 conda-forge. pillow 9.4.0 py310h023d228_1 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge. pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge. poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge. poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.39 pyha770c72_0 conda-forge. prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge. psutil 5.9.5 py310h1fa729e_0 conda-forge. pthread-stubs 0.4 h36c2ea0_1001 conda-forge. ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge. pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pydantic 2.0.3 pyhd8ed1ab_1 conda-forge. pydantic-core 2.3.0 py310hcb5633a_0 conda-forge. pygments 2.15.1 pyhd8ed1ab_0 conda-forge. pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge. pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge. pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. pytho,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:19219,usability,tool,toolbelt,19219,s 1.7.1 pyha2e5f31_6 conda-forge. python 3.10.12 hd12c33a_0_cpython conda-forge. python-build 0.10.0 pyhd8ed1ab_1 conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py310hcb5633a_0 conda-forge. scanpy 1.9.3 pyhd8ed1ab_0 conda-forge. scikit-learn 1.3.0 py310hf7d194e_0 conda-forge. scipy 1.11.1 py310ha4c1d20_0 conda-forge. scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. secretstorage 3.3.3 py310hff52083_1 conda-forge. session-info 1.0.0 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. shellingham 1.5.1 pyhd8ed1ab_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sleef 3.5.1 h9b69904_2 conda-forge. sniffio 1.3.0 pyhd8ed1ab_0 conda-forge. soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge. sparse 0.14.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:19387,usability,learn,learn,19387,-forge. python-editor 1.0.4 py_0 conda-forge. python-igraph 0.10.6 py310h33b8572_0 conda-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py310hcb5633a_0 conda-forge. scanpy 1.9.3 pyhd8ed1ab_0 conda-forge. scikit-learn 1.3.0 py310hf7d194e_0 conda-forge. scipy 1.11.1 py310ha4c1d20_0 conda-forge. scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. secretstorage 3.3.3 py310hff52083_1 conda-forge. session-info 1.0.0 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. shellingham 1.5.1 pyhd8ed1ab_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sleef 3.5.1 h9b69904_2 conda-forge. sniffio 1.3.0 pyhd8ed1ab_0 conda-forge. soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge. sparse 0.14.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:19475,usability,tool,tools,19475,-forge. python-installer 0.7.0 pyhd8ed1ab_0 conda-forge. python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.10 3_cp310 conda-forge. pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch. pytorch-cuda 11.8 h7e8668a_5 pytorch. pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge. pytorch-mutex 1.0 cuda pytorch. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyyaml 6.0 py310h5764c6d_5 conda-forge. pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge. rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge. re2 2023.03.02 h8c504da_0 conda-forge. readchar 4.0.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. referencing 0.30.0 pyhd8ed1ab_0 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge. rich 13.4.2 pyhd8ed1ab_0 conda-forge. rpds-py 0.9.2 py310hcb5633a_0 conda-forge. scanpy 1.9.3 pyhd8ed1ab_0 conda-forge. scikit-learn 1.3.0 py310hf7d194e_0 conda-forge. scipy 1.11.1 py310ha4c1d20_0 conda-forge. scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. secretstorage 3.3.3 py310hff52083_1 conda-forge. session-info 1.0.0 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. shellingham 1.5.1 pyhd8ed1ab_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sleef 3.5.1 h9b69904_2 conda-forge. sniffio 1.3.0 pyhd8ed1ab_0 conda-forge. soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge. sparse 0.14.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:20526,usability,tool,toolz,20526,d8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. secretstorage 3.3.3 py310hff52083_1 conda-forge. session-info 1.0.0 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. shellingham 1.5.1 pyhd8ed1ab_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sleef 3.5.1 h9b69904_2 conda-forge. sniffio 1.3.0 pyhd8ed1ab_0 conda-forge. soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge. sparse 0.14.0 pyhd8ed1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tomlkit 0.11.8 pyha770c72_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. torchaudio 2.0.2 py310_cu118 pytorch. torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge. torchtriton 2.0.0 py310 pytorch. torchvision 0.15.2 py310_cu118 pytorch. tornado 6.3.2 py310h2372a71_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge. typing 3.10.0.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py310hff52083_1 conda-forge. unicodedata2 15.0.0 py310h5764c6d_0 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. uvicorn 0.23.1 py310hff52083_0 conda-forge. virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. webencodings 0.5.1 py_1 conda-forge. websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge. websockets 11.0.3 py310h2372a71_0 conda-forge. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. widgetsnbextensi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:21080,usability,learn,learn,21080,1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tomlkit 0.11.8 pyha770c72_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. torchaudio 2.0.2 py310_cu118 pytorch. torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge. torchtriton 2.0.0 py310 pytorch. torchvision 0.15.2 py310_cu118 pytorch. tornado 6.3.2 py310h2372a71_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge. typing 3.10.0.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py310hff52083_1 conda-forge. unicodedata2 15.0.0 py310h5764c6d_0 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. uvicorn 0.23.1 py310hff52083_0 conda-forge. virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. webencodings 0.5.1 py_1 conda-forge. websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge. websockets 11.0.3 py310h2372a71_0 conda-forge. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pyhd8ed1ab_0 conda-forge. xlrd 1.2.0 pyh9f0ad1d_1 conda-forge. xorg-libxau 1.0.11 hd590300_0 conda-forge. xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yaml 0.2.5 h7f98852_2 conda-forge. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.5.2 hfc55251_7 conda-forge. </p>. </details> .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/issues/2480:21513,usability,widget,widgetsnbextension,21513,1ab_0 conda-forge. stack_data 0.6.2 pyhd8ed1ab_0 conda-forge. starlette 0.27.0 pyhd8ed1ab_0 conda-forge. starsessions 1.3.0 pyhd8ed1ab_0 conda-forge. statsmodels 0.14.0 py310h278f3c1_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. suitesparse 5.10.1 h9e50725_1 conda-forge. sympy 1.12 pypyh9d50eac_103 conda-forge. tbb 2021.9.0 hf52228f_0 conda-forge. texttable 1.6.7 pyhd8ed1ab_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tomlkit 0.11.8 pyha770c72_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. torchaudio 2.0.2 py310_cu118 pytorch. torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge. torchtriton 2.0.0 py310 pytorch. torchvision 0.15.2 py310_cu118 pytorch. tornado 6.3.2 py310h2372a71_0 conda-forge. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.9.0 pyhd8ed1ab_0 conda-forge. trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge. typing 3.10.0.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py310hff52083_1 conda-forge. unicodedata2 15.0.0 py310h5764c6d_0 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. uvicorn 0.23.1 py310hff52083_0 conda-forge. virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge. wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge. webencodings 0.5.1 py_1 conda-forge. websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge. websockets 11.0.3 py310h2372a71_0 conda-forge. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge. xarray 2023.7.0 pyhd8ed1ab_0 conda-forge. xlrd 1.2.0 pyh9f0ad1d_1 conda-forge. xorg-libxau 1.0.11 hd590300_0 conda-forge. xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yaml 0.2.5 h7f98852_2 conda-forge. zeromq 4.3.4 h9c3ff4c_1 conda-forge. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.5.2 hfc55251_7 conda-forge. </p>. </details> .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480
https://github.com/scverse/scanpy/pull/2482:13,deployability,continu,continue,13,"I’m going to continue this in #2977, thank you Rahul!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2482
https://github.com/scverse/scanpy/pull/2483:28,usability,support,support,28,Closing because any spatial support should live in spatialdata and squidpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2483
https://github.com/scverse/scanpy/issues/2485:601,availability,down,download-,601,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:766,availability,cluster,cluster,766,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:798,availability,Cluster,Clusters,798,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:871,availability,cluster,clustering,871,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:965,availability,cluster,cluster,965,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:99,deployability,contain,contain,99,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:492,deployability,build,build,492,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:766,deployability,cluster,cluster,766,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:798,deployability,Cluster,Clusters,798,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:871,deployability,cluster,clustering,871,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:965,deployability,cluster,cluster,965,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:820,safety,compl,completely,820,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:820,security,compl,completely,820,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:621,usability,user,user-images,621,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:913,usability,visual,visualisation,913,"Hi, @GeoffSCollins ! To do that, you need to add new columns to `adata.obs` table. A column should contain one value (e.g. `True`) for the group of interest and another value (e.g. `False`) for all the other groups. This is how you can create such columns for each of the values of column `leiden`:. ```python. import pandas as pd. adata.obs = pd.concat([. adata.obs,. pd.get_dummies(adata.obs[""leiden""], prefix=""is_cell_type"").astype(bool).astype(""category""),. ], axis=1). ```. Then you can build a violin plot:. ```python. sc.pl.violin(adata, keys=""total_counts"", groupby=""is_cell_type_NK""). ```. ![download-8](https://user-images.githubusercontent.com/35199218/236865939-768d759a-a581-4ba9-acb9-0104cbf16efd.png). I hope this is useful! Also, a small note: ""UMAP cluster"" is not a correct term. Clusters are obtained completely independently of UMAP by running leiden clustering algorithm. UMAP is just a nice visualisation (which is, however, built ignorant of cluster information).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:130,availability,sli,slightly,130,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:319,availability,down,down,319,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:329,integrability,rout,route,329,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:130,reliability,sli,slightly,130,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2485:39,usability,help,help,39,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485
https://github.com/scverse/scanpy/issues/2488:264,availability,down,download,264,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:8,deployability,updat,updated,8,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:30,deployability,version,version,30,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:45,deployability,modul,module,45,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:206,deployability,version,version,206,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:330,deployability,instal,install,330,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:30,integrability,version,version,30,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:206,integrability,version,version,206,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:30,modifiability,version,version,30,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:45,modifiability,modul,module,45,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:206,modifiability,version,version,206,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:8,safety,updat,updated,8,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:45,safety,modul,module,45,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:8,security,updat,updated,8,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like . `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:388,deployability,releas,release,388,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:423,deployability,updat,update,423,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:191,interoperability,coordinat,coordinates,191,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:423,safety,updat,update,423,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:423,security,updat,update,423,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2488:49,deployability,releas,release,49,"duplicate of #2345, #2565, and so on. we’ll do a release soon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488
https://github.com/scverse/scanpy/issues/2494:0,availability,Error,Error,0,Error here. ![image](https://github.com/scverse/scanpy/assets/117483585/c23b446d-f5f9-4775-95ce-eda0a49aba81). ![image](https://github.com/scverse/scanpy/assets/117483585/09ee6450-66b0-43e2-8079-fcb3e06735d4).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2494
https://github.com/scverse/scanpy/issues/2494:0,performance,Error,Error,0,Error here. ![image](https://github.com/scverse/scanpy/assets/117483585/c23b446d-f5f9-4775-95ce-eda0a49aba81). ![image](https://github.com/scverse/scanpy/assets/117483585/09ee6450-66b0-43e2-8079-fcb3e06735d4).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2494
https://github.com/scverse/scanpy/issues/2494:0,safety,Error,Error,0,Error here. ![image](https://github.com/scverse/scanpy/assets/117483585/c23b446d-f5f9-4775-95ce-eda0a49aba81). ![image](https://github.com/scverse/scanpy/assets/117483585/09ee6450-66b0-43e2-8079-fcb3e06735d4).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2494
https://github.com/scverse/scanpy/issues/2494:0,usability,Error,Error,0,Error here. ![image](https://github.com/scverse/scanpy/assets/117483585/c23b446d-f5f9-4775-95ce-eda0a49aba81). ![image](https://github.com/scverse/scanpy/assets/117483585/09ee6450-66b0-43e2-8079-fcb3e06735d4).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2494
https://github.com/scverse/scanpy/issues/2495:280,availability,error,error,280,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:286,integrability,messag,message,286,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:286,interoperability,messag,message,286,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:280,performance,error,error,280,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:572,performance,disk,disk,572,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:591,performance,time,time,591,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:20,safety,input,input,20,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:280,safety,error,error,280,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:20,usability,input,input,20,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:280,usability,error,error,280,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:426,usability,behavi,behaviour,426,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:529,usability,user,user,529,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as you’ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; I’d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me. - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:76,usability,close,closed,76,"With the merged PR which addresses the issue raised, this probably could be closed? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2495:8,usability,close,close,8,"We will close the issue for now, as based on the provided information and the discussion so far, it seems that the question has been addressed and hopefully resolved :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495
https://github.com/scverse/scanpy/issues/2496:48,modifiability,layer,layers,48,"I've run into the same issue. In my case `adata.layers[""analytic_pearson_residuals""].sum(1)` gives an array of nans because there are nans in `analytic_pearson[""X""]`, as indicated by RuntimeWarning. . I am still only investigating this, but if treating nans as 0 is OK there is numpy.nansum function that could be used instead of sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:170,usability,indicat,indicated,170,"I've run into the same issue. In my case `adata.layers[""analytic_pearson_residuals""].sum(1)` gives an array of nans because there are nans in `analytic_pearson[""X""]`, as indicated by RuntimeWarning. . I am still only investigating this, but if treating nans as 0 is OK there is numpy.nansum function that could be used instead of sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:115,integrability,filter,filter,115,"It seems that some genes did not express in all cells. I tried to used `sc.pp.filter_genes(adata, min_cells=1)` to filter out those genes, and addressed this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2498:0,deployability,Depend,Depends,0,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498
https://github.com/scverse/scanpy/issues/2498:0,integrability,Depend,Depends,0,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498
https://github.com/scverse/scanpy/issues/2498:0,modifiability,Depend,Depends,0,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498
https://github.com/scverse/scanpy/issues/2498:0,safety,Depend,Depends,0,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498
https://github.com/scverse/scanpy/issues/2498:0,testability,Depend,Depends,0,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498
https://github.com/scverse/scanpy/issues/2499:141,availability,error,error,141,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:313,availability,sli,slide,313,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:141,performance,error,error,141,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:313,reliability,sli,slide,313,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:141,safety,error,error,141,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:141,usability,error,error,141,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:260,usability,minim,minimal,260,"Hmm, that sounds like the code is trying to do something like `['a', 'b'] * 1.5`, i.e. repeating a sequence by multiplying. But the line the error points to looks correct: `np.multiply` should try to do element-wise multiplication as intended. Please create a minimal reproducible example. We don’t know what in `slide` causes the bug because we don’t know what it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:91,interoperability,share,share,91,"Hi,. Sorry I'm not exactly sure how to create a reproducible example. Is there a way I can share the h5ad object with you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:84,deployability,version,version,84,Do the data come from an FFPE 10X Visium experiment calculated with the SpaceRanger version 2.1? How does the tisue_position_list.csv within the spatial folder of the SpaceRanger output look? Maybe you could attach a screenshot or the csv.-file. (try drag and drop),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:84,integrability,version,version,84,Do the data come from an FFPE 10X Visium experiment calculated with the SpaceRanger version 2.1? How does the tisue_position_list.csv within the spatial folder of the SpaceRanger output look? Maybe you could attach a screenshot or the csv.-file. (try drag and drop),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:84,modifiability,version,version,84,Do the data come from an FFPE 10X Visium experiment calculated with the SpaceRanger version 2.1? How does the tisue_position_list.csv within the spatial folder of the SpaceRanger output look? Maybe you could attach a screenshot or the csv.-file. (try drag and drop),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:101,reliability,doe,does,101,Do the data come from an FFPE 10X Visium experiment calculated with the SpaceRanger version 2.1? How does the tisue_position_list.csv within the spatial folder of the SpaceRanger output look? Maybe you could attach a screenshot or the csv.-file. (try drag and drop),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:34,interoperability,share,share,34,"If the data isn’t sensitive, just share the .h5ad file with us in any way. if it is, you’d need to write code like this:. ```py. ad = AnnData(np.random.rand(10, 20), obs={...}, uns={...}). ```. with just the necessary pieces to make the bug appear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:57,deployability,version,version,57,"Hi,. The data comes from FFPE 10x Visium but Spaceranger version 2.0.0. I attached the csv file here. [tissue_positions_list.csv](https://github.com/scverse/scanpy/files/11858496/tissue_positions_list.csv). I also just emailed the h5ad file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:57,integrability,version,version,57,"Hi,. The data comes from FFPE 10x Visium but Spaceranger version 2.0.0. I attached the csv file here. [tissue_positions_list.csv](https://github.com/scverse/scanpy/files/11858496/tissue_positions_list.csv). I also just emailed the h5ad file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:57,modifiability,version,version,57,"Hi,. The data comes from FFPE 10x Visium but Spaceranger version 2.0.0. I attached the csv file here. [tissue_positions_list.csv](https://github.com/scverse/scanpy/files/11858496/tissue_positions_list.csv). I also just emailed the h5ad file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,deployability,pipelin,pipeline,966,"I see! File looks like this:. ```csv. barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres. GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629. CACGGTCTCCTTACGA-1,0,0,2,-1569,2811. ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993. GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174. ... ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that. - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and. - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,integrability,pipelin,pipeline,966,"I see! File looks like this:. ```csv. barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres. GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629. CACGGTCTCCTTACGA-1,0,0,2,-1569,2811. ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993. GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174. ... ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that. - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and. - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:937,reliability,doe,doesn,937,"I see! File looks like this:. ```csv. barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres. GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629. CACGGTCTCCTTACGA-1,0,0,2,-1569,2811. ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993. GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174. ... ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that. - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and. - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:975,reliability,doe,does,975,"I see! File looks like this:. ```csv. barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres. GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629. CACGGTCTCCTTACGA-1,0,0,2,-1569,2811. ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993. GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174. ... ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that. - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and. - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:757,security,ident,identical,757,"I see! File looks like this:. ```csv. barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres. GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629. CACGGTCTCCTTACGA-1,0,0,2,-1569,2811. ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993. GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174. ... ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that. - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and. - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:146,deployability,version,version,146,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:302,deployability,version,version,302,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:146,integrability,version,version,146,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:302,integrability,version,version,302,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:146,modifiability,version,version,146,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:302,modifiability,version,version,302,"> @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. Yes, I meant read_visium. If I use read_visium (using scanpy version 1.9.1) I get the following OSerror:. `OSError: Could not find 'spatial/tissue_positions_list.csv'`. Is the usage of read_visium only implemented in version 1.9.2?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:42,deployability,releas,release,42,It’s actually still unreleased. We should release 1.9.3 soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:20,performance,perform,perform,20,So we still need to perform the following work around:. 1) change file name of file `tissue_positions.csv` to `tissue_positions_list.csv`. 2) delete the header in the file. Right?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:20,usability,perform,perform,20,So we still need to perform the following work around:. 1) change file name of file `tissue_positions.csv` to `tissue_positions_list.csv`. 2) delete the header in the file. Right?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:7,deployability,instal,install,7,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:31,deployability,version,version,31,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:55,deployability,instal,install,55,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:31,integrability,version,version,31,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:31,modifiability,version,version,31,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2500:37,availability,error,error,37,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:265,availability,checkpoint,checkpoint,265,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:639,availability,checkpoint,checkpoint,639,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:974,availability,checkpoint,checkpoint,974,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1850,availability,error,error,1850,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1442,deployability,contain,contained,1442,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1376,integrability,Messag,Message,1376,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1460,integrability,messag,message,1460,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1376,interoperability,Messag,Message,1376,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1460,interoperability,messag,message,1460,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1604,interoperability,specif,specifically,1604,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1734,interoperability,distribut,distribution,1734,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:37,performance,error,error,37,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1850,performance,error,error,1850,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:265,reliability,checkpoint,checkpoint,265,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:639,reliability,checkpoint,checkpoint,639,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:974,reliability,checkpoint,checkpoint,974,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:37,safety,error,error,37,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:91,safety,except,exceptions,91,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1850,safety,error,error,1850,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1060,security,auth,auth,1060,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1356,security,auth,authored,1356,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1471,security,privil,privileged,1471,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1488,security,confidential,confidential,1488,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1617,security,authoriz,authorized,1617,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:560,testability,plan,planned,560,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:37,usability,error,error,37,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:129,usability,indicat,indicating,129,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:240,usability,Close,Closed,240,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1850,usability,error,error,1850,"Given the old function now raises an error, could you at least add a. FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the. new function to be used? Thanks! On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > as not planned. >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:125,availability,checkpoint,checkpoint,125,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:498,availability,checkpoint,checkpoint,498,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:840,availability,checkpoint,checkpoint,840,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1716,availability,error,error,1716,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1308,deployability,contain,contained,1308,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1242,integrability,Messag,Message,1242,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1326,integrability,messag,message,1326,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1242,interoperability,Messag,Message,1242,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1326,interoperability,messag,message,1326,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1470,interoperability,specif,specifically,1470,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1600,interoperability,distribut,distribution,1600,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1716,performance,error,error,1716,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:125,reliability,checkpoint,checkpoint,125,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:498,reliability,checkpoint,checkpoint,498,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:840,reliability,checkpoint,checkpoint,840,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1716,safety,error,error,1716,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:926,security,auth,auth,926,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1222,security,auth,authored,1222,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1337,security,privil,privileged,1337,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1354,security,confidential,confidential,1354,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1483,security,authoriz,authorized,1483,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1716,usability,error,error,1716,"Thanks! On Wed, Jun 7, 2023 at 9:34 AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > to track that! >. > —. > Reply to this email directly, view it on GitHub. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,. > or unsubscribe. > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>. > . > You are receiving this because you authored the thread.Message ID:. > ***@***.***>. >. -- . PLEASE NOTE: The information contained in this message is privileged and . confidential, and is intended only for the use of the individual to whom it . is addressed and others who have been specifically authorized to receive . it. If you are not the intended recipient, you are hereby notified that any . dissemination, distribution, or copying of this communication is strictly . prohibited. If you have received this communication in error, or if any . problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/pull/2502:31,availability,ping,ping,31,"@grst is also a good person to ping for tutorials. In general, we'll also want to link to more scverse tutorials eventually",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:113,integrability,event,eventually,113,"@grst is also a good person to ping for tutorials. In general, we'll also want to link to more scverse tutorials eventually",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:21,usability,person,person,21,"@grst is also a good person to ping for tutorials. In general, we'll also want to link to more scverse tutorials eventually",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:170,interoperability,registr,registry,170,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58. * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:253,interoperability,registr,registry,253,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58. * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:315,performance,content,content,315,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58. * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:35,usability,learn,learn,35,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58. * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:147,interoperability,registr,registry,147,"OK, I just wanted to get things in. Is there an issue about cleaning everything up? (make the scanpy-tutorials.rtfd.io redirect, make the tutorial registry visible, …?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:122,deployability,manag,manageable,122,"No, there's currently no meta issue (feel free to create). . However the list of all issues in scverse-tutorials is still manageable: https://github.com/scverse/scverse-tutorials/issues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:12,energy efficiency,current,currently,12,"No, there's currently no meta issue (feel free to create). . However the list of all issues in scverse-tutorials is still manageable: https://github.com/scverse/scverse-tutorials/issues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:122,energy efficiency,manag,manageable,122,"No, there's currently no meta issue (feel free to create). . However the list of all issues in scverse-tutorials is still manageable: https://github.com/scverse/scverse-tutorials/issues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:122,safety,manag,manageable,122,"No, there's currently no meta issue (feel free to create). . However the list of all issues in scverse-tutorials is still manageable: https://github.com/scverse/scverse-tutorials/issues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/issues/2506:108,deployability,automat,automatically,108,"Closed via #2507. I didn’t realize that you didn’t use the magic “fixes #...” words, to make GitHub do this automatically",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:108,testability,automat,automatically,108,"Closed via #2507. I didn’t realize that you didn’t use the magic “fixes #...” words, to make GitHub do this automatically",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:0,usability,Close,Closed,0,"Closed via #2507. I didn’t realize that you didn’t use the magic “fixes #...” words, to make GitHub do this automatically",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/pull/2509:219,availability,cluster,clustermap,219,"Great idea. I think we should make them display a bit more compactly, but that’s not the job of this PR. Direct links to changed docs pages:. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.clustermap.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.stacked_violin.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.tracksplot.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:185,deployability,build,build,185,"Great idea. I think we should make them display a bit more compactly, but that’s not the job of this PR. Direct links to changed docs pages:. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.clustermap.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.stacked_violin.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.tracksplot.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:219,deployability,cluster,clustermap,219,"Great idea. I think we should make them display a bit more compactly, but that’s not the job of this PR. Direct links to changed docs pages:. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.clustermap.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.stacked_violin.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.tracksplot.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:279,deployability,build,build,279,"Great idea. I think we should make them display a bit more compactly, but that’s not the job of this PR. Direct links to changed docs pages:. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.clustermap.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.stacked_violin.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.tracksplot.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:377,deployability,build,build,377,"Great idea. I think we should make them display a bit more compactly, but that’s not the job of this PR. Direct links to changed docs pages:. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.clustermap.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.stacked_violin.html. - https://icb-scanpy--2509.com.readthedocs.build/en/2509/generated/scanpy.pl.tracksplot.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2512:156,deployability,version,version,156,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:10,energy efficiency,current,currently,10,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:52,energy efficiency,reduc,reduce,52,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:94,energy efficiency,heat,heatmaps,94,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:156,integrability,version,version,156,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:156,modifiability,version,version,156,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:20,testability,plan,planning,20,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:193,deployability,modul,module,193,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:364,deployability,depend,dependency,364,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:364,integrability,depend,dependency,364,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:45,modifiability,refact,refactoring,45,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:74,modifiability,pac,package,74,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:193,modifiability,modul,module,193,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:295,modifiability,pac,packages,295,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:364,modifiability,depend,dependency,364,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:45,performance,refactor,refactoring,45,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:193,safety,modul,module,193,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:364,safety,depend,dependency,364,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:364,testability,depend,dependency,364,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:393,availability,avail,available,393,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:370,deployability,api,api,370,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:38,energy efficiency,cool,cool,38,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:234,energy efficiency,heat,heatmap,234,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:254,energy efficiency,heat,heatmap-with-sized-elements,254,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:301,energy efficiency,Heat,Heatmap,301,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:370,integrability,api,api,370,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:370,interoperability,api,api,370,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:393,reliability,availab,available,393,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:393,safety,avail,available,393,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:393,security,availab,available,393,Thanks for your interest! It would be cool if `Marsilea` could be part of `Scanpy`. . The `Marsilea` is shipped with a wide range of plot options which already include the [dot plot](https://marsilea.readthedocs.io/en/latest/tutorial/heatmap.html#matrix-heatmap-with-sized-elements) (We call it Sized Heatmap). Here is a [list](https://marsilea.readthedocs.io/en/latest/api/plots.html) of all available plot options for your reference. . Any further suggestions or requests to expand the plot options are welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:31,testability,plan,planning,31,"Thank you! I added this to our planning document. I’ll close the PR since as said, we’re not adding to external anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:40,usability,document,document,40,"Thank you! I added this to our planning document. I’ll close the PR since as said, we’re not adding to external anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:55,usability,close,close,55,"Thank you! I added this to our planning document. I’ll close the PR since as said, we’re not adding to external anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2518:27,integrability,event,events,27,I added comments. The only events I’m not more or less sure about are “reopened” and ”edited”.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2518
https://github.com/scverse/scanpy/pull/2518:87,performance,time,times,87,"takes less than a second to run, so it doesn’t matter at all if it runs a few too many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2518
https://github.com/scverse/scanpy/pull/2518:39,reliability,doe,doesn,39,"takes less than a second to run, so it doesn’t matter at all if it runs a few too many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2518
https://github.com/scverse/scanpy/issues/2519:29,deployability,instal,install,29,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:74,energy efficiency,gpu,gpu,74,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:74,performance,gpu,gpu,74,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:78,usability,support,support,78,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:405,deployability,depend,dependency,405,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:40,integrability,configur,configurable,40,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:405,integrability,depend,dependency,405,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:122,interoperability,specif,specific,122,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:40,modifiability,configur,configurable,40,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:148,modifiability,maintain,maintained,148,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:405,modifiability,depend,dependency,405,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:135,reliability,doe,doesn,135,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:365,reliability,doe,does,365,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:148,safety,maintain,maintained,148,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:405,safety,depend,dependency,405,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:40,security,configur,configurable,40,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:405,testability,depend,dependency,405,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:216,usability,learn,learn,216,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2520:36,availability,consist,consistent,36,"i don't think it's a bug as this is consistent across visium readers versions, i would suggest to flip the spatial axis of adata.obsm[spatial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:69,deployability,version,versions,69,"i don't think it's a bug as this is consistent across visium readers versions, i would suggest to flip the spatial axis of adata.obsm[spatial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:69,integrability,version,versions,69,"i don't think it's a bug as this is consistent across visium readers versions, i would suggest to flip the spatial axis of adata.obsm[spatial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:69,modifiability,version,versions,69,"i don't think it's a bug as this is consistent across visium readers versions, i would suggest to flip the spatial axis of adata.obsm[spatial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:36,usability,consist,consistent,36,"i don't think it's a bug as this is consistent across visium readers versions, i would suggest to flip the spatial axis of adata.obsm[spatial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:115,modifiability,layer,layers,115,"I wonder if `dimensions` does too much or too little. Dimensions should always match, no? Flipping just one of the layers while plotting has no use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:25,reliability,doe,does,25,"I wonder if `dimensions` does too much or too little. Dimensions should always match, no? Flipping just one of the layers while plotting has no use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:141,availability,sli,slide,141,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:193,availability,sli,slide,193,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:24,performance,synch,synchronize,24,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:15,reliability,doe,does,15,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:141,reliability,sli,slide,141,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:193,reliability,sli,slide,193,"Yes, the image does not synchronize with flipped spatial dots. . I find a way to flip the image by changing the image coords:. hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]. slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2530:630,deployability,Stack,StackedViolin,630,"We have plot classes that are used for all plots. The `stacked_violin` function is not more than:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_stacked_violin.py#L679-L723. And the colorbar plotting is defined here:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L493-L520. So currently, you’d have to do something like this, but I agree, this should be easier. ```py. from matplotlib import colormaps. from matplotlib.colorbar import Colorbar. from matplotlib.cm import ScalarMappable. class MyStackedViolin(StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = StackedViolin(adata, var_names, ...). vp.make_figure(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:847,deployability,Stack,StackedViolin,847,"We have plot classes that are used for all plots. The `stacked_violin` function is not more than:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_stacked_violin.py#L679-L723. And the colorbar plotting is defined here:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L493-L520. So currently, you’d have to do something like this, but I agree, this should be easier. ```py. from matplotlib import colormaps. from matplotlib.colorbar import Colorbar. from matplotlib.cm import ScalarMappable. class MyStackedViolin(StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = StackedViolin(adata, var_names, ...). vp.make_figure(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:398,energy efficiency,current,currently,398,"We have plot classes that are used for all plots. The `stacked_violin` function is not more than:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_stacked_violin.py#L679-L723. And the colorbar plotting is defined here:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L493-L520. So currently, you’d have to do something like this, but I agree, this should be easier. ```py. from matplotlib import colormaps. from matplotlib.colorbar import Colorbar. from matplotlib.cm import ScalarMappable. class MyStackedViolin(StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = StackedViolin(adata, var_names, ...). vp.make_figure(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:592,modifiability,Scal,ScalarMappable,592,"We have plot classes that are used for all plots. The `stacked_violin` function is not more than:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_stacked_violin.py#L679-L723. And the colorbar plotting is defined here:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L493-L520. So currently, you’d have to do something like this, but I agree, this should be easier. ```py. from matplotlib import colormaps. from matplotlib.colorbar import Colorbar. from matplotlib.cm import ScalarMappable. class MyStackedViolin(StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = StackedViolin(adata, var_names, ...). vp.make_figure(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:713,modifiability,Scal,ScalarMappable,713,"We have plot classes that are used for all plots. The `stacked_violin` function is not more than:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_stacked_violin.py#L679-L723. And the colorbar plotting is defined here:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L493-L520. So currently, you’d have to do something like this, but I agree, this should be easier. ```py. from matplotlib import colormaps. from matplotlib.colorbar import Colorbar. from matplotlib.cm import ScalarMappable. class MyStackedViolin(StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = StackedViolin(adata, var_names, ...). vp.make_figure(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:122,deployability,Stack,StackedViolin,122,"@flying-sheep Thanks for helping! . I tried your code, but it does not work as expected. ```. class MyStackedViolin(sc.pl.StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = MyStackedViolin(adata_luminals, L1_signature,. groupby='leiden_r1',. cmap='coolwarm',. swap_axes = True). vp.make_figure(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/81d244e9-ade6-491c-b5c9-c80151aef316).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:414,energy efficiency,cool,coolwarm,414,"@flying-sheep Thanks for helping! . I tried your code, but it does not work as expected. ```. class MyStackedViolin(sc.pl.StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = MyStackedViolin(adata_luminals, L1_signature,. groupby='leiden_r1',. cmap='coolwarm',. swap_axes = True). vp.make_figure(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/81d244e9-ade6-491c-b5c9-c80151aef316).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:205,modifiability,Scal,ScalarMappable,205,"@flying-sheep Thanks for helping! . I tried your code, but it does not work as expected. ```. class MyStackedViolin(sc.pl.StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = MyStackedViolin(adata_luminals, L1_signature,. groupby='leiden_r1',. cmap='coolwarm',. swap_axes = True). vp.make_figure(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/81d244e9-ade6-491c-b5c9-c80151aef316).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:62,reliability,doe,does,62,"@flying-sheep Thanks for helping! . I tried your code, but it does not work as expected. ```. class MyStackedViolin(sc.pl.StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = MyStackedViolin(adata_luminals, L1_signature,. groupby='leiden_r1',. cmap='coolwarm',. swap_axes = True). vp.make_figure(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/81d244e9-ade6-491c-b5c9-c80151aef316).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:25,usability,help,helping,25,"@flying-sheep Thanks for helping! . I tried your code, but it does not work as expected. ```. class MyStackedViolin(sc.pl.StackedViolin):. def _plot_colorbar(self, color_legend_ax, normalize):. mappable = ScalarMappable(norm=normalize, cmap=colormaps[self.cmap]). Colorbar(color_legend_ax, mappable=mappable, orientation='vertical'). vp = MyStackedViolin(adata_luminals, L1_signature,. groupby='leiden_r1',. cmap='coolwarm',. swap_axes = True). vp.make_figure(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/81d244e9-ade6-491c-b5c9-c80151aef316).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:490,deployability,API,API,490,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py. def _plot_legend(self, legend_ax, return_ax_dict, normalize): . self._plot_colorbar(legend_ax, normalize) . return_ax_dict['color_legend_ax'] = color_legend_ax. ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:490,integrability,API,API,490,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py. def _plot_legend(self, legend_ax, return_ax_dict, normalize): . self._plot_colorbar(legend_ax, normalize) . return_ax_dict['color_legend_ax'] = color_legend_ax. ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:490,interoperability,API,API,490,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py. def _plot_legend(self, legend_ax, return_ax_dict, normalize): . self._plot_colorbar(legend_ax, normalize) . return_ax_dict['color_legend_ax'] = color_legend_ax. ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:230,testability,simpl,simply,230,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py. def _plot_legend(self, legend_ax, return_ax_dict, normalize): . self._plot_colorbar(legend_ax, normalize) . return_ax_dict['color_legend_ax'] = color_legend_ax. ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:230,usability,simpl,simply,230,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py. def _plot_legend(self, legend_ax, return_ax_dict, normalize): . self._plot_colorbar(legend_ax, normalize) . return_ax_dict['color_legend_ax'] = color_legend_ax. ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:573,energy efficiency,cool,coolwarm,573,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:911,energy efficiency,cool,coolwarm,911,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:1249,energy efficiency,cool,coolwarm,1249,"ge](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle and only keep the last one. Could you please show me how to delete it? . Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:1587,energy efficiency,cool,coolwarm,1587,"ge](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle and only keep the last one. Could you please show me how to delete it? . Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:1840,energy efficiency,draw,draw,1840,"ge](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle and only keep the last one. Could you please show me how to delete it? . Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:222,integrability,sub,subplots,222,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:340,integrability,sub,subplots,340,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:66,security,control,control,66,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:66,testability,control,control,66,"@flying-sheep Thanks! Now it works better, but the size is out of control. ![image](https://github.com/scverse/scanpy/assets/33963919/75aff552-647b-4097-b8be-f366655c7a45). When I combine several plots together using `plt.subplots`. ```. from matplotlib.pyplot import rc_context. with rc_context({'figure.figsize': (3, 3)}):. fig, ax = plt.subplots(1, 4, figsize=(12,4)). ax[0] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[0]], . var_names = gene, use_raw = True, ax=ax[0],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[0], show=False). ax[1] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[1]], . var_names = gene, use_raw = True, ax=ax[1],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[1], show=False). ax[2] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[2]], . var_names = gene, use_raw = True, ax=ax[2],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[2], show=False). ax[3] = sc.pl.stacked_violin(adata_luminals[adata_luminals.obs['sample'] == adata_luminals.obs['sample'].cat.categories[3]], . var_names = gene, use_raw = True, ax=ax[3],. groupby='leiden_r1', . cmap='coolwarm', dendrogram=False,. swap_axes = True, stripplot = False,. title = adata_luminals.obs['sample'].cat.categories[3], show=False). #fig.delaxes(fig.axes[11]). #fig.delaxes(fig.axes[18]). #fig.delaxes(fig.axes[25]). #fig.delaxes(fig.axes[32]). plt.draw(). ```. ![image](https://github.com/scverse/scanpy/assets/33963919/30a459f3-d562-4b7a-b39a-36b7d836cda8). I would like to omit the color bars in the middle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:15,testability,simpl,simply,15,That should be simply `groupby='sample'` instead of multiple plot calls.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:15,usability,simpl,simply,15,That should be simply `groupby='sample'` instead of multiple plot calls.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:82,modifiability,variab,variable,82,"Yes I would like to separate both `sample` and `leiden_r1`, I should create a new variable `adata.obs['leiden+sample']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2531:99,deployability,build,build,99,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:183,deployability,updat,updating,183,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:981,deployability,stack,stack-data,981,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:300,modifiability,deco,decorator,300,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:589,modifiability,pac,packaging,589,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:544,performance,network,networkx,544,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:91,reliability,doe,doesn,91,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:183,safety,updat,updating,183,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:183,security,updat,updating,183,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:544,security,network,networkx,544,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:947,security,session,session-info,947,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:707,usability,tool,toolkit,707,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:901,usability,learn,learn,901,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1134,usability,learn,learn,1134,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment? ```. anndata==0.9.1. asttokens==2.2.1. backcall==0.2.0. contourpy==1.1.0. cycler==0.11.0. decorator==5.1.1. executing==1.2.0. fonttools==4.40.0. h5py==3.9.0. igraph==0.10.4. ipython==8.14.0. jedi==0.18.2. joblib==1.2.0. kiwisolver==1.4.4. llvmlite==0.40.1. louvain==0.8.0. matplotlib==3.7.1. matplotlib-inline==0.1.6. natsort==8.4.0. networkx==3.1. numba==0.57.1. numpy==1.24.3. packaging==23.1. pandas==2.0.2. parso==0.8.3. patsy==0.5.3. pexpect==4.8.0. pickleshare==0.7.5. Pillow==9.5.0. prompt-toolkit==3.0.38. ptyprocess==0.7.0. pure-eval==0.2.2. Pygments==2.15.1. pynndescent==0.5.10. pyparsing==3.1.0. python-dateutil==2.8.2. python-igraph==0.10.4. pytz==2023.3. scanpy==1.9.3. scikit-learn==1.2.2. scipy==1.11.0. seaborn==0.12.2. session-info==1.0.0. six==1.16.0. stack-data==0.6.2. statsmodels==0.14.0. stdlib-list==0.9.0. texttable==1.6.7. threadpoolctl==3.1.0. tqdm==4.65.0. traitlets==5.9.0. tzdata==2023.3. umap-learn==0.5.3. wcwidth==0.2.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:39,deployability,updat,updating,39,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:144,deployability,updat,updated,144,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:152,deployability,version,versions,152,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:167,deployability,Version,Versions,167,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2314,deployability,updat,updated,2314,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:236,energy efficiency,Core,CoreFoundation,236,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:454,energy efficiency,cloud,cloudpickle,454,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:152,integrability,version,versions,152,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:167,integrability,Version,Versions,167,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1266,interoperability,platform,platformdirs,1266,"L 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:152,modifiability,version,versions,152,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:167,modifiability,Version,Versions,167,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:597,modifiability,deco,decorator,597,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1158,modifiability,pac,packaging,1158,### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. ---,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2183,modifiability,pac,packaged,2183,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:39,safety,updat,updating,39,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:144,safety,updat,updated,144,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2314,safety,updat,updated,2314,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:39,security,updat,updating,39,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:144,security,updat,updated,144,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:395,security,certif,certifi,395,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1774,security,soc,socks,1774,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2294,security,Session,Session,2294,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2314,security,updat,updated,2314,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1875,usability,tool,toolz,1875,"3.1.0. babel 2.12.1. backcall 0.2.0. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.3. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_jupyter_utils NA. pydev_jupyter_vars NA. pydevconsole NA. pydevd_file_utils NA. pydevd_plugins NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.11.0. seaborn 0.12.2. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. statsmodels 0.14.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.1. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-26 14:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:38,availability,avail,available,38,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:73,deployability,instal,install,73,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:93,deployability,version,versions,93,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:93,integrability,version,versions,93,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:106,interoperability,specif,specified,106,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:85,modifiability,pac,package,85,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:93,modifiability,version,versions,93,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:19,reliability,doe,doesn,19,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:38,reliability,availab,available,38,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:38,safety,avail,available,38,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:38,security,availab,available,38,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:196,availability,down,download,196,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:208,deployability,instal,install,208,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:228,deployability,version,version,228,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:256,deployability,version,version,256,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:628,deployability,instal,installation,628,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:721,deployability,manag,manage,721,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:721,energy efficiency,manag,manage,721,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:228,integrability,version,version,228,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:256,integrability,version,version,256,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:704,interoperability,specif,specifically,704,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:228,modifiability,version,version,228,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:256,modifiability,version,version,256,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:305,modifiability,pac,packages,305,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:731,modifiability,pac,packages,731,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:721,safety,manag,manage,721,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:452,testability,simpl,simple,452,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:452,usability,simpl,simple,452,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:. ```. WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)). Reason for being yanked: License Violation. ```. Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:118,usability,close,close,118,"The scipy thing is just a fix for this: https://github.com/scipy/scipy/issues/18765. Nothing suspicious luckily! I’ll close this, but if you find something we can fix here, we’re happy to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/pull/2536:25,deployability,API,API,25,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:25,integrability,API,API,25,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:156,integrability,transform,transformers,156,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:25,interoperability,API,API,25,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:156,interoperability,transform,transformers,156,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:189,interoperability,specif,specify,189,"@ivirshup apart from the API design, this is done. We’re waiting on sklearn-ann to get upstreamed, but we could also just finish it without the third party transformers (allowing people to specify their own). What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:257,modifiability,maintain,maintain,257,"What “Usage” section? We have a “Usage principles” page and functions/classes can have an “Examples” section. I’m pretty sure there was a very good reason for not passing an instance, but this was all so long ago that I forgot. I’ll see if it’s possible to maintain backwards compat while doing that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:257,safety,maintain,maintain,257,"What “Usage” section? We have a “Usage principles” page and functions/classes can have an “Examples” section. I’m pretty sure there was a very good reason for not passing an instance, but this was all so long ago that I forgot. I’ll see if it’s possible to maintain backwards compat while doing that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:276,modifiability,maintain,maintain,276,"> What “Usage” section? We have a “Usage principles” page and functions/classes can have an “Examples” section. Yes, examples. > I’m pretty sure there was a very good reason for not passing an instance, but this was all so long ago that I forgot. I’ll see if it’s possible to maintain backwards compat while doing that! Maybe it was about passing in arguments to the function? But I do think it would be quite useful to pass in a class that gets `.fit` so you can keep a reference to the index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:276,safety,maintain,maintain,276,"> What “Usage” section? We have a “Usage principles” page and functions/classes can have an “Examples” section. Yes, examples. > I’m pretty sure there was a very good reason for not passing an instance, but this was all so long ago that I forgot. I’ll see if it’s possible to maintain backwards compat while doing that! Maybe it was about passing in arguments to the function? But I do think it would be quite useful to pass in a class that gets `.fit` so you can keep a reference to the index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:109,deployability,manag,managed,109,"I think it was just so convoluted that it seemed too hard to do while keeping backwards compatibility, but I managed it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:109,energy efficiency,manag,managed,109,"I think it was just so convoluted that it seemed too hard to do while keeping backwards compatibility, but I managed it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:88,interoperability,compatib,compatibility,88,"I think it was just so convoluted that it seemed too hard to do while keeping backwards compatibility, but I managed it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:109,safety,manag,managed,109,"I think it was just so convoluted that it seemed too hard to do while keeping backwards compatibility, but I managed it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/issues/2540:15,deployability,updat,update,15,can you please update your code sample so we can just copy and paste it? There’s an `import scanpy as ac` and a `adata = ???` missing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:15,safety,updat,update,15,can you please update your code sample so we can just copy and paste it? There’s an `import scanpy as ac` and a `adata = ???` missing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:15,security,updat,update,15,can you please update your code sample so we can just copy and paste it? There’s an `import scanpy as ac` and a `adata = ???` missing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:293,deployability,resourc,resources,293,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:293,energy efficiency,resourc,resources,293,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:293,performance,resourc,resources,293,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:293,safety,resourc,resources,293,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:74,security,ssl,ssl,74,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:169,security,ssl,ssl,169,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:205,security,ssl,ssl,205,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:293,testability,resourc,resources,293,"Here we go:. ```py. import scanpy as sc. from pathlib import Path. import ssl. import urllib.request. adata_file = ""test_adata.h5ad"". if not Path(adata_file).exists():. ssl._create_default_https_context = ssl._create_unverified_context. urllib.request.urlretrieve(""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file). adata = sc.read_h5ad(adata_file). t_celltypes = [""Naive CD4+ T cells"", ""Naive CD8+ T cells""]. sc.tl.rank_genes_groups(. adata,. groupby=""cell_type"",. groups=t_celltypes,. reference=""rest"",. method=""wilcoxon"",. ). # This works. sc.pl.rank_genes_groups(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). # We get the KeyError here. sc.pl.rank_genes_groups_violin(. adata, groups=t_celltypes, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:41,integrability,sub,subscribe,41,"Ah, this is a duplicate of #2258, please subscribe there! someone in a linked issue said that `use_raw=True` can help, but we’ll nevertheless fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:113,usability,help,help,113,"Ah, this is a duplicate of #2258, please subscribe there! someone in a linked issue said that `use_raw=True` can help, but we’ll nevertheless fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/pull/2545:62,interoperability,specif,specifying,62,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:210,reliability,Doe,Does,210,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:19,safety,test,testing,19,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:196,safety,valid,valid,196,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:508,safety,test,tested,508,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:696,safety,test,tests,696,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:19,testability,test,testing,19,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:508,testability,test,tested,508,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:696,testability,test,tests,696,"> Nice! Needs some testing though to make sure it works (e.g. specifying a sequence of markers). > . > You should also make sure the types are correct: `Optional[X]` means `X | None`. Is `None` a valid option? Does it make sense that the type is different in multiple spots? (`Optional[Sequence[str]]` vs `Union[str, Sequence[str], None]` vs `Union[str, Sequence[str]]`). You're absolutely right about the types. I changed the types such as all are accepting the same `Union[str, Sequence[str]]` now. I also tested several situations I could think of, in [this notebook](https://colab.research.google.com/drive/1ltg0Qs_dlxS_RMuN7z1DdLLV7VLBtZtd?usp=sharing) and they all worked fine. If any more tests (different situations) are needed, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:79,modifiability,paramet,parameter,79,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:23,safety,test,test,23,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:115,safety,test,test,115,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:150,safety,test,tests,150,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:213,safety,test,tests,213,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:267,safety,test,test,267,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:360,safety,test,tests,360,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:18,testability,unit,unit,18,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:23,testability,test,test,23,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:115,testability,test,test,115,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:150,testability,test,tests,150,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:213,testability,test,tests,213,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:267,testability,test,test,267,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:360,testability,test,tests,360,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:407,usability,help,help,407,"I’m talking about unit test. please add one here. You can just add one or more parameter sets to the list, and the test will create a file at `scanpy/tests/figures/master_{id}.png`. Just copy that file to `scanpy/tests/_images/` and commit it together with the added test. https://github.com/scverse/scanpy/blob/9c6996f374370a7d50768c634e76e50173d22839/scanpy/tests/test_plotting.py#L973-L1082. if you need help, please tell us!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:239,availability,error,error,239,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:221,deployability,fail,fails,221,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:320,deployability,fail,fails,320,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:239,performance,error,error,239,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:221,reliability,fail,fails,221,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:320,reliability,fail,fails,320,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:46,safety,test,tests,46,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:239,safety,error,error,239,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:343,safety,test,test,343,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:46,testability,test,tests,46,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:253,testability,understand,understandable,253,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:343,testability,test,test,343,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:239,usability,error,error,239,"Thanks for the reference, I added a number of tests to where you mentioned. I also changed the `broadcasting` method for `markers` so now it has the same process as `color` and `dimensions`, and therefore if broadcasting fails, the output error is more understandable (same as when `color` and `dimensions` broadcasting fails). I also added a test for `marker broadcasting`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2546:89,availability,sla,slated,89,@ivirshup this is probably a good idea if https://github.com/scverse/anndata/pull/999 is slated to go into some future anndata version. can we merge that other PR anyway? you didn’t respond there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:127,deployability,version,version,127,@ivirshup this is probably a good idea if https://github.com/scverse/anndata/pull/999 is slated to go into some future anndata version. can we merge that other PR anyway? you didn’t respond there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:127,integrability,version,version,127,@ivirshup this is probably a good idea if https://github.com/scverse/anndata/pull/999 is slated to go into some future anndata version. can we merge that other PR anyway? you didn’t respond there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:127,modifiability,version,version,127,@ivirshup this is probably a good idea if https://github.com/scverse/anndata/pull/999 is slated to go into some future anndata version. can we merge that other PR anyway? you didn’t respond there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:89,reliability,sla,slated,89,@ivirshup this is probably a good idea if https://github.com/scverse/anndata/pull/999 is slated to go into some future anndata version. can we merge that other PR anyway? you didn’t respond there,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:20,deployability,fail,fails,20,I added a test that fails on master,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:20,reliability,fail,fails,20,I added a test that fails on master,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:10,safety,test,test,10,I added a test that fails on master,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:10,testability,test,test,10,I added a test that fails on master,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/issues/2547:101,energy efficiency,reduc,reduce,101,I realized that this is an issue originated from coercing the sparse matrix to be of np.int8 type to reduce the size the ann object and the mtx file to be written for another application. Making it np.int32 fixes the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:51,availability,error,error,51,"@maximillo Hi, sorry to bother you, I met the same error. But I don't konw how to change np.int8 to np.int32. Any help would be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:51,performance,error,error,51,"@maximillo Hi, sorry to bother you, I met the same error. But I don't konw how to change np.int8 to np.int32. Any help would be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:51,safety,error,error,51,"@maximillo Hi, sorry to bother you, I met the same error. But I don't konw how to change np.int8 to np.int32. Any help would be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:51,usability,error,error,51,"@maximillo Hi, sorry to bother you, I met the same error. But I don't konw how to change np.int8 to np.int32. Any help would be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:114,usability,help,help,114,"@maximillo Hi, sorry to bother you, I met the same error. But I don't konw how to change np.int8 to np.int32. Any help would be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2550:207,modifiability,maintain,maintained,207,"I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! @davidsebfischer it’s maintained, right? I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:207,safety,maintain,maintained,207,"I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! @davidsebfischer it’s maintained, right? I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:239,usability,close,close,239,"I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! @davidsebfischer it’s maintained, right? I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:215,modifiability,maintain,maintained,215,"> I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! > . > @davidsebfischer it’s maintained, right? > . > I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all. Hi Philipp, I will try diffxpy. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:215,safety,maintain,maintained,215,"> I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! > . > @davidsebfischer it’s maintained, right? > . > I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all. Hi Philipp, I will try diffxpy. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:253,usability,close,close,253,"> I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot! > . > @davidsebfischer it’s maintained, right? > . > I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all. Hi Philipp, I will try diffxpy. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2551:74,availability,error,error,74,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:720,deployability,stack,stackoverflow,720,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:390,energy efficiency,alloc,allocated,390,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:520,energy efficiency,alloc,allocates,520,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:805,energy efficiency,alloc,allocate,805,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:74,performance,error,error,74,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:419,performance,memor,memory,419,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:530,performance,memor,memory,530,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:74,safety,error,error,74,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:98,security,access,access,98,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:21,usability,help,help,21,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:74,usability,error,error,74,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:302,usability,close,close,302,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:419,usability,memor,memory,419,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:453,usability,close,close,453,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:530,usability,memor,memory,530,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:897,usability,help,help,897,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:76,availability,error,error,76,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:732,deployability,stack,stackoverflow,732,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1254,deployability,stack,stackoverflow,1254,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:398,energy efficiency,alloc,allocated,398,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:530,energy efficiency,alloc,allocates,530,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:817,energy efficiency,alloc,allocate,817,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:76,performance,error,error,76,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:427,performance,memor,memory,427,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:540,performance,memor,memory,540,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1020,performance,memor,memory,1020,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1068,performance,memor,memory,1068,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1146,performance,memor,memory,1146,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1326,performance,time,time,1326,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:76,safety,error,error,76,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:100,security,access,access,100,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:23,usability,help,help,23,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:76,usability,error,error,76,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:310,usability,close,close,310,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:427,usability,memor,memory,427,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:461,usability,close,close,461,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:540,usability,memor,memory,540,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:909,usability,help,help,909,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1020,usability,memor,memory,1020,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1068,usability,memor,memory,1068,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1146,usability,memor,memory,1146,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. > . > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes? > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code. > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help? Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2556:72,integrability,sub,subsample,72,"Could you please provide a minimal working example of this bug? Can you subsample your AnnData objects and maybe create an artificial recreation of it here, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:27,usability,minim,minimal,27,"Could you please provide a minimal working example of this bug? Can you subsample your AnnData objects and maybe create an artificial recreation of it here, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:209,deployability,log,logarithmize,209,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1413,deployability,log,logarithmize,1413,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:112,integrability,batch,batch,112,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:737,integrability,Sub,Subject,737,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:794,integrability,batch,batch,794,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:373,interoperability,coordinat,coordinates,373,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:999,interoperability,coordinat,coordinates,999,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:162,modifiability,layer,layers,162,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:933,modifiability,layer,layers,933,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1342,modifiability,layer,layers,1342,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:112,performance,batch,batch,112,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:794,performance,batch,batch,794,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:587,reliability,doe,does,587,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1469,reliability,doe,doesn,1469,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:209,safety,log,logarithmize,209,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1413,safety,log,logarithmize,1413,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:209,security,log,logarithmize,209,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1413,security,log,logarithmize,1413,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:209,testability,log,logarithmize,209,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1413,testability,log,logarithmize,1413,"This is the anndata working properly:. ```. AnnData object with n_obs × n_vars = 24759 × 29612. obs: 'sample', 'batch', 'n_counts'. var: 'ensembl_id', 'n_cells'. layers: 'counts'. ```. After normalization and logarithmize it with:. ```. adata.X = sc.pp.normalize_total(adata, inplace=False)['X']. adata.X = sc.pp.log1p(adata.X). ```. And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:. ```. AnnData object with n_obs × n_vars = 17217 × 33704. obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'. var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'. obsm: 'X_umap'. layers: 'counts'. ```. Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: . ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):. ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:. ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2557:32,deployability,scale,scalefactor,32,"Ok, Thanks, I think it lack the scalefactor file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2557:32,energy efficiency,scale,scalefactor,32,"Ok, Thanks, I think it lack the scalefactor file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2557:32,modifiability,scal,scalefactor,32,"Ok, Thanks, I think it lack the scalefactor file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2557:32,performance,scale,scalefactor,32,"Ok, Thanks, I think it lack the scalefactor file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2558:206,performance,time,time,206,"The only red I can see is from the image data. We don’t influence that, so it must be in the initial image you added. If that is not what you wanted to know, please explain. I’m going to close this for the time being. Happy to re-open if this is not solved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2558
https://github.com/scverse/scanpy/issues/2558:187,usability,close,close,187,"The only red I can see is from the image data. We don’t influence that, so it must be in the initial image you added. If that is not what you wanted to know, please explain. I’m going to close this for the time being. Happy to re-open if this is not solved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2558
https://github.com/scverse/scanpy/pull/2561:40,testability,simpl,simplifies,40,"@ivirshup and I went over this, it just simplifies the doc setup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/pull/2561:40,usability,simpl,simplifies,40,"@ivirshup and I went over this, it just simplifies the doc setup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/pull/2563:100,safety,test,testing,100,"@flying-sheep , haven't considered all combinations yet but wanted to check if this way is good for testing warnings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:100,testability,test,testing,100,"@flying-sheep , haven't considered all combinations yet but wanted to check if this way is good for testing warnings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:66,testability,simpl,simply,66,"Looks great! You can get rid of `expect_warning`, as you can just simply check `if expected_warning_message is not None` instead, otherwise pretty ideal!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:66,usability,simpl,simply,66,"Looks great! You can get rid of `expect_warning`, as you can just simply check `if expected_warning_message is not None` instead, otherwise pretty ideal!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:134,deployability,build,build,134,"You also need to fix the links. `dask.arrays.linalg.svd_compressed` should be `dask.arrays.linalg.svd_compressed`, then the docs will build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/issues/2564:38,deployability,version,version,38,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:76,deployability,releas,release,76,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:38,integrability,version,version,38,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:38,modifiability,version,version,38,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2565:277,usability,support,support,277,"`tissue_positions_list.csv` is spaceranger 1.3, `tissue_positions.csv ` is spaceranger 2.0. The code expects one of them:. https://github.com/scverse/scanpy/blob/05068c347ac176d664ada3b0d5979255d35ce686/scanpy/readwrite.py#L417-L421. Seems like you have none of them. We don’t support incomplete or otherwise nonstandard output directories. If you think you have a normal output directory, please show us a directory listing and point us to the docs where the 10x people explain the changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:74,deployability,instal,installed,74,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:145,deployability,instal,installed,145,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:56,safety,detect,detected,56,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:56,security,detect,detected,56,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:4,usability,confirm,confirm,4,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:13,deployability,releas,release,13,Seems like a release is in order. that code was merged in April,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/pull/2569:11,reliability,doe,does,11,> Also how does this work for. you seem to have sent this too early. > What about using the checklist on anndata and just checking that all the checkboxes in the PR description have been ticked? That gives me an idea!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:64,deployability,releas,release,64,"OK, this should work. The only issue is that if users check “No release notes necessary” while not checking another box, the “check-relnotes” job still runs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:48,usability,user,users,48,"OK, this should work. The only issue is that if users check “No release notes necessary” while not checking another box, the “check-relnotes” job still runs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2577:207,interoperability,registr,registry,207,"Dear @katosh,. unfortunately, we do not accept contributions to external anymore and are deprecating it. However, we'd happily list mellon in the [scverse ecosystem](https://scverse.org/packages/#ecosystem) registry if all requirements are ticked off. Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:186,modifiability,pac,packages,186,"Dear @katosh,. unfortunately, we do not accept contributions to external anymore and are deprecating it. However, we'd happily list mellon in the [scverse ecosystem](https://scverse.org/packages/#ecosystem) registry if all requirements are ticked off. Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:121,deployability,depend,depend,121,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:121,integrability,depend,depend,121,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:121,modifiability,depend,depend,121,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:484,modifiability,pac,packages,484,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:112,reliability,doe,does,112,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:350,reliability,doe,does,350,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:381,reliability,doe,does,381,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:121,safety,depend,depend,121,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:121,testability,depend,depend,121,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html. Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:123,deployability,depend,depend,123,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:123,integrability,depend,depend,123,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:123,modifiability,depend,depend,123,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:485,modifiability,pac,packages,485,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:114,reliability,doe,does,114,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:351,reliability,doe,does,351,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:382,reliability,doe,does,382,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:123,safety,depend,depend,123,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:123,testability,depend,depend,123,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:592,usability,support,support,592,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless? No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:16,deployability,depend,dependency,16,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:16,integrability,depend,dependency,16,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:16,modifiability,depend,dependency,16,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:16,safety,depend,dependency,16,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:16,testability,depend,dependency,16,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:179,testability,understand,understand,179,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:47,usability,interact,interaction,47,"AnnData a large dependency given that the only interaction is. ```python. ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]). ```. I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/issues/2578:35,usability,support,support,35,"For the highly_variable_genes dask support, are you also considering cases where the dask chunks are scipy sparse matrices?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:4,energy efficiency,current,currently,4,"Not currently, but since the scope of that PR got reduced, it shouldn’t be too much work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:50,energy efficiency,reduc,reduced,50,"Not currently, but since the scope of that PR got reduced, it shouldn’t be too much work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2579:40,availability,sli,slight,40,"Interestingly, the same code run with a slight modification produces the intended behaviour. Is this intended? `. adata_sub_master = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. adata_sub = adata_sub_master.copy(). sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes"")`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:40,reliability,sli,slight,40,"Interestingly, the same code run with a slight modification produces the intended behaviour. Is this intended? `. adata_sub_master = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. adata_sub = adata_sub_master.copy(). sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes"")`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:47,security,modif,modification,47,"Interestingly, the same code run with a slight modification produces the intended behaviour. Is this intended? `. adata_sub_master = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. adata_sub = adata_sub_master.copy(). sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes"")`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:82,usability,behavi,behaviour,82,"Interestingly, the same code run with a slight modification produces the intended behaviour. Is this intended? `. adata_sub_master = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. adata_sub = adata_sub_master.copy(). sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes"")`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2583:57,deployability,automat,automatically,57,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:41,interoperability,plug,plugin,41,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:57,testability,automat,automatically,57,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:108,usability,hint,hints,108,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:239,availability,operat,operations,239,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:86,deployability,Updat,Updated,86,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:720,deployability,updat,update,720,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:325,interoperability,semant,semantically,325,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:602,interoperability,specif,specific,602,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:765,performance,memor,memory,765,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:86,safety,Updat,Updated,86,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:720,safety,updat,update,720,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:72,security,hack,hackathon,72,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:86,security,Updat,Updated,86,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:720,security,updat,update,720,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:824,security,modif,modify,824,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:765,usability,memor,memory,765,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706. * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`. * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`. * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658. * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:72,deployability,updat,update,72,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …). - leave original AnnData alone, return. - new AnnData. - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done. but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:130,modifiability,layer,layer,130,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …). - leave original AnnData alone, return. - new AnnData. - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done. but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:72,safety,updat,update,72,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …). - leave original AnnData alone, return. - new AnnData. - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done. but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:72,security,updat,update,72,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …). - leave original AnnData alone, return. - new AnnData. - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done. but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:26,usability,Behavi,Behaviors,26,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …). - leave original AnnData alone, return. - new AnnData. - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done. but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:245,integrability,rout,route,245,What if we had copy-on-write behavior for AnnData? Then we could never modify AnnData inplace but always return a view of an AnnData with references to the objects that were unchanged and only the new data added. . Pandas seems to be going that route: https://github.com/pandas-dev/pandas/blob/57390ada100466dac777e5b66d5a4f2a72700c38/web/pandas/pdeps/0008-inplace-methods-in-pandas.md (HT @bernheder),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:71,security,modif,modify,71,What if we had copy-on-write behavior for AnnData? Then we could never modify AnnData inplace but always return a view of an AnnData with references to the objects that were unchanged and only the new data added. . Pandas seems to be going that route: https://github.com/pandas-dev/pandas/blob/57390ada100466dac777e5b66d5a4f2a72700c38/web/pandas/pdeps/0008-inplace-methods-in-pandas.md (HT @bernheder),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:29,usability,behavi,behavior,29,What if we had copy-on-write behavior for AnnData? Then we could never modify AnnData inplace but always return a view of an AnnData with references to the objects that were unchanged and only the new data added. . Pandas seems to be going that route: https://github.com/pandas-dev/pandas/blob/57390ada100466dac777e5b66d5a4f2a72700c38/web/pandas/pdeps/0008-inplace-methods-in-pandas.md (HT @bernheder),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:24,deployability,version,version,24,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:24,integrability,version,version,24,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:24,modifiability,version,version,24,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:266,performance,time,time,266,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:173,security,modif,modification,173,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:65,testability,understand,understand,65,"Our views are a limited version of COW (`_init_as_actual`). If I understand the idea correctly, I’d be for it if we could guarantee that all nested data structures delegate modification to us. I don’t know if that’s possible with things like dask arrays and (at the time of writing) pandas data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:243,availability,state,states,243,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:130,deployability,contain,container-like,130,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:243,integrability,state,states,243,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:296,modifiability,paramet,parameter,296,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:345,security,modif,modify,345,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:226,usability,document,document,226,"@flying-sheep This guarantee seems hard to ask for though? @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. E.g. even for data frames for value changes that document already states the following: . > In contrast, the `inplace` parameter will be kept for any methods that only modify the underlying data of a pandas object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:184,deployability,contain,container-like,184,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:233,integrability,coupl,couple,233,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:233,modifiability,coupl,couple,233,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:394,modifiability,layer,layer,394,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:917,reliability,doe,does,917,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:1030,security,modif,modify,1030,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:233,testability,coupl,couple,233,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:871,usability,support,support,871,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:938,usability,behavi,behavior,938,"> @flying-sheep This guarantee seems hard to ask for though? Agree it's challenging, but maybe not impossible. > @grst It would be great but it's unclear how to do that properly for a container-like data structure such as AnnData. A couple of examples how I think this would look like: . * `sc.tl.pca` adds a key to `.varm`, to `.obsm` and to `.uns`. We create a new AnnData object where every layer, every element of `varm`, every element of `.obsm`, `.X`, `.obs`, and `.var` are references to the objects already present in the previous AnnData object. There's now just additional keys in `.obsm` and `.varm` pointing to the new objects. * assign a new column in obs: `adata.obs[""key""] = value`. Create a new AnnData object where all elements are pointers to the objects that already exist. Only `.obs` becomes a copy of the previous `obs` data frame (for objects that support copy on write themselves, like pandas does (or will?), the behavior could be delegated and not even the entire data frame would need to be copied. . * modify a value in `X`: `adata.X[20, 25] = 42`. Again everything would be pointers to the old objects, but X needs to be copied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2584:129,usability,support,supported,129,"It’s possible, just use `na_color`. Any [matplotlib color](https://matplotlib.org/stable/tutorials/colors/colors.html) should be supported",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2584:217,deployability,contain,contain,217,"@flying-sheep This doesn't seem to work. I tried with two matplotlib colours, 'w' and 'C1' using the code. `sc.pl.spatial(A1, img_key=""hires"", color=[""Lag3""], bw=True, na_color='w')`. I know that a lot of these spots contain zero counts. I just want zero counts to be transparent. Should I try it a different way? ![Screen Shot 2023-08-11 at 2 27 22 pm](https://github.com/scverse/scanpy/assets/32261323/fa0d88ef-d7da-43d6-ac47-b229d27f8a8c).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2584:19,reliability,doe,doesn,19,"@flying-sheep This doesn't seem to work. I tried with two matplotlib colours, 'w' and 'C1' using the code. `sc.pl.spatial(A1, img_key=""hires"", color=[""Lag3""], bw=True, na_color='w')`. I know that a lot of these spots contain zero counts. I just want zero counts to be transparent. Should I try it a different way? ![Screen Shot 2023-08-11 at 2 27 22 pm](https://github.com/scverse/scanpy/assets/32261323/fa0d88ef-d7da-43d6-ac47-b229d27f8a8c).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2584:307,deployability,api,api,307,"Ah, you also need to set the spots that you want to be that color to `NaN`. Furthermore, for transparency, you need to use an RGBA color (the “A” stands for alpha channel, which is how opaque a color is). You can also use something like [`matplotlib.colors.to_rgba('C1', .3)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.to_rgba.html#matplotlib.colors.to_rgba) to make an opaque color transparent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2584:307,integrability,api,api,307,"Ah, you also need to set the spots that you want to be that color to `NaN`. Furthermore, for transparency, you need to use an RGBA color (the “A” stands for alpha channel, which is how opaque a color is). You can also use something like [`matplotlib.colors.to_rgba('C1', .3)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.to_rgba.html#matplotlib.colors.to_rgba) to make an opaque color transparent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2584:307,interoperability,api,api,307,"Ah, you also need to set the spots that you want to be that color to `NaN`. Furthermore, for transparency, you need to use an RGBA color (the “A” stands for alpha channel, which is how opaque a color is). You can also use something like [`matplotlib.colors.to_rgba('C1', .3)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.to_rgba.html#matplotlib.colors.to_rgba) to make an opaque color transparent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/issues/2586:538,energy efficiency,current,currently,538,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:988,interoperability,distribut,distribution,988,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1042,interoperability,distribut,distribution,1042,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:413,reliability,doe,does,413,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:639,reliability,doe,does,639,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:82,safety,test,test,82,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:313,safety,test,tests,313,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:333,safety,test,test,333,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:367,safety,test,test,367,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:389,safety,test,test,389,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:326,security,sign,signed,326,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:82,testability,test,test,82,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:313,testability,test,tests,313,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:333,testability,test,test,333,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:367,testability,test,test,367,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:389,testability,test,test,389,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:880,usability,tool,tools,880,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1333,usability,help,helps,1333,"Hi,. thanks for your interest in scanpy! Regarding your question on ordering, and test statistic scores vs p-values:. 1. The structured array is [ordered according to scores](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups), not the p-values. 2. For tests with a signed test statistic (for example the t-test and the wilcoxon test), a ‘larger’ score does necessarily correspond to a lower p-value: rather, a score ‘further away from 0’ corresponds to lower p-value. Hence as currently the output stored in `adata.uns['rank_genes_groups']` is ordered according to scores, this does not mean it is ordered according to p-values. To the part where you mention you went into the code: My best guess is you refer to [this computation](https://github.com/scverse/scanpy/blob/1df92d61f00cec83b2ab2feb9c977cfcda84ebc0/scanpy/tools/_rank_genes_groups.py#L290). Here, considering the absolute value is used to get the two tails of the distribution when using the survival function (`stats.distribution.sf`). Generally if you’re interested in the Wilcoxon computation and the normal approximation used here, you can check [this](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) for a start. This also introduces the relation between the scores and the p-value. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:50,usability,close,close,50,"I hope this has answered your question, so I will close the issue now - kindly let us know if you have a follow up question to this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:480,availability,down,downregulation,480,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:293,energy efficiency,current,currently,293,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:211,interoperability,convers,conversely,211,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:66,safety,test,tests,66,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:84,safety,test,test,84,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:108,safety,test,tests,108,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:127,safety,test,tests,127,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:77,security,sign,signed,77,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:15,testability,understand,understand,15,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:66,testability,test,tests,66,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:84,testability,test,test,84,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:108,testability,test,tests,108,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:127,testability,test,tests,127,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:427,usability,indicat,indicated,427,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:470,usability,indicat,indicated,470,"Hi, @eroell, I understand what you're saying. You're referring to tests with signed test statistics, like t-tests and Wilcoxon tests. You mentioned that larger scores typically correspond to lower p-values, and conversely, scores further from 0 also tend to have lower p-values. However, I am currently confused about what the positive and negative values of these scores represent. Previously, I believed that positive scores indicated upregulation and negative scores indicated downregulation, but recent results have shown me that this viewpoint is incorrect. ![image](https://github.com/scverse/scanpy/assets/103617127/5f7d7b5b-939d-4ffe-8822-b68b59ac354f).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2587:295,deployability,observ,observation,295,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:382,deployability,observ,observation,382,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:663,deployability,observ,observation,663,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:911,modifiability,pac,packages,911,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:1552,reliability,doe,does,1552,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:871,safety,test,test,871,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:1416,safety,test,testing,1416,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:295,testability,observ,observation,295,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:382,testability,observ,observation,382,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:663,testability,observ,observation,663,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:871,testability,test,test,871,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:1416,testability,test,testing,1416,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:1490,usability,document,documentation,1490,"Hi Alma,. thanks for raising your thoughts here! I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:. ```py. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['distances']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1. np.testing.assert_equal(nn, k-1). ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point! That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:48,usability,document,documentation,48,At the moment I believe this might be more of a documentation issue rather than a bug so I changed the label - if you'd have a follow-up or related issue kindly let me know :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:8,usability,close,close,8,"We will close the issue for now, hopefully this has been addressed helpfully :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:67,usability,help,helpfully,67,"We will close the issue for now, hopefully this has been addressed helpfully :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:32,availability,slo,slow,32,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:258,modifiability,maintain,maintaining,258,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:275,modifiability,pac,package,275,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:32,reliability,slo,slow,32,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:258,safety,maintain,maintaining,258,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:221,security,team,team,221,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:161,usability,document,documentation,161,"Hej,. first of all, sorry for a slow reply. Thank you @eroell for the explanation, that fully makes sense - but as you're saying, perhaps a clarification in the documentation would make sense. I wasn't the only one in my team confused by this. Thank you for maintaining this package and all the great work you guys are doing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/pull/2589:21,deployability,releas,release,21,I added a test and a release node,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589
https://github.com/scverse/scanpy/pull/2589:10,safety,test,test,10,I added a test and a release node,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589
https://github.com/scverse/scanpy/pull/2589:10,testability,test,test,10,I added a test and a release node,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589
https://github.com/scverse/scanpy/pull/2589:18,deployability,releas,release,18,"@Intron7 FYI: the release notes were in the wrong file. this is in milestone 1.9.4, so they go in the 1.9.4 file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589
https://github.com/scverse/scanpy/pull/2590:60,usability,guidanc,guidance,60,Ok @ivirshup I think we're ready to go here. Thanks for the guidance!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:236,modifiability,responsibil,responsibility,236,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:212,reliability,doe,doesn,212,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:280,reliability,doe,does,280,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:29,safety,review,review,29,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:29,testability,review,review,29,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:60,usability,close,closer,60,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:129,usability,document,document,129,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:289,usability,document,document,289,"> It looks like I'll need to review the code I didn't touch closer from after porting the PR. . if you add something like `TODO: document (unchanged from original code)` to the relevant parts, that’s also OK. It doesn’t need to be your responsibility to figure out what that code does and document it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:206,safety,review,review,206,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:284,safety,test,tests,284,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:206,testability,review,review,206,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:284,testability,test,tests,284,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:84,usability,prefer,prefer,84,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:233,usability,person,personally,233,"@flying-sheep Do you want me to resolve your comments as I addressed them or do you prefer to do that yourself? I've seen it done both ways. Once I know, I'll either resolve or not and then re-request your review. The only thing I'm personally still curious about is if you think the tests are ""too"" duplicated still, but there might be other new things/poorly-fixed old things to look at.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:43,deployability,updat,updated,43,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:204,energy efficiency,current,currently,204,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:280,reliability,doe,doesn,280,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:43,safety,updat,updated,43,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:475,safety,compl,complicated,475,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:43,security,updat,updated,43,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:456,security,sign,significantly,456,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:475,security,compl,complicated,475,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:23,usability,close,close,23,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:231,usability,support,support,231,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:390,usability,support,support,390,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:497,interoperability,compatib,compatible,497,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:387,performance,time,time,387,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:656,performance,cach,cached,656,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:606,reliability,doe,doesn,606,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:237,safety,compl,complexity,237,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:237,security,compl,complexity,237,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:352,usability,user,user,352,"> also as I asked before: why go away from dataclasses? I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now. * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:19,reliability,doe,doesn,19,"`sparse_indicator` doesn’t have its `weights` branches hit at all, maybe we should remove that? Or will this be used at some point?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:203,modifiability,paramet,parameterizing,203,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point? I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:19,reliability,doe,doesn,19,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point? I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:284,reliability,doe,does,284,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point? I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:279,safety,test,test,279,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point? I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:279,testability,test,test,279,"> sparse_indicator doesn’t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point? I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:120,modifiability,concern,concerned,120,"> I think it will be used at some point, but also happy to remove. OK, good to know! Then this PR is fine as far as I’m concerned.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/pull/2590:120,testability,concern,concerned,120,"> I think it will be used at some point, but also happy to remove. OK, good to know! Then this PR is fine as far as I’m concerned.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590
https://github.com/scverse/scanpy/issues/2592:77,usability,minim,minimal,77,"Hi,. thanks for your interest and for the question here! Could you provide a minimal reproducible example of how a file in the manner of your `cis_scanpy.h5ad` is created, so I could check this out?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592
https://github.com/scverse/scanpy/issues/2592:16,usability,close,close,16,Sorry forgot to close this it fixed itself somehow!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592
https://github.com/scverse/scanpy/issues/2592:25,deployability,updat,update,25,"All good, thanks for the update! Glad to hear it works now as you'd expect it to :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592
https://github.com/scverse/scanpy/issues/2592:25,safety,updat,update,25,"All good, thanks for the update! Glad to hear it works now as you'd expect it to :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592
https://github.com/scverse/scanpy/issues/2592:25,security,updat,update,25,"All good, thanks for the update! Glad to hear it works now as you'd expect it to :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592
https://github.com/scverse/scanpy/issues/2598:80,deployability,releas,release,80,"fixed in #2424, reported many times. please use the search function. we’ll do a release soon!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2598
https://github.com/scverse/scanpy/issues/2598:30,performance,time,times,30,"fixed in #2424, reported many times. please use the search function. we’ll do a release soon!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2598
https://github.com/scverse/scanpy/pull/2605:105,availability,error,error,105,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:178,energy efficiency,CPU,CPU,178,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:46,performance,cach,cache,46,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:105,performance,error,error,105,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:178,performance,CPU,CPU,178,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:105,safety,error,error,105,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:36,usability,clear,clear,36,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:105,usability,error,error,105,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:23,usability,help,help,23,"For sure, but it won’t help with this issue, I had to merge https://github.com/scverse/scanpy/pull/2687 for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:9,deployability,depend,dependencies,9,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:9,integrability,depend,dependencies,9,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:9,modifiability,depend,dependencies,9,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:9,safety,depend,dependencies,9,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:111,safety,review,review,111,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:9,testability,depend,dependencies,9,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:111,testability,review,review,111,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:35,usability,support,support,35,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:10,availability,ping,ping,10,@ivirshup ping,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:36,usability,help,helps,36,"TODO: check out doctest-plus, if it helps enough, switch to it, then merge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/pull/2605:57,reliability,doe,doesn,57,"OK, doctest-plus can’t even handle relative imports, and doesn’t respect `--import-mode`. It needs quite some work before it works for us.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605
https://github.com/scverse/scanpy/issues/2608:23,availability,error,error,23,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:73,availability,error,error,73,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:41,deployability,continu,continuing,41,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:358,deployability,version,version,358,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:441,energy efficiency,core,core,441,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:358,integrability,version,version,358,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:358,modifiability,version,version,358,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:23,performance,error,error,23,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:73,performance,error,error,73,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:23,safety,error,error,23,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:73,safety,error,error,73,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:23,usability,error,error,23,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:73,usability,error,error,73,"After fixing the above error locally and continuing I ran into a similar error in the next step:. `sc.external.tl.palantir_results(...)` internally [calls](https://github.com/scverse/scanpy/blob/master/scanpy/external/tl/_palantir.py#L294) `run_palantir(ms_data=ms_data, ...)` with keyword `ms_data` which seems to have changed to just `data` in the [recent version of palantir](https://github.com/dpeerlab/Palantir/blob/master/src/palantir/core.py#L35).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2608:165,usability,statu,status,165,Palantir can now [take anndata directly](https://github.com/dpeerlab/Palantir#new-features) and the argument names have changed to a more general `data`. How is the status on this @soerenab? I would gladly provide a PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608
https://github.com/scverse/scanpy/issues/2609:140,deployability,version,version,140,"Hello, I'm a bit confused. Satija 2015 references Seurat v1, but I'm not sure the function was already implemented. When I ported the first version into `scanpy`, Seurat v2 was already out, the proper reference should then be [Butler 2018](https://www.nature.com/articles/nbt.4096)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2609
https://github.com/scverse/scanpy/issues/2609:140,integrability,version,version,140,"Hello, I'm a bit confused. Satija 2015 references Seurat v1, but I'm not sure the function was already implemented. When I ported the first version into `scanpy`, Seurat v2 was already out, the proper reference should then be [Butler 2018](https://www.nature.com/articles/nbt.4096)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2609
https://github.com/scverse/scanpy/issues/2609:140,modifiability,version,version,140,"Hello, I'm a bit confused. Satija 2015 references Seurat v1, but I'm not sure the function was already implemented. When I ported the first version into `scanpy`, Seurat v2 was already out, the proper reference should then be [Butler 2018](https://www.nature.com/articles/nbt.4096)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2609
https://github.com/scverse/scanpy/issues/2609:152,usability,document,documentation,152,"I believe that this scoring algorithm was implemented in Seurat, but it looks to be originally devised in Tirosh et al. (2016) cited above. Here is the documentation for the corresponding function in Seurat, which cites Tirosh et al: [https://satijalab.org/seurat/reference/addmodulescore](https://satijalab.org/seurat/reference/addmodulescore)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2609
https://github.com/scverse/scanpy/pull/2610:5,deployability,Releas,Release,5,> 1. Release note please. I'll do so. > 2. Why do we have `N_PCS` in the settings? Wasn't aware of that actually. I don't know I just came across this while working on RSC and thought the behavior was not what I expected. I hope that this is more in line with what the user intended.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2610
https://github.com/scverse/scanpy/pull/2610:188,usability,behavi,behavior,188,> 1. Release note please. I'll do so. > 2. Why do we have `N_PCS` in the settings? Wasn't aware of that actually. I don't know I just came across this while working on RSC and thought the behavior was not what I expected. I hope that this is more in line with what the user intended.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2610
https://github.com/scverse/scanpy/pull/2610:269,usability,user,user,269,> 1. Release note please. I'll do so. > 2. Why do we have `N_PCS` in the settings? Wasn't aware of that actually. I don't know I just came across this while working on RSC and thought the behavior was not what I expected. I hope that this is more in line with what the user intended.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2610
https://github.com/scverse/scanpy/issues/2611:353,testability,context,context,353,"> Thank you so much for the beautiful tool that is Scanpy. thank you! I’m happy to hear that! > I tried both options 'order' and 'categories_order'. Can you please give a fully reproducible example (i.e. where we just need to copy the code and run it in an empty directory and it shows the problem). I can’t reproduce this on scanpy master without more context, it seems to work fine:. ```py. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.stacked_violin(. adata,. adata.var_names,. groupby='cell_type',. use_raw=False,. title='cat order',. categories_order=['progenitor', 'Mk', 'Ery', 'Neu', 'Mo'],. ). ```. ![grafik](https://github.com/scverse/scanpy/assets/291575/4c321471-897e-4276-9b20-fb05f932b2be).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:38,usability,tool,tool,38,"> Thank you so much for the beautiful tool that is Scanpy. thank you! I’m happy to hear that! > I tried both options 'order' and 'categories_order'. Can you please give a fully reproducible example (i.e. where we just need to copy the code and run it in an empty directory and it shows the problem). I can’t reproduce this on scanpy master without more context, it seems to work fine:. ```py. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.stacked_violin(. adata,. adata.var_names,. groupby='cell_type',. use_raw=False,. title='cat order',. categories_order=['progenitor', 'Mk', 'Ery', 'Neu', 'Mo'],. ). ```. ![grafik](https://github.com/scverse/scanpy/assets/291575/4c321471-897e-4276-9b20-fb05f932b2be).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:221,energy efficiency,core,core,221,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:125,safety,input,input,125,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:399,testability,simpl,simple,399,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:27,usability,help,help,27,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:125,usability,input,input,125,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:399,usability,simpl,simple,399,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:498,usability,help,help,498,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:. correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:63,availability,consist,consistent,63,"Great it helped. And good to know! We’ll make things much more consistent in the future, a plotting overhaul is overdue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:9,usability,help,helped,9,"Great it helped. And good to know! We’ll make things much more consistent in the future, a plotting overhaul is overdue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/issues/2611:63,usability,consist,consistent,63,"Great it helped. And good to know! We’ll make things much more consistent in the future, a plotting overhaul is overdue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611
https://github.com/scverse/scanpy/pull/2612:157,availability,slo,slower,157,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:617,availability,fault,faults,617,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1654,availability,fault,faults,1654,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:5,energy efficiency,measur,measurements,5,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:483,energy efficiency,clock,clock,483,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:500,energy efficiency,CPU,CPUs,500,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:569,energy efficiency,cpu,cpu-migrations,569,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:617,energy efficiency,fault,faults,617,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:785,energy efficiency,idl,idle,785,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:879,energy efficiency,idl,idle,879,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1520,energy efficiency,clock,clock,1520,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1537,energy efficiency,CPU,CPUs,1537,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1606,energy efficiency,cpu,cpu-migrations,1606,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1654,energy efficiency,fault,faults,1654,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1821,energy efficiency,idl,idle,1821,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1915,energy efficiency,idl,idle,1915,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:785,interoperability,idl,idle,785,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:879,interoperability,idl,idle,879,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1821,interoperability,idl,idle,1821,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1915,interoperability,idl,idle,1915,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:339,performance,Perform,Performance,339,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:500,performance,CPU,CPUs,500,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:569,performance,cpu,cpu-migrations,569,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:617,performance,fault,faults,617,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1193,performance,time,time,1193,"py master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1375,performance,Perform,Performance,1375,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1537,performance,CPU,CPUs,1537,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1606,performance,cpu,cpu-migrations,1606,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1654,performance,fault,faults,1654,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:2229,performance,time,time,2229,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:157,reliability,slo,slower,157,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:617,reliability,fault,faults,617,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1654,reliability,fault,faults,1654,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:54,safety,test,tests,54,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:141,safety,Test,Tests,141,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:280,safety,test,test,280,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:300,safety,test,tests,300,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:389,safety,test,test,389,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:409,safety,test,tests,409,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:617,safety,fault,faults,617,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1316,safety,test,test,1316,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1336,safety,test,tests,1336,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1425,safety,test,test,1425,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1445,safety,test,tests,1445,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1654,safety,fault,faults,1654,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:54,testability,test,tests,54,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:141,testability,Test,Tests,141,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:280,testability,test,test,280,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:300,testability,test,tests,300,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:389,testability,test,test,389,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:409,testability,test,tests,409,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:532,testability,context,context-switches,532,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1316,testability,test,test,1316,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1336,testability,test,tests,1336,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1425,testability,test,test,1425,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1445,testability,test,tests,1445,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1569,testability,context,context-switches,1569,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:126,usability,user,users,126,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:339,usability,Perform,Performance,339,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console. $ git switch master. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2612:1375,usability,Perform,Performance,1375,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ). 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%). 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%). 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%). 	 257.750.810.841 instructions:u # 2,44 insn per cycle. 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%). 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%). 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%). 	. 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ). ```. - this PR:. ```console. $ git switch hvg_PR_numba. $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py. 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):. 	. 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ). 	 0 context-switches:u # 0,000 /sec. 	 0 cpu-migrations:u # 0,000 /sec. 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ). 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%). 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%). 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%). 	 373.047.679.552 instructions:u # 2,19 insn per cycle. 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%). 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%). 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%). 	. 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612
https://github.com/scverse/scanpy/pull/2621:260,performance,memor,memory,260,"I don’t think that’s possible without changing what kinds of checks we do. E.g. `check_nonnegative_integers` checks if every single value in the data is a whole number. But sure, it’s more useful to make sure no `n_obs × ?` array ever gets converted into a in-memory array if dask is used. The question is “make everything work with dask now, make sure everything is fast later” or “make a few functions work and go fast now, handle other functions later”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:260,usability,memor,memory,260,"I don’t think that’s possible without changing what kinds of checks we do. E.g. `check_nonnegative_integers` checks if every single value in the data is a whole number. But sure, it’s more useful to make sure no `n_obs × ?` array ever gets converted into a in-memory array if dask is used. The question is “make everything work with dask now, make sure everything is fast later” or “make a few functions work and go fast now, handle other functions later”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:255,usability,behavi,behavior,255,"> The question is “make everything work with dask now, make sure everything is fast later” or “make a few functions work and go fast now, handle other functions later”. My worry with this is that if we do ever make it so the computation is delayed it's a behavior change for dask arrays. E.g. the result changes from immediate to delayed and now functions may have to call `.compute` to get a result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:2207,availability,operat,operations,2207,". 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]. 814146044537405006 --> 1050532709569538834. ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid. flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]. ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>. <summary>mean_var graph</summary>. ```mermaid. flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]. step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]. step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:2362,availability,operat,operations,2362,". 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]. 814146044537405006 --> 1050532709569538834. ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid. flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]. ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>. <summary>mean_var graph</summary>. ```mermaid. flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]. step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]. step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:2458,energy efficiency,optim,optimization,2458,". 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]. 814146044537405006 --> 1050532709569538834. ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid. flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]. ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>. <summary>mean_var graph</summary>. ```mermaid. flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]. step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]. step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:2458,performance,optimiz,optimization,2458,". 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]. 814146044537405006 --> 1050532709569538834. ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid. flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]. ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>. <summary>mean_var graph</summary>. ```mermaid. flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]. step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]. step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:241,security,sign,signbit,241,"OK, so the graph in `test_check_nonnegative_integers`, generated via. ```py. import dask. dask.visualize(rv, engine=""cytoscape"", filename=request.node.name). ```. ```mermaid. flowchart LR. 4342680003182880388[""(0, 0)""]. 4363265869429493506((signbit)). 4342680003182880388 --> 4363265869429493506. 550394025789815978[""(0, 1)""]. 5167392774578066548((signbit)). 550394025789815978 --> 5167392774578066548. 3764517430824237394[""(1, 0)""]. 5336730508589856979((signbit)). 3764517430824237394 --> 5336730508589856979. 2743123325277761031[""(1, 1)""]. 2513425685193572888((signbit)). 2743123325277761031 --> 2513425685193572888. 284808496767994156[""(0, 0)""]. 4363265869429493506 --> 284808496767994156. 6263727941369393084((any)). 284808496767994156 --> 6263727941369393084. 6222832259334004269[""(0, 1)""]. 5167392774578066548 --> 6222832259334004269. 7256567839680908872((any)). 6222832259334004269 --> 7256567839680908872. 8881403918513157720[""(1, 0)""]. 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:348,security,sign,signbit,348,"OK, so the graph in `test_check_nonnegative_integers`, generated via. ```py. import dask. dask.visualize(rv, engine=""cytoscape"", filename=request.node.name). ```. ```mermaid. flowchart LR. 4342680003182880388[""(0, 0)""]. 4363265869429493506((signbit)). 4342680003182880388 --> 4363265869429493506. 550394025789815978[""(0, 1)""]. 5167392774578066548((signbit)). 550394025789815978 --> 5167392774578066548. 3764517430824237394[""(1, 0)""]. 5336730508589856979((signbit)). 3764517430824237394 --> 5336730508589856979. 2743123325277761031[""(1, 1)""]. 2513425685193572888((signbit)). 2743123325277761031 --> 2513425685193572888. 284808496767994156[""(0, 0)""]. 4363265869429493506 --> 284808496767994156. 6263727941369393084((any)). 284808496767994156 --> 6263727941369393084. 6222832259334004269[""(0, 1)""]. 5167392774578066548 --> 6222832259334004269. 7256567839680908872((any)). 6222832259334004269 --> 7256567839680908872. 8881403918513157720[""(1, 0)""]. 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:455,security,sign,signbit,455,"OK, so the graph in `test_check_nonnegative_integers`, generated via. ```py. import dask. dask.visualize(rv, engine=""cytoscape"", filename=request.node.name). ```. ```mermaid. flowchart LR. 4342680003182880388[""(0, 0)""]. 4363265869429493506((signbit)). 4342680003182880388 --> 4363265869429493506. 550394025789815978[""(0, 1)""]. 5167392774578066548((signbit)). 550394025789815978 --> 5167392774578066548. 3764517430824237394[""(1, 0)""]. 5336730508589856979((signbit)). 3764517430824237394 --> 5336730508589856979. 2743123325277761031[""(1, 1)""]. 2513425685193572888((signbit)). 2743123325277761031 --> 2513425685193572888. 284808496767994156[""(0, 0)""]. 4363265869429493506 --> 284808496767994156. 6263727941369393084((any)). 284808496767994156 --> 6263727941369393084. 6222832259334004269[""(0, 1)""]. 5167392774578066548 --> 6222832259334004269. 7256567839680908872((any)). 6222832259334004269 --> 7256567839680908872. 8881403918513157720[""(1, 0)""]. 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:563,security,sign,signbit,563,"OK, so the graph in `test_check_nonnegative_integers`, generated via. ```py. import dask. dask.visualize(rv, engine=""cytoscape"", filename=request.node.name). ```. ```mermaid. flowchart LR. 4342680003182880388[""(0, 0)""]. 4363265869429493506((signbit)). 4342680003182880388 --> 4363265869429493506. 550394025789815978[""(0, 1)""]. 5167392774578066548((signbit)). 550394025789815978 --> 5167392774578066548. 3764517430824237394[""(1, 0)""]. 5336730508589856979((signbit)). 3764517430824237394 --> 5336730508589856979. 2743123325277761031[""(1, 1)""]. 2513425685193572888((signbit)). 2743123325277761031 --> 2513425685193572888. 284808496767994156[""(0, 0)""]. 4363265869429493506 --> 284808496767994156. 6263727941369393084((any)). 284808496767994156 --> 6263727941369393084. 6222832259334004269[""(0, 1)""]. 5167392774578066548 --> 6222832259334004269. 7256567839680908872((any)). 6222832259334004269 --> 7256567839680908872. 8881403918513157720[""(1, 0)""]. 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:2275,security,sign,signbit,2275,". 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]. 814146044537405006 --> 1050532709569538834. ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid. flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]. ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>. <summary>mean_var graph</summary>. ```mermaid. flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]. step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]. step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2621:95,usability,visual,visualize,95,"OK, so the graph in `test_check_nonnegative_integers`, generated via. ```py. import dask. dask.visualize(rv, engine=""cytoscape"", filename=request.node.name). ```. ```mermaid. flowchart LR. 4342680003182880388[""(0, 0)""]. 4363265869429493506((signbit)). 4342680003182880388 --> 4363265869429493506. 550394025789815978[""(0, 1)""]. 5167392774578066548((signbit)). 550394025789815978 --> 5167392774578066548. 3764517430824237394[""(1, 0)""]. 5336730508589856979((signbit)). 3764517430824237394 --> 5336730508589856979. 2743123325277761031[""(1, 1)""]. 2513425685193572888((signbit)). 2743123325277761031 --> 2513425685193572888. 284808496767994156[""(0, 0)""]. 4363265869429493506 --> 284808496767994156. 6263727941369393084((any)). 284808496767994156 --> 6263727941369393084. 6222832259334004269[""(0, 1)""]. 5167392774578066548 --> 6222832259334004269. 7256567839680908872((any)). 6222832259334004269 --> 7256567839680908872. 8881403918513157720[""(1, 0)""]. 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)). 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]. 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)). 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]. 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)). 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]. 7256567839680908872 --> 687812693798660380. 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]. 5898621639535744825 --> 3901936098833081796. 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]. 1659302467096852217 --> 8795010127805778162. 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]. 7976077601232067203 --> 1203378416021505679. 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500. 5169565091578776769[""()""]. 9179805111332178500 --> 5169565091578776769. 814146044537405006((and)). 5169565091578776769 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621
https://github.com/scverse/scanpy/pull/2624:244,energy efficiency,load,load,244,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:. `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:244,performance,load,load,244,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:. `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:265,performance,memor,memory,265,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:. `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:265,usability,memor,memory,265,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:. `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:35,availability,error,error,35,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:41,integrability,messag,message,41,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:105,integrability,sub,subsampling,105,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:41,interoperability,messag,message,41,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:35,performance,error,error,35,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:35,safety,error,error,35,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/pull/2624:35,usability,error,error,35,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624
https://github.com/scverse/scanpy/issues/2626:44,usability,user,user,44,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:201,usability,user,user,201,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:234,usability,tool,tools,234,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:339,usability,user,user,339,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:413,usability,user,users,413,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:544,usability,user,user,544,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:625,usability,guidanc,guidance,625,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:648,usability,help,helpful,648,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2626:656,usability,support,support,656,"Hi, thanks for your interest in scanpy! For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub! Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626
https://github.com/scverse/scanpy/issues/2628:312,deployability,api,api,312,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:321,deployability,modul,module-scanpy,321,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:741,deployability,updat,updates,741,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:312,integrability,api,api,312,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:730,integrability,sub,subject,730,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:312,interoperability,api,api,312,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:321,modifiability,modul,module-scanpy,321,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:321,safety,modul,module-scanpy,321,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:741,safety,updat,updates,741,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:582,security,access,accessing,582,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:741,security,updat,updates,741,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:717,usability,behavi,behaviour,717,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2628:842,usability,help,helps,842,"Hi, . Thank you for your interest in scanpy and for raising your question here! It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`. For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""). ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes. This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628
https://github.com/scverse/scanpy/issues/2629:1291,availability,operat,operator,1291,"np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescale",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:68,deployability,observ,observations,68,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:246,deployability,scale,scale,246,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1510,deployability,scale,scale,1510,"[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1820,deployability,fail,fails,1820,"tself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1864,deployability,scale,scaled,1864,"this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2101,deployability,scale,scale,2101,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2217,deployability,scale,scale,2217,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2308,deployability,scale,scale,2308,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2532,deployability,scale,scale,2532,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2578,deployability,scale,scale,2578,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:161,energy efficiency,Load,Loading,161,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:246,energy efficiency,scale,scale,246,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:457,energy efficiency,estimat,estimator,457,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1510,energy efficiency,scale,scale,1510,"[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1864,energy efficiency,scale,scaled,1864,"this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2101,energy efficiency,scale,scale,2101,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2217,energy efficiency,scale,scale,2217,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2308,energy efficiency,scale,scale,2308,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2532,energy efficiency,scale,scale,2532,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2578,energy efficiency,scale,scale,2578,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:246,modifiability,scal,scale,246,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1510,modifiability,scal,scale,1510,"[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1864,modifiability,scal,scaled,1864,"this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2101,modifiability,scal,scale,2101,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2217,modifiability,scal,scale,2217,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2308,modifiability,scal,scale,2308,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2532,modifiability,scal,scale,2532,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2578,modifiability,scal,scale,2578,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:161,performance,Load,Loading,161,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:246,performance,scale,scale,246,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1510,performance,scale,scale,1510,"[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1864,performance,scale,scaled,1864,"this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2101,performance,scale,scale,2101,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2217,performance,scale,scale,2217,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2308,performance,scale,scale,2308,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2532,performance,scale,scale,2532,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2578,performance,scale,scale,2578,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1820,reliability,fail,fails,1820,"tself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:954,safety,test,test,954,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1806,safety,test,test,1806,"n object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:613,security,modif,modifies,613,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2162,security,control,control,2162,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:68,testability,observ,observations,68,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:836,testability,simpl,simply,836,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:954,testability,test,test,954,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1806,testability,test,test,1806,"n object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2162,testability,control,control,2162,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:836,usability,simpl,simply,836,"Hi, thanks for your interest in scanpy! I’ll try to comment on your observations here with your code example:. ```. import scanpy as sc. import numpy as np. ### Loading and preprocessing data. adata = sc.datasets.pbmc3k_processed(). ### Defining scale function. def mean_var(X, axis=0):. mean = np.mean(X, axis=axis, dtype=np.float64). mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64). var = mean_sq - mean**2. # enforce R convention (unbiased estimator) for variance. var *= X.shape[axis] / (X.shape[axis] - 1). return mean, var. ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1652,usability,close,closeness,1652,"anpy object - which is used again later in the snippet. → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of flo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:1750,usability,close,closeness,1750," code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2363,usability,close,closeness,2363,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2464,usability,close,closeness,2464,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2641,usability,close,closeness,2641,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2753,usability,close,closeness,2753,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2798,usability,behavi,behaviour,2798,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2629:2903,usability,help,helps,2903,"ate as `True`, but is not what you intend to test). ```. def my_scale_function(X, clip=False):. # need to make a copy of X. Y = X.copy(). mean, var = mean_var(Y, axis=0). Y -= mean. std = np.sqrt(var). #std[std == 0] = 1. Y /= std. if clip:. Y = np.clip(X, -10, 10). return np.matrix(Y). ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```. ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""). mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled)). ```. ```. Do a numpy check for closeness of floats:. False. ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that. ```. adata.X.var(0). ```. ```. array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,. 0.9996219 ], dtype=float32). ```. This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```. mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(adata.X, mtx_rescaled_sc)). ```. ```. Do a numpy check for closeness of floats:. False. ```. But not anymore if we call `sc.pp.scale` again. ```. mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""). print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)). ```. ```. Do a numpy check for closeness of floats:. True. ```. This is the behaviour which we would expect: I also think that the UMAPs generated should be reproducible. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629
https://github.com/scverse/scanpy/issues/2630:54,energy efficiency,core,cores,54,Think that for `n_job` it's generally `-1` to use all cores and concerning the `max_memory` I vaguely remember that the setting isn't really used so you can ignore it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2630
https://github.com/scverse/scanpy/issues/2630:64,modifiability,concern,concerning,64,Think that for `n_job` it's generally `-1` to use all cores and concerning the `max_memory` I vaguely remember that the setting isn't really used so you can ignore it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2630
https://github.com/scverse/scanpy/issues/2630:102,safety,reme,remember,102,Think that for `n_job` it's generally `-1` to use all cores and concerning the `max_memory` I vaguely remember that the setting isn't really used so you can ignore it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2630
https://github.com/scverse/scanpy/issues/2630:64,testability,concern,concerning,64,Think that for `n_job` it's generally `-1` to use all cores and concerning the `max_memory` I vaguely remember that the setting isn't really used so you can ignore it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2630
https://github.com/scverse/scanpy/issues/2630:17,usability,document,document,17,We might want to document that a bit better. I fatfingered.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2630
https://github.com/scverse/scanpy/issues/2632:14,usability,tool,tooling,14,"Some official tooling in scanpy/anndata for this would be nice, but for now you can do. ```python. column = ""<your anndata.obs column>"". colors = {""category1"": ""#123456""} # your color dict. adata.uns[f""{column}_colors""] = [. colors[cat] for cat in adata.obs[column].cat.categories. ]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2632
https://github.com/scverse/scanpy/issues/2634:59,security,auth,author,59,Here is a Twitter thread with a further explanation by the author: https://twitter.com/lpachter/status/1694387749967847874,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2634
https://github.com/scverse/scanpy/issues/2634:96,usability,statu,status,96,Here is a Twitter thread with a further explanation by the author: https://twitter.com/lpachter/status/1694387749967847874,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2634
https://github.com/scverse/scanpy/issues/2636:206,deployability,updat,update,206,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:6,interoperability,coordinat,coordinate,6,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:206,safety,updat,update,206,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:206,security,updat,update,206,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:323,security,cookie,cookiecutter-scverse,323,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:138,usability,close,closed,138,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:363,usability,close,close,363,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with. - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:44,integrability,sub,submodule,44,"Since we already made the decision to use a submodule in scanpy (why it was that way) I don't think we need to discuss more before going back to it. Still happy to have discussions about better ways to do things, but we've already established a setup and workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:255,usability,workflow,workflow,255,"Since we already made the decision to use a submodule in scanpy (why it was that way) I don't think we need to discuss more before going back to it. Still happy to have discussions about better ways to do things, but we've already established a setup and workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:601,deployability,version,version,601,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:187,energy efficiency,current,current,187,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:79,integrability,sub,submodules,79,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:601,integrability,version,version,601,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:601,modifiability,version,version,601,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:256,safety,valid,valid,256,"Sure, I just suggested that since I don‘t feel like it’s worth the effort:. 1. submodules are a temporary solution (to be replaced by the one we’ll settle on in the linked issue). 2. the current solution works well enough (intersphinx makes sure links are valid and have a nice default text). If you feel it’s important enough to switch to it for the next few weeks, it would be nice to settle on a canonical URL for each of the tutorials, i.e. that the tutorials on https://scanpy-tutorials.readthedocs.io redirect to the ones on https://scanpy.readthedocs.io. Otherwise some people will link to one version, others to the other.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:204,deployability,releas,release,204,"After @ilan-gold mentioned that scanpy’s tutorials are actually not reproducible, I made an issue for that: https://github.com/scverse/scanpy-tutorials/issues/79. Maybe we need to address that before the release, that’ll also get rid of the warnings. If you need to suppress them, I think this extension could be an acceptable solution: https://github.com/picnixz/sphinx-zeta-suppress",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/issues/2636:294,modifiability,extens,extension,294,"After @ilan-gold mentioned that scanpy’s tutorials are actually not reproducible, I made an issue for that: https://github.com/scverse/scanpy-tutorials/issues/79. Maybe we need to address that before the release, that’ll also get rid of the warnings. If you need to suppress them, I think this extension could be an acceptable solution: https://github.com/picnixz/sphinx-zeta-suppress",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636
https://github.com/scverse/scanpy/pull/2637:38,safety,review,review,38,"Oh sorry, forgot about that. Happy to review a followup :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2637
https://github.com/scverse/scanpy/pull/2637:38,testability,review,review,38,"Oh sorry, forgot about that. Happy to review a followup :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2637
https://github.com/scverse/scanpy/issues/2645:34,deployability,upgrad,upgraded,34,I noticed I'm on scanpy 1.9.3 and upgraded to scanpy 1.9.4 as well and tried again and have the same issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/issues/2645:34,modifiability,upgrad,upgraded,34,I noticed I'm on scanpy 1.9.3 and upgraded to scanpy 1.9.4 as well and tried again and have the same issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/issues/2645:115,deployability,version,version,115,Good to hear! I know the feeling of figuring out things myself right after filing an issue report. It’s the public version of https://rubberduckdebugging.com/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/issues/2645:108,integrability,pub,public,108,Good to hear! I know the feeling of figuring out things myself right after filing an issue report. It’s the public version of https://rubberduckdebugging.com/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/issues/2645:115,integrability,version,version,115,Good to hear! I know the feeling of figuring out things myself right after filing an issue report. It’s the public version of https://rubberduckdebugging.com/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/issues/2645:115,modifiability,version,version,115,Good to hear! I know the feeling of figuring out things myself right after filing an issue report. It’s the public version of https://rubberduckdebugging.com/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645
https://github.com/scverse/scanpy/pull/2646:42,safety,test,tests,42,thanks! let’s see if re-running the flaky tests makes CI pass,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2646
https://github.com/scverse/scanpy/pull/2646:42,testability,test,tests,42,thanks! let’s see if re-running the flaky tests makes CI pass,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2646
https://github.com/scverse/scanpy/pull/2647:12,safety,test,tests,12,"thanks! the tests here are a bit flaky it seems, and have nothing to do with your change",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2647
https://github.com/scverse/scanpy/pull/2647:12,testability,test,tests,12,"thanks! the tests here are a bit flaky it seems, and have nothing to do with your change",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2647
https://github.com/scverse/scanpy/pull/2648:54,deployability,fail,failing,54,thanks! seems like I need to fix something about that failing pbmc3k notebook first though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2648
https://github.com/scverse/scanpy/pull/2648:54,reliability,fail,failing,54,thanks! seems like I need to fix something about that failing pbmc3k notebook first though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2648
https://github.com/scverse/scanpy/issues/2653:130,modifiability,paramet,parameter,130,"Check out the docs: https://scanpy.readthedocs.io/en/latest/external/generated/scanpy.external.tl.phenograph.html. Seems like the parameter is called `clustering_algo`, not `method`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653
https://github.com/scverse/scanpy/issues/2653:49,reliability,doe,doesn,49,Phenograph accepts all additional `**kwargs` and doesn’t validate them. We can’t do it for them.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653
https://github.com/scverse/scanpy/issues/2653:57,safety,valid,validate,57,Phenograph accepts all additional `**kwargs` and doesn’t validate them. We can’t do it for them.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653
https://github.com/scverse/scanpy/issues/2653:57,security,validat,validate,57,Phenograph accepts all additional `**kwargs` and doesn’t validate them. We can’t do it for them.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653
https://github.com/scverse/scanpy/pull/2655:167,safety,test,testing,167,@WeipengMO if you calculate it like this you are right. However when we move from 64Bit to 32Bit for neighbors the results are reproducible at least to the best of my testing. I would still be open to round the results. @flying-sheep what do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:167,testability,test,testing,167,@WeipengMO if you calculate it like this you are right. However when we move from 64Bit to 32Bit for neighbors the results are reproducible at least to the best of my testing. I would still be open to round the results. @flying-sheep what do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:319,availability,error,error,319,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:319,performance,error,error,319,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:42,safety,test,testing,42,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:77,safety,test,testing,77,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:94,safety,test,testing,94,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:186,safety,test,testing,186,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:319,safety,error,error,319,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:42,testability,test,testing,42,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:77,testability,test,testing,77,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:94,testability,test,testing,94,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:186,testability,test,testing,186,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:319,usability,error,error,319,"Can you give me the full code you ran for testing and the results from numpy testing for. `np.testing.assert_array_equal(adata.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""])`. `np.testing.assert_array_equal(adata.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data)`. The first one should give you an error. The second one shouldn't. How big is your dataset? Please note that if you use scanpy 1.9.6 that changes of this PR won't have taken effect yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1651,availability,Error,Error,1651,"data_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2477,availability,Error,Error,2477,"). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:409,deployability,instal,install,409,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:437,deployability,instal,install,437,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1791,deployability,resourc,resource,1791,"00, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2617,deployability,resourc,resource,2617,"armony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3000,deployability,log,loguru,3000,"45477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3131,deployability,modul,modules,3131,"64897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3151,deployability,depend,dependencies,3151,"0.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:4728,deployability,updat,updated,4728,"). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_test.ipynb.zip](https://github.com/scverse/scanpy/files/13441924/harmony_test.ipynb.zip).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1791,energy efficiency,resourc,resource,1791,"00, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2617,energy efficiency,resourc,resource,2617,"armony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:810,integrability,batch,batch,810,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1456,integrability,batch,batch,1456," = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1514,integrability,batch,batch,1514,"monypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3151,integrability,depend,dependencies,3151,"0.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1704,interoperability,Mismatch,Mismatched,1704,"layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2530,interoperability,Mismatch,Mismatched,2530,"30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3893,interoperability,platform,platformdirs,3893,"oat32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_test.ipynb.zip](https://github.com/scverse/scanpy/files/13441924/harmony_test.ipynb.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:709,modifiability,layer,layers,709,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:965,modifiability,layer,layers,965,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3131,modifiability,modul,modules,3131,"64897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3151,modifiability,depend,dependencies,3151,"0.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3356,modifiability,deco,decorator,3356,"ting.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3812,modifiability,pac,packaging,3812,". y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:810,performance,batch,batch,810,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1456,performance,batch,batch,1456," = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1514,performance,batch,batch,1514,"monypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1651,performance,Error,Error,1651,"data_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1791,performance,resourc,resource,1791,"00, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2477,performance,Error,Error,2477,"). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2617,performance,resourc,resource,2617,"armony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:360,safety,except,except,360,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```. import scanpy as sc. import pandas as pd. import numpy as np. from anndata import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1187,safety,test,test,1187," import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.0873",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1544,safety,test,testing,1544,", key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched element",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1651,safety,Error,Error,1651,"data_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1791,safety,resourc,resource,1791,"00, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2223,safety,test,test,2223,"s of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkwar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2358,safety,test,testing,2358,"b204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decora",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2477,safety,Error,Error,2477,"). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2617,safety,resourc,resource,2617,"armony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3000,safety,log,loguru,3000,"45477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3131,safety,modul,modules,3131,"64897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3151,safety,depend,dependencies,3151,"0.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3401,safety,except,exceptiongroup,3401,"ties""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:4728,safety,updat,updated,4728,"). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_test.ipynb.zip](https://github.com/scverse/scanpy/files/13441924/harmony_test.ipynb.zip).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1758,security,ssh,ssh-,1758,"e_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2584,security,ssh,ssh-,2584,"pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2957,security,session,session,2957,"., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3000,security,log,loguru,3000,"45477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:4708,security,Session,Session,4708,"). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_test.ipynb.zip](https://github.com/scverse/scanpy/files/13441924/harmony_test.ipynb.zip).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:4728,security,updat,updated,4728,"). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. pynndescent 0.5.5. pyparsing 3.0.9. pytz 2023.3.post1. rich NA. scipy 1.10.1. setuptools 68.0.0. six 1.16.0. sklearn 1.3.2. sparse 0.14.0. stack_data 0.6.2. statsmodels 0.13.1. sympy 1.11.1. texttable 1.6.7. threadpoolctl 3.2.0. torch 2.0.1. tornado 6.3.3. tqdm 4.66.1. traitlets 5.10.0. typing_extensions NA. umap 0.5.3. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.1. -----. IPython 8.15.0. jupyter_client 8.3.1. jupyter_core 5.3.1. -----. Python 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]. Linux-6.2.0-36-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-11-23 00:08. ```. I uploaded the ipynb file as attachment. 👇. [harmony_test.ipynb.zip](https://github.com/scverse/scanpy/files/13441924/harmony_test.ipynb.zip).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1187,testability,test,test,1187," import AnnData. def harmony_integrate(. adata: AnnData,. key: str,. basis: str = ""X_pca"",. adjusted_basis: str = ""X_pca_harmony"",. **kwargs,. ):. try:. import harmonypy. except ImportError:. raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.0873",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1544,testability,test,testing,1544,", key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched element",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1664,testability,Assert,AssertionError,1664,"a.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1791,testability,resourc,resource,1791,"00, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2223,testability,test,test,2223,"s of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkwar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2358,testability,test,testing,2358,"b204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decora",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2490,testability,Assert,AssertionError,2490,"ate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2617,testability,resourc,resource,2617,"armony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3000,testability,log,loguru,3000,"45477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:3151,testability,depend,dependencies,3151,"0.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.9.0. harmonypy NA. igraph 0.10.8. importlib_metadata NA. importlib_resources NA. ipykernel 6.25.2. ipywidgets 8.1.1. jax 0.4.20. jaxlib 0.4.20. jedi 0.19.0. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.10.1. llvmlite 0.41.1. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. numba 0.58.1. nvfuser NA. opt_einsum v3.0.0. packaging 23.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.10.0. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 13.0.0. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.16.1. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:1651,usability,Error,Error,1651,"data_merge = adata.copy(). adata_merge.X = adata_merge.layers['counts']. sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(). sc.experimental.pp.normalize_pearson_residuals(adata_merge). adata_merge.layers['apr'] = adata_merge.X.copy(). sc.tl.pca(adata_merge, svd_solver=""arpack""). adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(). adata2 = adata_merge.copy(). ```. The frist test:. ```. # scanpy 1.9.6 that changes of this PR won't have taken effect yet. # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py. harmony_integrate(adata1, key='batch', basis='X_pca_30'). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/pull/2655:2477,usability,Error,Error,2477,"). harmony_integrate(adata2, key='batch', basis='X_pca_30'). np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%). Max absolute difference: 1.20792265e-12. Max relative difference: 4.37537551e-09. x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... y: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,. 0.564897],. [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,... ```. The second test:. ```. sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'). sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'). np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data). ```. It raised the Error:. ```. AssertionError: . Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%). Max absolute difference: 0.99820393. Max relative difference: 810.4644. x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],. dtype=float32). y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],. dtype=float32). ```. This is my session_info:. ```. Click to view session information. -----. anndata 0.9.2. loguru 0.7.2. matplotlib 3.8.0. numpy 1.26.0. pandas 1.4.3. scanpy 1.9.6. seaborn 0.12.2. session_info 1.0.0. -----. Click to view modules imported as dependencies. PIL 9.4.0. argcomplete NA. asttokens NA. attr 23.1.0. awkward 2.4.2. awkward_cpp NA. backcall 0.2.0. cffi 1.15.1. comm 0.1.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.8.0. decorator 5.1.1. dot_parser NA. etils 1.4.1. exceptiongroup 1.1.3. executing 1.2.0. get_annotations NA. gmpy2 2.1.2. h5py 3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655
https://github.com/scverse/scanpy/issues/2656:32,deployability,API,API,32,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:90,deployability,depend,dependencies,90,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:226,deployability,depend,dependency,226,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:32,integrability,API,API,32,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:90,integrability,depend,dependencies,90,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:226,integrability,depend,dependency,226,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:32,interoperability,API,API,32,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:90,modifiability,depend,dependencies,90,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:226,modifiability,depend,dependency,226,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:90,safety,depend,dependencies,90,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:226,safety,depend,dependency,226,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:90,testability,depend,dependencies,90,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:226,testability,depend,dependency,226,"How would you suggest doing the API for this? Another `kwarg` for backend? The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
https://github.com/scverse/scanpy/issues/2656:280,modifiability,paramet,parameters,280,I have just been exploring the holoviz ecosystem a bit and wasn't aware how nice this is! Ideally we could use something like [hvPlot](https://hvplot.holoviz.org/) and leave it to the user to select a backend. . The problem is that the scanpy plotting functions have way too many parameters. Supporting all of them in different backends sounds daunting if not impossible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656
