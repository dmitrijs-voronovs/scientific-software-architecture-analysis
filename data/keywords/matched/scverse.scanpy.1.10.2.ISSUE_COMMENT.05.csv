id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/373:584,interoperability,Distribut,Distributions,584,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:664,interoperability,distribut,distributions,664,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:680,interoperability,Distribut,Distributions,680,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:725,interoperability,distribut,distributions,725,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:897,interoperability,Distribut,Distributions,897,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1005,interoperability,Distribut,Distributions,1005,"g up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1075,interoperability,Distribut,Distribution,1075,"t! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1196,interoperability,Distribut,Distribution,1196,"ations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. Discr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1319,interoperability,Distribut,Distribution,1319," you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1450,interoperability,Distribut,Distribution,1450,"ich type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1605,interoperability,Distribut,Distribution,1605,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1812,interoperability,Distribut,Distribution,1812,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2127,interoperability,Distribut,Distribution,2127,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:427,modifiability,abstract,abstract,427,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:603,modifiability,pac,package,603,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1023,modifiability,paramet,parametric,1023,"versation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1138,modifiability,abstract,abstract,1138,"pe theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. /",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:643,security,model,model,643,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1665,security,ident,identity,1665,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:119,testability,context,context,119,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1066,usability,support,support,1066,"the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subty",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:64,availability,operat,operations,64,"Thank you for the additional explanations, @ivirshup! > ... set operations on a type lattice. OK, I can imagine that; one just needs to define what a type lattice is exactly. But let's not get into that, I roughly picture what is... > `DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}`. The concept of an intersection type as such makes a lot of sense to me, but as mentioned above, I'd see this as a special form of subclassing using the intersection of properties instead of the union. And yes, your example is nice. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. I get that. I can easily imagine more examples. But I cannot imagine why you would ever need to explicitly express that in the Scanpy docs: listing via `OneOf` will always do the job for us. Even in this very generic case that Phil mentioned (via `hasattr('foo')`), which we have several times, we just use the `OneOf` way in the docs and I cannot think of any problems that this might generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:436,integrability,sub,subclassing,436,"Thank you for the additional explanations, @ivirshup! > ... set operations on a type lattice. OK, I can imagine that; one just needs to define what a type lattice is exactly. But let's not get into that, I roughly picture what is... > `DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}`. The concept of an intersection type as such makes a lot of sense to me, but as mentioned above, I'd see this as a special form of subclassing using the intersection of properties instead of the union. And yes, your example is nice. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. I get that. I can easily imagine more examples. But I cannot imagine why you would ever need to explicitly express that in the Scanpy docs: listing via `OneOf` will always do the job for us. Even in this very generic case that Phil mentioned (via `hasattr('foo')`), which we have several times, we just use the `OneOf` way in the docs and I cannot think of any problems that this might generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:269,interoperability,Distribut,Distribution,269,"Thank you for the additional explanations, @ivirshup! > ... set operations on a type lattice. OK, I can imagine that; one just needs to define what a type lattice is exactly. But let's not get into that, I roughly picture what is... > `DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}`. The concept of an intersection type as such makes a lot of sense to me, but as mentioned above, I'd see this as a special form of subclassing using the intersection of properties instead of the union. And yes, your example is nice. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. I get that. I can easily imagine more examples. But I cannot imagine why you would ever need to explicitly express that in the Scanpy docs: listing via `OneOf` will always do the job for us. Even in this very generic case that Phil mentioned (via `hasattr('foo')`), which we have several times, we just use the `OneOf` way in the docs and I cannot think of any problems that this might generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:977,performance,time,times,977,"Thank you for the additional explanations, @ivirshup! > ... set operations on a type lattice. OK, I can imagine that; one just needs to define what a type lattice is exactly. But let's not get into that, I roughly picture what is... > `DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}`. The concept of an intersection type as such makes a lot of sense to me, but as mentioned above, I'd see this as a special form of subclassing using the intersection of properties instead of the union. And yes, your example is nice. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. I get that. I can easily imagine more examples. But I cannot imagine why you would ever need to explicitly express that in the Scanpy docs: listing via `OneOf` will always do the job for us. Even in this very generic case that Phil mentioned (via `hasattr('foo')`), which we have several times, we just use the `OneOf` way in the docs and I cannot think of any problems that this might generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:238,integrability,sub,substitute,238,"I agree that there's no need for it, I just like this stuff :). Just to be sure I get what you're saying:. For the ordered map example, is your position that since we can say something like `Union[OrderedDict, pd.Series]`, this is a fine substitute for `Intersect[Mapping, Sequence]`? If not, what should the type description be? To me, `Union[Mapping, Sequence]` would imply either a `list` or `dict` could be used, when neither fits the bill (ignoring 3.7s dict ordering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:710,integrability,wrap,wrap,710,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thats a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnt needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnt been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalas unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:617,modifiability,scal,scala,617,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thats a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnt needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnt been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalas unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:646,modifiability,scal,scala-lang,646,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thats a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnt needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnt been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalas unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:256,usability,behavi,behavior,256,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thats a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnt needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnt been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalas unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:427,usability,behavi,behavior,427,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thats a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnt needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnt been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalas unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:973,availability,error,error,973,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1376,availability,operat,operations,1376,", which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more bea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:66,integrability,sub,substitute,66,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:423,integrability,sub,subclassing,423,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:973,performance,error,error,973,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2396,performance,content,content,2396,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:540,reliability,pra,practice,540,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1537,reliability,doe,doesn,1537,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:973,safety,error,error,973,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2357,security,sign,signature,2357,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:118,testability,context,context,118,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2093,testability,simpl,simple,2093,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:973,usability,error,error,973,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1279,usability,learn,learned,1279,"en `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2050,usability,user,user,2050,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2093,usability,simpl,simple,2093,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:35,deployability,log,logical,35,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1634,deployability,contain,contains,1634,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1693,deployability,contain,contains,1693,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1734,deployability,contain,contains,1734,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1882,energy efficiency,model,model,1882,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:466,integrability,interfac,interfaces,466,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:755,integrability,encapsulat,encapsulated,755,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:810,integrability,abstract,abstract,810,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:839,integrability,interfac,interfaces,839,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1173,integrability,sub,subset,1173,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1944,integrability,sub,subtype,1944,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:466,interoperability,interfac,interfaces,466,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:839,interoperability,interfac,interfaces,839,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:204,modifiability,polymorph,polymorphism,204,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:466,modifiability,interfac,interfaces,466,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:755,modifiability,encapsul,encapsulated,755,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:810,modifiability,abstract,abstract,810,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:839,modifiability,interfac,interfaces,839,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1968,modifiability,inherit,inherits,1968,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:350,performance,overhead,overhead,350,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:378,reliability,doe,doesn,378,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:35,safety,log,logical,35,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:276,safety,compl,completely,276,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1813,safety,except,except,1813,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:35,security,log,logical,35,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:276,security,compl,completely,276,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1882,security,model,model,1882,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:35,testability,log,logical,35,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:481,testability,simpl,simple,481,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:574,testability,understand,understand,574,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:481,usability,simpl,simple,481,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:496,usability,document,documentation,496,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:529,usability,behavi,behavior,529,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:538,usability,clear,clear,538,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:736,usability,behavi,behaviors,736,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:787,usability,intuit,intuitively,787,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes propert",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1806,usability,close,close,1806,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python. from typing import Any, Union. class A():. pass. class B(A):. pass. class C(A):. pass. class D():. pass. class E(D):. pass. ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```. Any. / \. A D. / \ |. B C E. \ | /. Union[]. ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:240,integrability,protocol,protocol,240,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. Thats what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if its a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:240,interoperability,protocol,protocol,240,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. Thats what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if its a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:71,modifiability,polymorph,polymorphism,71,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. Thats what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if its a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:29,deployability,contain,contains,29,"Btw: IPython 7.2 is live and contains my signature rendering prettificattion except that I made a mistake and it renders wrong. Anyway, I fix that in ipython/ipython#11525 and things should be pretty in IPython 7.2.1:. ![grafik](https://user-images.githubusercontent.com/291575/49578355-7707f200-f941-11e8-89e9-aa78ae023d8b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:78,safety,except,except,78,"Btw: IPython 7.2 is live and contains my signature rendering prettificattion except that I made a mistake and it renders wrong. Anyway, I fix that in ipython/ipython#11525 and things should be pretty in IPython 7.2.1:. ![grafik](https://user-images.githubusercontent.com/291575/49578355-7707f200-f941-11e8-89e9-aa78ae023d8b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:41,security,sign,signature,41,"Btw: IPython 7.2 is live and contains my signature rendering prettificattion except that I made a mistake and it renders wrong. Anyway, I fix that in ipython/ipython#11525 and things should be pretty in IPython 7.2.1:. ![grafik](https://user-images.githubusercontent.com/291575/49578355-7707f200-f941-11e8-89e9-aa78ae023d8b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:238,usability,user,user-images,238,"Btw: IPython 7.2 is live and contains my signature rendering prettificattion except that I made a mistake and it renders wrong. Anyway, I fix that in ipython/ipython#11525 and things should be pretty in IPython 7.2.1:. ![grafik](https://user-images.githubusercontent.com/291575/49578355-7707f200-f941-11e8-89e9-aa78ae023d8b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:293,availability,error,error,293,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:303,integrability,sub,subscripted,303,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:156,modifiability,deco,decorators,156,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:293,performance,error,error,293,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:293,safety,error,error,293,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:293,usability,error,error,293,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:414,usability,clear,clear,414,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](. https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:136,deployability,modul,module,136,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:743,integrability,protocol,protocol,743,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:808,integrability,protocol,protocol,808,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1113,integrability,abstract,abstract,1113,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1242,integrability,sub,subclasses,1242,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:743,interoperability,protocol,protocol,743,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:808,interoperability,protocol,protocol,808,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:136,modifiability,modul,module,136,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1078,modifiability,deco,decorator,1078,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1113,modifiability,abstract,abstract,1113,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1855,modifiability,inherit,inherit,1855,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1847,reliability,doe,doesn,1847,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1910,reliability,doe,does,1910,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:136,safety,modul,module,136,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:403,testability,simpl,simply,403,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:403,usability,simpl,simply,403,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:854,usability,document,documented,854,"I only just now got the distinction between types and classes in python. So when they talk about types, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation. - ABCs and regular classes can be used for `isinstance` and `issubclass` checking. - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thats not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whats the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html). - `typing.Mapping` is a generic type, to be used in annotations only. Theres a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py. class EnumerableMixin:. """"""silly mixin class for iterables"""""". def enumerate(self, start=0):. yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):. pass. for i, e in EnumerableList.enumerate(): print(i, e). ```. ABC example:. ```py. class PositiveNumbers(collections.abc.Set):. def __contains__(self, i):. return isinstance(i, int) and i >= 0. def __iter__(self): return itertools.count(). def __len__(self): return float('inf'). # __lt__ is mixed in! print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnt inherit from collections.abc.Set, the __subclasshook__ does its magic here. isinstance({}, collections.abc.Set). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:83,deployability,modul,module,83,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:42,integrability,interfac,interfaces,42,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:277,integrability,sub,subscripted,277,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:42,interoperability,interfac,interfaces,42,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:42,modifiability,interfac,interfaces,42,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:83,modifiability,modul,module,83,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:290,modifiability,paramet,parametric,290,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:441,reliability,doe,does,441,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:83,safety,modul,module,83,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:130,testability,understand,understand,130,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:99,usability,close,closely,99,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation? As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check  e.g. `List[int]`  which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:57,safety,test,test,57,"the runtime checks would be too costly or impossible. to test for `List[int]`, youd have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just cant test it at all  if youd iterate the thing to check the objects it yields, you exhaust it and its no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:188,safety,test,test,188,"the runtime checks would be too costly or impossible. to test for `List[int]`, youd have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just cant test it at all  if youd iterate the thing to check the objects it yields, you exhaust it and its no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:57,testability,test,test,57,"the runtime checks would be too costly or impossible. to test for `List[int]`, youd have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just cant test it at all  if youd iterate the thing to check the objects it yields, you exhaust it and its no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:188,testability,test,test,188,"the runtime checks would be too costly or impossible. to test for `List[int]`, youd have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just cant test it at all  if youd iterate the thing to check the objects it yields, you exhaust it and its no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:298,usability,usab,usable,298,"the runtime checks would be too costly or impossible. to test for `List[int]`, youd have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just cant test it at all  if youd iterate the thing to check the objects it yields, you exhaust it and its no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/374:142,deployability,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:389,deployability,api,api,389,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:555,deployability,contain,contained,555,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:142,integrability,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:389,integrability,api,api,389,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:1453,integrability,sub,subscribed,1453,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:389,interoperability,api,api,389,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:589,interoperability,specif,specifying,589,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:142,modifiability,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:142,safety,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:1650,security,auth,auth,1650,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:142,testability,depend,dependency,142,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:63,usability,close,closer,63,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:118,usability,Clear,Clearly,118,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:314,usability,minim,minimal,314,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:730,usability,user,user-images,730,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/374:1127,usability,user,user-images,1127,"This is caused by an adjustment that tries to keep the legends closer to. the figure compared to the default placing. Clearly, there is some. dependency with the font size that I was not aware of. I will prepare a fix. soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:. >. > import scanpy.api as sc. > adata = sc.datasets.pbmc68k_reduced(). > sc.tl.pca(adata). > sc.pp.neighbors(adata). > sc.tl.umap(adata). >. > when you plot the umap of the bulk labels contained in adata.obs without. > specifying any further settings (i.e. sc.pl.umap(adata, color =. > ['bulk_labels']) ) everything looks fine. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>. >. > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,. > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller. > than the default font size it selects for your legend, the legend overlaps. > with the right edge of the plot. >. > [image: image]. > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>. >. > For me this sometimes leads to issues that I can no longer export figures. > with my desired fontsize for presentations, etc. without it overlapping the. > plot in an ugly way. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/374>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374
https://github.com/scverse/scanpy/issues/375:101,availability,consist,consistency,101,"Not only violin but most plotting options do not consider what you have in `.var`. I this guarantees consistency in that what you see, for example in a violin plot, is always *per obs*. I think that taking the transpose, as you did, should be the right solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:101,usability,consist,consistency,101,"Not only violin but most plotting options do not consider what you have in `.var`. I this guarantees consistency in that what you see, for example in a violin plot, is always *per obs*. I think that taking the transpose, as you did, should be the right solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:54,reliability,Doe,Does,54,So you are saying this is a feature and not a bug  . Does that mean you think one should not be able to plot `.var` covariates by default?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:307,modifiability,variab,variables,307,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str. > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:297,security,access,accessing,297,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str. > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:84,availability,replic,replicate,84,That's the internal structure that is in the `sc.pl.scatter()` function. Could just replicate that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:18,availability,error,error,18,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:187,availability,replic,replicating,187,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:300,availability,error,error,300,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:374,interoperability,specif,specified,374,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:18,performance,error,error,18,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:300,performance,error,error,300,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:18,safety,error,error,18,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:300,safety,error,error,300,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:18,usability,error,error,18,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:300,usability,error,error,300,"ouch, its pretty error prone to just guess! What if a column is in both `.var` and `.obs`? People will never figure out what they need to do in order to get what they want. I dont like replicating that or that it ever went into any function. Explicit is better than implicit. We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:39,performance,time,time,39,"@falexwolf and I discussed this at the time, and came to the conclusion we should be able to assume that the `.var` columns will not be ambiguously named. And as it defaults to `.obs`, it works the same as it would have done before anyway. I see the issue though, as scanpy functions do write to `.obs` and `.var`. On the one hand we would have to check that unambiguous naming is always upheld which can be difficult with scanpy growing, and on the other hand users may not be aware of what columns there are already in `.obs` when they name `.var` columns. While I like the workaround with `adata.T`, it feels strange to tell people to transpose their data to overcome a technical restriction in the plotting function. Explicit is the better solution there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:461,usability,user,users,461,"@falexwolf and I discussed this at the time, and came to the conclusion we should be able to assume that the `.var` columns will not be ambiguously named. And as it defaults to `.obs`, it works the same as it would have done before anyway. I see the issue though, as scanpy functions do write to `.obs` and `.var`. On the one hand we would have to check that unambiguous naming is always upheld which can be difficult with scanpy growing, and on the other hand users may not be aware of what columns there are already in `.obs` when they name `.var` columns. While I like the workaround with `adata.T`, it feels strange to tell people to transpose their data to overcome a technical restriction in the plotting function. Explicit is the better solution there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:138,deployability,observ,observations,138,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:235,deployability,observ,observations,235,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:302,deployability,API,API,302,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:302,integrability,API,API,302,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:302,interoperability,API,API,302,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:369,modifiability,paramet,parameter,369,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:138,testability,observ,observations,138,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:235,testability,observ,observations,235,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:399,testability,simpl,simpler,399,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:399,usability,simpl,simpler,399,"you wanted to say implicit right? and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations! also, theres no technical restriction. its about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:735,availability,consist,consistent,735,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:366,deployability,observ,observations,366,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:350,modifiability,variab,variables,350,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:590,modifiability,variab,variables,590,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:366,testability,observ,observations,366,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:295,usability,user,user,295,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:423,usability,clear,clear,423,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:436,usability,user,user,436,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:463,usability,clear,clear,463,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:492,usability,user,user,492,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:668,usability,user,user-friendly,668,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:735,usability,consist,consistent,735,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:774,usability,clear,clear,774,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:102,availability,error,error,102,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:367,availability,error,error,367,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:182,interoperability,specif,specified,182,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:102,performance,error,error,102,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:367,performance,error,error,367,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:102,safety,error,error,102,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:367,safety,error,error,367,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:102,usability,error,error,102,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:367,usability,error,error,367,"what if i told you that we can have our cake and eat it too? as said before:. > We could throw a nice error if the column isnt in `.obs` but is in `.var` instead, like. > . > > You specified column dropout_per_gene which is not in `.obs`, but in `.var`. Did you mean to call `sc.pl.violin(adata.T, ...)`? Near zero frustration, because people can just do what the error tells them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:701,availability,redund,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:74,deployability,scale,scale,74,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:701,deployability,redundan,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:74,energy efficiency,scale,scale,74,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:498,integrability,wrap,wrapper,498,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:498,interoperability,wrapper,wrapper,498,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:74,modifiability,scal,scale,74,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:618,modifiability,maintain,maintain,618,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:660,modifiability,refact,refactoring,660,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:74,performance,scale,scale,74,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:118,performance,disk,disk,118,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:386,performance,time,time,386,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:660,performance,refactor,refactoring,660,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:701,reliability,redundan,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:618,safety,maintain,maintain,618,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:701,safety,redund,redundant,701,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:738,safety,compl,completely,738,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:738,security,compl,completely,738,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:143,usability,efficien,efficient,143,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/pull/376:760,availability,Error,Error,760,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:76,deployability,API,APIs,76,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:119,deployability,API,APIs,119,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:923,deployability,build,build,923,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:86,energy efficiency,core,core,86,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:76,integrability,API,APIs,76,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:119,integrability,API,APIs,119,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:76,interoperability,API,APIs,76,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:119,interoperability,API,APIs,119,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:57,modifiability,paramet,parameters,57,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:162,modifiability,paramet,parameters,162,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:192,modifiability,paramet,parameters,192,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:312,modifiability,paramet,parameters,312,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:397,modifiability,paramet,parameter,397,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:485,modifiability,paramet,parameter,485,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:931,modifiability,deco,decorator,931,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:967,modifiability,paramet,parameters,967,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:760,performance,Error,Error,760,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:760,safety,Error,Error,760,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:98,security,team,team,98,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:1072,security,sign,signature,1072,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:760,usability,Error,Error,760,"@falexwolf:. It was a mistake to use optional positional parameters for our APIs. The core python team designs all its APIs using almost exclusively keyword-only parameters, because this way, parameters can be added without having to go to the end. (And theyre *much* more conservative and thoughtful in adding parameters in the first place). E.g. in this case theres only one central important parameter apart from `adata`, so a `*` should be after `n_top`, and @grst could add his parameter before `ax`, leaving `ax` at the end of the pameter list without breaking anything:. ```py. def highest_expr_genes(. adata: AnnData,. n_top: int = 30,. *,. show: Optional[bool] = None,. ... ax: Optional[Axes] = None,. **kwds,. ). highest_expr_genes(ad, 12, True) # Error: show is a keyword-only param. highest_expr_genes(ad, 12, show=True) # Works. ```. I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:173,availability,avail,available,173,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:485,deployability,automat,automatically,485,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:499,deployability,updat,updated,499,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:25,modifiability,concern,concern,25,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:173,reliability,availab,available,173,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:173,safety,avail,available,173,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:499,safety,updat,updated,499,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:173,security,availab,available,173,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:273,security,modif,modify,273,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:499,security,updat,updated,499,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:25,testability,concern,concern,25,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:266,testability,simpl,simply,266,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:383,testability,simpl,simply,383,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:485,testability,automat,automatically,485,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:266,usability,simpl,simply,266,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:383,usability,simpl,simply,383,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON. adata.var = adata.var.reset_index().set_index(annot_col). # adata.var_names is automatically updated. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:43,security,modif,modify,43,"Good point @fidelram. If you dont want to modify your adata object permanently:. ```py. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. Is this accepable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:322,availability,consist,consistently,322,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:483,availability,avail,available,483,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:386,interoperability,specif,specific,386,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:468,interoperability,specif,specificity,468,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:116,modifiability,paramet,parameter,116,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:39,reliability,doe,does,39,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:483,reliability,availab,available,483,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:483,safety,avail,available,483,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:483,security,availab,available,483,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:322,usability,consist,consistently,322,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:432,usability,user,users,432,"@fidelram Actually, a similar solution does exist for other plotting functions. @falexwolf added the `gene_symbols` parameter to `sc.pl.rank_genes_groups()` and `sc.pl.rank_genes_groups_violin()` as I was having the same issue of using Ensembl gene IDs. I'm not sure if he regrets doing that now though or not... it's not consistently applied. In either case, Ensembl gene IDs are more specific than HGNC/MGI gene symbols. For some users it may be important that this specificity is available (and you don't keep having to make copies of the data to plot). Also, if alternative splicing is investigated with single-cell data, then you cannot keep gene symbols as `var_names` throughout your dataset as a default. So this is a future-proof approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:131,availability,consist,consistency,131,"In this case it should probably be called `gene_symbols` as well, even though thats not the best name for the parameter, just for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:111,modifiability,paramet,parameter,111,"In this case it should probably be called `gene_symbols` as well, even though thats not the best name for the parameter, just for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:131,usability,consist,consistency,131,"In this case it should probably be called `gene_symbols` as well, even though thats not the best name for the parameter, just for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:425,availability,consist,consistency,425,"@flying-sheep, thanks for catching the argument positioning! @fidelram, I definitely want to keep ENSG as main identifiers. As a related side note, why is actually everyone using gene symbols for single cell data? Though I'd rather have this supported by the plotting function, I could live with the workaround proposed by @flying-sheep. . @LuckyMD, thanks for pointing this out, I renamed the argument to `gene_symbols` for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:111,security,ident,identifiers,111,"@flying-sheep, thanks for catching the argument positioning! @fidelram, I definitely want to keep ENSG as main identifiers. As a related side note, why is actually everyone using gene symbols for single cell data? Though I'd rather have this supported by the plotting function, I could live with the workaround proposed by @flying-sheep. . @LuckyMD, thanks for pointing this out, I renamed the argument to `gene_symbols` for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:242,usability,support,supported,242,"@flying-sheep, thanks for catching the argument positioning! @fidelram, I definitely want to keep ENSG as main identifiers. As a related side note, why is actually everyone using gene symbols for single cell data? Though I'd rather have this supported by the plotting function, I could live with the workaround proposed by @flying-sheep. . @LuckyMD, thanks for pointing this out, I renamed the argument to `gene_symbols` for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:425,usability,consist,consistency,425,"@flying-sheep, thanks for catching the argument positioning! @fidelram, I definitely want to keep ENSG as main identifiers. As a related side note, why is actually everyone using gene symbols for single cell data? Though I'd rather have this supported by the plotting function, I could live with the workaround proposed by @flying-sheep. . @LuckyMD, thanks for pointing this out, I renamed the argument to `gene_symbols` for consistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:398,integrability,protocol,protocols,398,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:398,interoperability,protocol,protocols,398,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:117,performance,time,time,117,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:16,safety,compl,completely,16,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:426,safety,detect,detect,426,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:16,security,compl,completely,16,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:426,security,detect,detect,426,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:433,security,sign,signals,433,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:221,usability,user,user,221,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:121,usability,stop,stop,121,"> As a related side note, why is actually everyone using gene symbols for single cell data? They are? They should really stop doing that, its a bad idea! I thought the field had moved past that long ago! The PR looks good as it is now!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:70,availability,consist,consistently,70,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:117,availability,consist,consistent,117,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:183,deployability,automat,automatized,183,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:931,deployability,build,build,931,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:147,modifiability,paramet,parameter,147,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:219,modifiability,paramet,parameter,219,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:264,modifiability,deco,decorator,264,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:613,modifiability,paramet,parameter,613,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:939,modifiability,deco,decorator,939,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:975,modifiability,paramet,parameters,975,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:291,reliability,doe,does,291,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:1080,security,sign,signature,1080,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:183,testability,automat,automatized,183,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:70,usability,consist,consistently,70,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:117,usability,consist,consistent,117,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:279,usability,effectiv,effectively,279,"> I'm not sure if he regrets doing that now though or not... it's not consistently applied. Yes, we should have some consistent way of adding this parameter. The best way would be an automatized approach that added the parameter to all plotting functions, maybe a decorator that effectively does this:. ```. ad_plot = adata.copy(). ad_plot.var = adata.var.reset_index(drop=True).set_index(annot_col). sc.pl.highest_expr_genes(ad_plot, ...). del ad_plot. ```. without creating a copy of the full data, though. A view would be much better. Some thoughts would be required... I think the name `gene_symbols` for the parameter is OK. I think that using gene symbols in the data analysis scripts for single-cell is also fine. Convenience is important. But for sure, we don't want to keep people who want this to be treated more rigorously from doing it. ----. > I have an idea how to fix this without breaking everything: I think I can build a decorator that allows people to use parameters positionally, but sends a DeprecationWarning when they do, while the docs only report the new signature. This sounds extremely sensible to me. I didn't actually want people to call things with positional arguments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:40,modifiability,paramet,parameter,40,"> I think the name gene_symbols for the parameter is OK. Oh its definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like its faster to see! > I didn't actually want people to call things with positional arguments. Lets pay attention to not include any new function without `*` anymore, OK?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:332,interoperability,specif,specific,332,"> Lets pay attention to not include any new function without * anymore, OK? OK! > Oh its definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like its faster to see! Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:412,interoperability,specif,specific,412,"> Lets pay attention to not include any new function without * anymore, OK? OK! > Oh its definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like its faster to see! Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:284,modifiability,layer,layers,284,"> Lets pay attention to not include any new function without * anymore, OK? OK! > Oh its definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like its faster to see! Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/pull/376:310,modifiability,variab,variables,310,"> Lets pay attention to not include any new function without * anymore, OK? OK! > Oh its definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like its faster to see! Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376
https://github.com/scverse/scanpy/issues/377:45,deployability,scale,scale,45,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:45,energy efficiency,scale,scale,45,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:45,modifiability,scal,scale,45,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:45,performance,scale,scale,45,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:33,security,control,control,33,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:33,testability,control,control,33,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:307,usability,user,user-images,307,"you can use `vmin` and `vmax` to control the scale range. You can also change the colormap using `cmap`. For example: . ````PYTHON. rcParams['figure.figsize'] = 4,4. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=['CD3D'], use_raw=False, cmap='bwr', vmin=-3, vmax=3). ````. ![image](https://user-images.githubusercontent.com/4964309/48904466-bffa7980-ee5e-11e8-94ac-bd29a4552152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:119,availability,error,error,119,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:78,modifiability,paramet,parameter,78,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:119,performance,error,error,119,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:119,safety,error,error,119,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:119,usability,error,error,119,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:137,usability,visual,visualise,137,"@fidelram, just wanted to add that I still cannot use the ```use_raw=False``` parameter, for example I keep getting an error when try to visualise my data with ```sc.pl.rank_genes_groups_heatmap``` or ```sc.pl.rank_genes_groups_matrixplot```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:106,availability,slo,slow,106,"Should be solved via: https://github.com/theislab/scanpy/pull/372#issuecomment-441022399. Sorry for being slow. Thank you, @fidelram!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/377:106,reliability,slo,slow,106,"Should be solved via: https://github.com/theislab/scanpy/pull/372#issuecomment-441022399. Sorry for being slow. Thank you, @fidelram!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/377
https://github.com/scverse/scanpy/issues/378:234,testability,Simpl,Simply,234,The distance matrix you are passing might not be what sklearn wants: by densifying you'll get many zeros that sklearn probably assumes to be true zeros (it likely expects a dense distance matrix with all values actually be computed). Simply pass the data matrix `X_pca` to circumvent this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/378
https://github.com/scverse/scanpy/issues/378:234,usability,Simpl,Simply,234,The distance matrix you are passing might not be what sklearn wants: by densifying you'll get many zeros that sklearn probably assumes to be true zeros (it likely expects a dense distance matrix with all values actually be computed). Simply pass the data matrix `X_pca` to circumvent this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/378
https://github.com/scverse/scanpy/issues/379:0,energy efficiency,Cool,Cool,0,Cool!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/379
https://github.com/scverse/scanpy/pull/380:192,deployability,pipelin,pipeline,192,"As an added note, it would be great to see this 'gene_symbol' argument used uniformly across the plotting functions. We've had to handle it in pretty hacky ways to make it work throughout the pipeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380
https://github.com/scverse/scanpy/pull/380:192,integrability,pipelin,pipeline,192,"As an added note, it would be great to see this 'gene_symbol' argument used uniformly across the plotting functions. We've had to handle it in pretty hacky ways to make it work throughout the pipeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380
https://github.com/scverse/scanpy/pull/380:150,security,hack,hacky,150,"As an added note, it would be great to see this 'gene_symbol' argument used uniformly across the plotting functions. We've had to handle it in pretty hacky ways to make it work throughout the pipeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380
https://github.com/scverse/scanpy/pull/380:2,safety,compl,completely,2,I completely agree! We had this discussion already at some point in another issue (unfortunately don't find that issue right now). I just made a dedicated issue for that: https://github.com/theislab/scanpy/issues/385,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380
https://github.com/scverse/scanpy/pull/380:2,security,compl,completely,2,I completely agree! We had this discussion already at some point in another issue (unfortunately don't find that issue right now). I just made a dedicated issue for that: https://github.com/theislab/scanpy/issues/385,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380
https://github.com/scverse/scanpy/issues/381:83,availability,cluster,cluster,83,"Did you recompute using `tl.paga` in between? You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from . > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:112,availability,cluster,cluster,112,"Did you recompute using `tl.paga` in between? You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from . > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:83,deployability,cluster,cluster,83,"Did you recompute using `tl.paga` in between? You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from . > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:112,deployability,cluster,cluster,112,"Did you recompute using `tl.paga` in between? You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from . > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:319,usability,clear,clear,319,"Did you recompute using `tl.paga` in between? You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from . > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:273,availability,error,error,273,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:256,deployability,updat,updated,256,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:273,performance,error,error,273,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:256,safety,updat,updated,256,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:273,safety,error,error,273,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:256,security,updat,updated,256,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:273,usability,error,error,273,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:111,availability,cluster,clusters,111,"I had the same issue and was suggested to use the 'palette' options. colours will not change when you get less clusters than the time before! tir. 22. jan. 2019 5.06 AM skrev jiawen wang <notifications@github.com>:. > And leiden_colors has the same case. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/381#issuecomment-456264623>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIXvUbBbGv0Lry4qoCeBQP3aDCi9zqNrks5vFo5IgaJpZM4Y7Vbi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:111,deployability,cluster,clusters,111,"I had the same issue and was suggested to use the 'palette' options. colours will not change when you get less clusters than the time before! tir. 22. jan. 2019 5.06 AM skrev jiawen wang <notifications@github.com>:. > And leiden_colors has the same case. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/381#issuecomment-456264623>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIXvUbBbGv0Lry4qoCeBQP3aDCi9zqNrks5vFo5IgaJpZM4Y7Vbi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:304,integrability,sub,subscribed,304,"I had the same issue and was suggested to use the 'palette' options. colours will not change when you get less clusters than the time before! tir. 22. jan. 2019 5.06 AM skrev jiawen wang <notifications@github.com>:. > And leiden_colors has the same case. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/381#issuecomment-456264623>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIXvUbBbGv0Lry4qoCeBQP3aDCi9zqNrks5vFo5IgaJpZM4Y7Vbi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:129,performance,time,time,129,"I had the same issue and was suggested to use the 'palette' options. colours will not change when you get less clusters than the time before! tir. 22. jan. 2019 5.06 AM skrev jiawen wang <notifications@github.com>:. > And leiden_colors has the same case. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/381#issuecomment-456264623>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIXvUbBbGv0Lry4qoCeBQP3aDCi9zqNrks5vFo5IgaJpZM4Y7Vbi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/issues/381:527,security,auth,auth,527,"I had the same issue and was suggested to use the 'palette' options. colours will not change when you get less clusters than the time before! tir. 22. jan. 2019 5.06 AM skrev jiawen wang <notifications@github.com>:. > And leiden_colors has the same case. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/381#issuecomment-456264623>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIXvUbBbGv0Lry4qoCeBQP3aDCi9zqNrks5vFo5IgaJpZM4Y7Vbi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/pull/382:623,energy efficiency,current,current,623,"It would be great is someone could look over my matrix exporter quickly. I am not sure if it uses the fastest way to export the matrix to text and whether it wastes memory. First I do ad.X.transpose().tocsr() then I loop over the gene indices and do row = mat.getrow(i).todense(). In retrospect transpose().tocsr() seems weird. Here is the code:. ://github.com/maximilianh/cellBrowser/blob/master/src/cbPyLib/cellbrowser/cellbrowser.py#L2613. In both places, I need if-then-else constructs, as the anndata matrix can also be not-sparse, and then the tocsr() and getrow() functions are not defined (this may be a bug in the current spring exporter, it probably doesn't work with not-sparse matrices)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:165,performance,memor,memory,165,"It would be great is someone could look over my matrix exporter quickly. I am not sure if it uses the fastest way to export the matrix to text and whether it wastes memory. First I do ad.X.transpose().tocsr() then I loop over the gene indices and do row = mat.getrow(i).todense(). In retrospect transpose().tocsr() seems weird. Here is the code:. ://github.com/maximilianh/cellBrowser/blob/master/src/cbPyLib/cellbrowser/cellbrowser.py#L2613. In both places, I need if-then-else constructs, as the anndata matrix can also be not-sparse, and then the tocsr() and getrow() functions are not defined (this may be a bug in the current spring exporter, it probably doesn't work with not-sparse matrices)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:660,reliability,doe,doesn,660,"It would be great is someone could look over my matrix exporter quickly. I am not sure if it uses the fastest way to export the matrix to text and whether it wastes memory. First I do ad.X.transpose().tocsr() then I loop over the gene indices and do row = mat.getrow(i).todense(). In retrospect transpose().tocsr() seems weird. Here is the code:. ://github.com/maximilianh/cellBrowser/blob/master/src/cbPyLib/cellbrowser/cellbrowser.py#L2613. In both places, I need if-then-else constructs, as the anndata matrix can also be not-sparse, and then the tocsr() and getrow() functions are not defined (this may be a bug in the current spring exporter, it probably doesn't work with not-sparse matrices)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:165,usability,memor,memory,165,"It would be great is someone could look over my matrix exporter quickly. I am not sure if it uses the fastest way to export the matrix to text and whether it wastes memory. First I do ad.X.transpose().tocsr() then I loop over the gene indices and do row = mat.getrow(i).todense(). In retrospect transpose().tocsr() seems weird. Here is the code:. ://github.com/maximilianh/cellBrowser/blob/master/src/cbPyLib/cellbrowser/cellbrowser.py#L2613. In both places, I need if-then-else constructs, as the anndata matrix can also be not-sparse, and then the tocsr() and getrow() functions are not defined (this may be a bug in the current spring exporter, it probably doesn't work with not-sparse matrices)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:114,availability,avail,available,114,"Yes, you have the choice of either having 'gene_symbols' as your index or 'gene_ids', what is not in the index is available as a column in `.var`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:114,reliability,availab,available,114,"Yes, you have the choice of either having 'gene_symbols' as your index or 'gene_ids', what is not in the index is available as a column in `.var`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:114,safety,avail,available,114,"Yes, you have the choice of either having 'gene_symbols' as your index or 'gene_ids', what is not in the index is available as a column in `.var`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:114,security,availab,available,114,"Yes, you have the choice of either having 'gene_symbols' as your index or 'gene_ids', what is not in the index is available as a column in `.var`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:162,deployability,updat,update,162,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:108,performance,perform,performance,108,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:162,safety,updat,update,162,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:162,security,updat,update,162,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:108,usability,perform,performance,108,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:33,deployability,log,logic,33,"Thanks! Using the most formal of logic :-), line length is now 80 chars.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:33,safety,log,logic,33,"Thanks! Using the most formal of logic :-), line length is now 80 chars.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:33,security,log,logic,33,"Thanks! Using the most formal of logic :-), line length is now 80 chars.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:33,testability,log,logic,33,"Thanks! Using the most formal of logic :-), line length is now 80 chars.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:52,modifiability,pac,package,52,"I wonder if cellbrowser should be a suggested extra package for scanpy now... but spring isn't one either, so probably not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:58,modifiability,pac,package,58,"Thank you, I'll merge this! We'll get back to the ""extra"" package question, soon... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/issues/383:233,deployability,contain,contains,233,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:196,modifiability,variab,variable,196,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:409,modifiability,pac,pac,409,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:461,modifiability,variab,variable,461,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:127,usability,visual,visualize,127,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:767,usability,help,helpful,767,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:919,usability,help,helps,919,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:. ```. adata_needed.obs['cell_type'] = my_list_with_cd_labels. scn.tl.pca(adata_needed). scn.pl.pac(adata_needed, color='cell_type'). ```. Here the variable `my_list_with_cd_labels` should look someting like this:. `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`. Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/383:27,usability,clear,clear,27,Thank you! It was just not clear from the tutorial in the beginning that the same thing is used to define colors in plots :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/385:157,deployability,automat,automatically,157,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:907,modifiability,paramet,parameters,907,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:959,modifiability,paramet,parameter,959,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:215,reliability,doe,doesn,215,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:767,safety,compl,complicated,767,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:576,security,iso,isoform,576,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:767,security,compl,complicated,767,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:157,testability,automat,automatically,157,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:307,usability,user,user,307,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:372,usability,user,user,372,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:531,usability,person,person,531,"While I think this is a good idea for most applications there are a few issues I see with this:. 1. As far as I'm aware there is no other functionality that automatically looks for a column in `.var` or `.obs` that doesn't have a matching function that puts that column there. So here you would rely on the user to spell `'gene_symbols'` exactly as you expect without the user necessarily knowing that the functions are looking for this. 2. There are cases where someone may want to plot other IDs than gene symbols even when that person has gene symbols stored (For example: isoform expression). For this to work, I think one would need a `sc.pp.store_gene_symbols()` function. However, then you end up with functions for everything which makes the whole thing more complicated than it should be. Also, you would need to be able to turn the gene symbol lookup off. In that case you have the same number of parameters compared to just adding a `gene_symbols` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:232,availability,avail,available,232,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:232,reliability,availab,available,232,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:232,safety,avail,available,232,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:150,security,ident,identifier,150,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:232,security,availab,available,232,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:0,usability,Person,Personally,0,"Personally, I don't like it because. * Explicit is better than implicit. * there could be cases where I would like to plot Ensembl/Entrez or whatever identifier I have in `var_names` directly, even if I have a `gene_symbols` column available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:215,modifiability,paramet,parameter,215,"Addressing a few issues:. - The convention that gene symbols should be stored as `'gene_symbols'` would be broadly advertised in the docs. It's the convention used by `read_10x_h5` and `read_10x_mtx` already. - The parameter `gene_symbols` would still exist but would rarely be used. If used, it can be used to point to the index and to arbitrary fields. But its default value `gene_symbols=None` would trigger lookup of a column `'gene_symbols'` if it exists, and lookup of the index if it doesn't. That's the same behavior as for `.raw`. There would not be any additional function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:491,reliability,doe,doesn,491,"Addressing a few issues:. - The convention that gene symbols should be stored as `'gene_symbols'` would be broadly advertised in the docs. It's the convention used by `read_10x_h5` and `read_10x_mtx` already. - The parameter `gene_symbols` would still exist but would rarely be used. If used, it can be used to point to the index and to arbitrary fields. But its default value `gene_symbols=None` would trigger lookup of a column `'gene_symbols'` if it exists, and lookup of the index if it doesn't. That's the same behavior as for `.raw`. There would not be any additional function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:516,usability,behavi,behavior,516,"Addressing a few issues:. - The convention that gene symbols should be stored as `'gene_symbols'` would be broadly advertised in the docs. It's the convention used by `read_10x_h5` and `read_10x_mtx` already. - The parameter `gene_symbols` would still exist but would rarely be used. If used, it can be used to point to the index and to arbitrary fields. But its default value `gene_symbols=None` would trigger lookup of a column `'gene_symbols'` if it exists, and lookup of the index if it doesn't. That's the same behavior as for `.raw`. There would not be any additional function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:556,usability,user,user,556,I wasn't aware of `read_10x_h5` creating a `.var['gene_symbols']` column. In that case it's quite an elegant solution for people who read in 10x data directly. There will probably be more people who use `read_10x_h5` and `read_10x_mtx` than people who do not want to plot gene symbols. In that case I guess the convenience of the many outweighs the needs of the few. Maybe output something to say that the `gene_symbols` column in `var` is being used for plotting when it is found. Then it's not like some magic is happening in the background of which the user is ignorant.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:53,deployability,updat,update,53,"Yeah, I decided just to go for it . I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:53,safety,updat,update,53,"Yeah, I decided just to go for it . I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/385:53,security,updat,update,53,"Yeah, I decided just to go for it . I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385
https://github.com/scverse/scanpy/issues/386:71,deployability,updat,update,71,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:652,integrability,sub,subscribed,652,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:4,modifiability,variab,variable,4,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:251,modifiability,variab,variable,251,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:579,modifiability,variab,variable,579,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:71,safety,updat,update,71,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:71,security,updat,update,71,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:849,security,auth,auth,849,"The variable y_axis is something I introduced in my latest PR. If you. update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,. > particularly when swap_axes=True. Examples here. > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,. > particularly code in line 7, show this. How do I do this? When I use it now. > with my code, it always chooses a uniform y-axis limit for all genes. Which. > option do I use for variable y-axis limits? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:2,safety,reme,remember,2,"I remember I did something like that in the past but I cant not figure it. out now. My suggestion is to try something like (not sure it will work):. axes = sc.pl.stacked_violin(......, show=False). for ax in axes:. ax.set_ylim(0, 5). On Thu, May 16, 2019 at 10:16 PM JPV95 <notifications@github.com> wrote:. > Is there a way to set a fixed y-axis? If I want the y-axis to be bound. > between say 0 and 5? >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386?email_source=notifications&email_token=ABF37VMGLXVMB6NKK6Y6UD3PVW6JJA5CNFSM4GH7A7BKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVS6DZY#issuecomment-493216231>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNWE5AGCXB2ETKTKQTPVW6JJANCNFSM4GH7A7BA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:808,security,auth,auth,808,"I remember I did something like that in the past but I cant not figure it. out now. My suggestion is to try something like (not sure it will work):. axes = sc.pl.stacked_violin(......, show=False). for ax in axes:. ax.set_ylim(0, 5). On Thu, May 16, 2019 at 10:16 PM JPV95 <notifications@github.com> wrote:. > Is there a way to set a fixed y-axis? If I want the y-axis to be bound. > between say 0 and 5? >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/386?email_source=notifications&email_token=ABF37VMGLXVMB6NKK6Y6UD3PVW6JJA5CNFSM4GH7A7BKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVS6DZY#issuecomment-493216231>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNWE5AGCXB2ETKTKQTPVW6JJANCNFSM4GH7A7BA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:4,safety,reme,remember,4,"> I remember I did something like that in the past but I cant not figure it out now. My suggestion is to try something like (not sure it will work): axes = sc.pl.stacked_violin(......, show=False) for ax in axes: ax.set_ylim(0, 5). @fidelram Thanks a lot for the helpful suggestions here. I am a bit unsure on how to ouput the list of Axes objects (`axes`) as a plot after setting the ylim on each element of the list. Could you advise? Thanks a lot in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:263,usability,help,helpful,263,"> I remember I did something like that in the past but I cant not figure it out now. My suggestion is to try something like (not sure it will work): axes = sc.pl.stacked_violin(......, show=False) for ax in axes: ax.set_ylim(0, 5). @fidelram Thanks a lot for the helpful suggestions here. I am a bit unsure on how to ouput the list of Axes objects (`axes`) as a plot after setting the ylim on each element of the list. Could you advise? Thanks a lot in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:58,availability,error,error,58,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:42,deployability,fail,failed,42,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:330,deployability,modul,module,330,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:330,modifiability,modul,module,330,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:58,performance,error,error,58,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:42,reliability,fail,failed,42,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:58,safety,error,error,58,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:330,safety,modul,module,330,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:141,security,rotat,rotation,141,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:266,testability,Trace,Traceback,266,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:58,usability,error,error,58,"Hi,. I tried to set the y-axis limit, but failed with the error:. >>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). >>> for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File ""<stdin>"", line 2, in <module>. AttributeError: 'str' object has no attribute 'set_ylim'. I use scanpy 1.8.1. Do you have any idea? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:759,availability,error,error,759,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:743,deployability,fail,failed,743,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:594,integrability,Sub,Subject,594,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:187,modifiability,paramet,parameter,187,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:656,modifiability,variab,variable,656,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:759,performance,error,error,759,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:101,reliability,doe,doesn,101,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:743,reliability,fail,failed,743,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:759,safety,error,error,759,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:1243,safety,safe,safelinks,1243," array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:1700,safety,safe,safelinks,1700," I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:2220,safety,safe,safelinks,2220,"on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=jfOeBNWAPuUo7LZ0gx6ArEZqlajNpIB1XHvuXjBYblo%3D&reserved=0>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:2700,safety,safe,safelinks,2700,"on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=jfOeBNWAPuUo7LZ0gx6ArEZqlajNpIB1XHvuXjBYblo%3D&reserved=0>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:286,security,access,access,286,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:843,security,rotat,rotation,843,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:1793,security,auth,auth,1793,"olin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:2148,security,Triag,Triage,2148,"g this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NONHFLCjdRyCv1jUBuSyGgy4%2FX8do5WWWrrPPyLk5tw%3D&reserved=0>. Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=%2FXLG4aYkL8kN8XOgwQdaIBq3OoN%2FjjSAE984xAYIRDM%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542588546%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C10",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:964,testability,Trace,Traceback,964,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:12,usability,document,documentation,12,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:759,usability,error,error,759,"Look at the documentation before you ask questions. The object returned from the function you called doesnt return a matplotlib object, it returns a dictionary, assuming that the show parameter is off. You cant loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ylim property. Get Outlook for iOS<https://aka.ms/o0ukef>. ________________________________. From: ZxyChopcat ***@***.***>. Sent: Thursday, September 16, 2021 1:24:05 PM. To: theislab/scanpy ***@***.***>. Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>. Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,. I tried to set the y-axis limit, but failed with the error:. `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:. ... ax.set_ylim(0, 5). ... Traceback (most recent call last):. File """", line 2, in. AttributeError: 'str' object has no attribute 'set_ylim'. `. I use scanpy 1.8.1. Do you have any idea? Thanks! . You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:105,integrability,sub,subplots,105,"For anyone who want to change the y-axis range for `sc.pl.violin`, here is the code:. ```. fig, ax = plt.subplots(). sc.pl.violin(adata, ['n_genes_by_counts'],. rotation=90,. jitter=0.4,ax=ax, show=False). ax.set_ylim(1000, 5000). plt.show(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:161,security,rotat,rotation,161,"For anyone who want to change the y-axis range for `sc.pl.violin`, here is the code:. ```. fig, ax = plt.subplots(). sc.pl.violin(adata, ['n_genes_by_counts'],. rotation=90,. jitter=0.4,ax=ax, show=False). ax.set_ylim(1000, 5000). plt.show(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/387:311,availability,cluster,cluster,311,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:526,availability,consist,consistency,526,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:311,deployability,cluster,cluster,311,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:476,deployability,updat,update,476,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:305,energy efficiency,green,green,305,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:353,energy efficiency,green,green,353,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:344,interoperability,exchang,exchange,344,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:476,safety,updat,update,476,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:476,security,updat,update,476,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:14,usability,custom,custom,14,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:526,usability,consist,consistency,526,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:488,availability,cluster,cluster,488,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:714,availability,consist,consistency,714,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:488,deployability,cluster,cluster,488,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:661,deployability,updat,update,661,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:482,energy efficiency,green,green,482,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:530,energy efficiency,green,green,530,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:786,integrability,sub,subscribed,786,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:521,interoperability,exchang,exchange,521,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:661,safety,updat,update,661,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:661,security,updat,update,661,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:1009,security,auth,auth,1009,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:173,usability,custom,custom,173,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:714,usability,consist,consistency,714,"I am partial color blind as well. So I second any initiative in this. direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. > . >. > It would, of course, be easy to change this, but then everything changes. > for everyone and many people will wonder why everything looks different now. > (""where is my green cluster?""). If we do it, we only exchange green with. > another color, so that at least all other colors will be unaffected... >. > I would have liked to wait until a major update, because I consider this. > breaking backward consistency, though... >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:195,energy efficiency,green,green,195,"I dont consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. If we only switch the green or red color with another, most people wont even notice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:41,interoperability,compatib,compatibility,41,"I dont consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. If we only switch the green or red color with another, most people wont even notice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:203,energy efficiency,green,green,203,"> I dont consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. > . > If we only switch the green or red color with another, most people wont even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. > . > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:284,energy efficiency,green,green,284,"> I dont consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. > . > If we only switch the green or red color with another, most people wont even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. > . > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:43,interoperability,compatib,compatibility,43,"> I dont consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. > . > If we only switch the green or red color with another, most people wont even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. > . > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:220,availability,down,down,220,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:19,energy efficiency,green,green,19,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:185,energy efficiency,green,green,185,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:204,energy efficiency,green,green,204,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:109,energy efficiency,green,green,109,I'm fine with such rather small changes. This should indeed not bother people. Tell me when you decided on a green that satisfies @ftheis. We don't want to make this change multiple times... :wink:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:182,performance,time,times,182,I'm fine with such rather small changes. This should indeed not bother people. Tell me when you decided on a green that satisfies @ftheis. We don't want to make this change multiple times... :wink:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:565,deployability,version,version,565,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:478,energy efficiency,green,green,478,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:565,integrability,version,version,565,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:980,integrability,sub,subtle,980,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:565,modifiability,version,version,565,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:34,usability,close,close,34,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:256,usability,user,user-images,256,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:375,usability,user,user-images,375,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:744,usability,user,user-images,744,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:863,usability,user,user-images,863,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green. - blue and purple. - orange and kakhi. Playing around a bit, its easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly. --- | ---. ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:604,deployability,version,version,604,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:66,energy efficiency,green,green,66,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:509,energy efficiency,green,green,509,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:604,integrability,version,version,604,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:604,modifiability,version,version,604,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:308,usability,close,close,308,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested? > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:. > . > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. > . > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf. > . > Normal	Deuteranomaly. > 	. > 	 red and green. > 	 blue and purple. > 	 orange and kakhi. > Playing around a bit, its easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf. > . > Normal	Deuteranomaly. > 	. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread. > . ---. Fabian Theis. Institute of Computational Biology - http://icb.helmholtz-muenchen.de. Helmholtz Zentrum Mnchen and Depts. Mathematics&Life Sciences, TU Mnchen. Helmholtz Zentrum Muenchen. Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH). Ingolstaedter Landstr. 1. 85764 Neuherberg. www.helmholtz-muenchen.de. Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann. Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter. Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen. Registergericht: Amtsgericht Muenchen HRB 6466. USt-IdNr: DE 129521671.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:62,energy efficiency,green,green,62,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:183,energy efficiency,model,model,183,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:183,security,model,model,183,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:199,usability,close,closeness,199,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:160,safety,except,except,160,"@ftheis So `#ff7f0e` and `#2ca02c` is the problem, rather than `#d62728` and `#2ca02c`? The `#ff7f0e` is an orange which appears distinguishable from the rest (except `#b5bd61`) in the deuteranomaly simulated colours above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:199,testability,simul,simulated,199,"@ftheis So `#ff7f0e` and `#2ca02c` is the problem, rather than `#d62728` and `#2ca02c`? The `#ff7f0e` is an orange which appears distinguishable from the rest (except `#b5bd61`) in the deuteranomaly simulated colours above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:82,energy efficiency,green,green,82,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:25,modifiability,extens,extension,25,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:386,modifiability,paramet,parameters,386,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:488,safety,compl,completely,488,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:488,security,compl,completely,488,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:234,testability,simul,simulation,234,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:206,usability,confirm,confirmed,206,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:427,usability,minim,minimum,427,"when you use the firefox extension I quoted above, youll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:0,energy efficiency,cool,cool,0,cool... I didn't know backticks made that possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:69,usability,user,user-images,69,Fixed in 58fae77cc15893503c0c34ce0295dd6f67af2bd7. ![grafik](https://user-images.githubusercontent.com/291575/49696239-acae1480-fba7-11e8-8801-10c9ac825481.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/388:10,deployability,patch,patch,10,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them... if ('vmax' in kwds) and ('vmin' in kwds):. _vmax = kwds['vmax']. _vmin = kwds['vmin']. else: . _vmax = max(mean_flat). _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) . normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:459,integrability,sub,submit,459,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them... if ('vmax' in kwds) and ('vmin' in kwds):. _vmax = kwds['vmax']. _vmin = kwds['vmin']. else: . _vmax = max(mean_flat). _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) . normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:10,safety,patch,patch,10,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them... if ('vmax' in kwds) and ('vmin' in kwds):. _vmax = kwds['vmax']. _vmin = kwds['vmin']. else: . _vmax = max(mean_flat). _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) . normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:10,security,patch,patch,10,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them... if ('vmax' in kwds) and ('vmin' in kwds):. _vmax = kwds['vmax']. _vmin = kwds['vmin']. else: . _vmax = max(mean_flat). _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) . normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:78,usability,user,user,78,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them... if ('vmax' in kwds) and ('vmin' in kwds):. _vmax = kwds['vmax']. _vmin = kwds['vmin']. else: . _vmax = max(mean_flat). _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) . normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:145,deployability,patch,patch,145,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:625,integrability,sub,submit,625,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:697,integrability,sub,subscribed,697,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:145,safety,patch,patch,145,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:145,security,patch,patch,145,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:920,security,auth,auth,920,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:223,usability,user,user,223,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem... >. > import matplotlib.colors. >. > #if user defined, then use the vmax, vmin keywords, else use data to generate them... > if ('vmax' in kwds) and ('vmin' in kwds):. > _vmax = kwds['vmax']. > _vmin = kwds['vmin']. > else:. > _vmax = max(mean_flat). > _vmin = min(mean_flat). >. > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)). > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). >. > I'll submit a pull request. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:380,deployability,patch,patch,380,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:896,integrability,sub,submit,896,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:974,integrability,sub,subscribed,974,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:380,safety,patch,patch,380,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:380,security,patch,patch,380,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1209,security,auth,auth,1209,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1329,security,auth,authored,1329,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1546,security,auth,auth,1546,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:83,usability,user,user,83,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:466,usability,user,user,466,"Hi Fidel,. Please note new pull request. dotplot can take vmin vmax arguments from user. <https://github.com/theislab/scanpy/pull/390>. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:109,availability,error,error,109,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:203,availability,error,error,203,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:299,availability,error,error,299,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:577,availability,error,error,577,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:932,deployability,patch,patch,932,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1448,integrability,sub,submit,1448,"rator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1526,integrability,sub,subscribed,1526,"rator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:57,modifiability,deco,decorator,57,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:160,modifiability,deco,decorator,160,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:237,modifiability,paramet,parameter,237,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:263,modifiability,deco,decorator,263,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:340,modifiability,deco,decorator,340,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:544,modifiability,deco,decorator,544,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:639,modifiability,deco,decorators,639,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:109,performance,error,error,109,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:203,performance,error,error,203,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:299,performance,error,error,299,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:577,performance,error,error,577,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:515,reliability,doe,does,515,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:109,safety,error,error,109,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:203,safety,error,error,203,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:299,safety,error,error,299,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:577,safety,error,error,577,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:932,safety,patch,patch,932,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:932,security,patch,patch,932,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1761,security,auth,auth,1761,"rator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1881,security,auth,authored,1881,"rator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:2098,security,auth,auth,2098,"rator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:608,testability,understand,understand,608,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:109,usability,error,error,109,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:203,usability,error,error,203,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:299,usability,error,error,299,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:577,usability,error,error,577,"mmmm... looks like there are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1018,usability,user,user,1018,"here are some difficulties here. The decorator sitting ontop of dotplot() causes a weird error for kwds. dictionary lookups. If I leave the decorator in place, then I get a. keywords error when, vmin is left out as a parameter. If I take the. decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to. ensure that the __doc__ string starts with a '\' character. And in the case. of dotplot() it already does. When I comment out the decorator, the code. works. This error is too strange for me to understand. I don't often use. decorators, and it seems to be the problem here. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-44438",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:235,deployability,updat,update,235,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:519,deployability,patch,patch,519,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1035,integrability,sub,submit,1035,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1113,integrability,sub,subscribed,1113,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:40,modifiability,deco,decorator,40,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:184,safety,test,testing,184,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:235,safety,updat,update,235,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:259,safety,test,testing,259,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:519,safety,patch,patch,519,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:235,security,updat,update,235,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:519,security,patch,patch,519,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1348,security,auth,auth,1348,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1468,security,auth,authored,1468,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:1685,security,auth,auth,1685,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:184,testability,test,testing,184,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:259,testability,test,testing,259,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:605,usability,user,user,605,"mmmm #2... The previous email regarding decorator issue, may have been a problem from. namespace shadowing in a jupyter notebook. I think the previous pull. request works--but suggest testing with and without vmin vmax arguments. I'll update you as I do more testing myself. Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>. wrote:. > The change is quite useful. Please go ahead and add a PR. >. > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. >. > > Here is a patch that fixes the above problem... > >. > > import matplotlib.colors. > >. > > #if user defined, then use the vmax, vmin keywords, else use data to. > generate them... > > if ('vmax' in kwds) and ('vmin' in kwds):. > > _vmax = kwds['vmax']. > > _vmin = kwds['vmin']. > > else:. > > _vmax = max(mean_flat). > > _vmin = min(mean_flat). > >. > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),. > vmax=max(mean_flat)). > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). > >. > > I'll submit a pull request. > >. > > . > > You are receiving this because you are subscribed to this thread. > > Reply to this email directly, view it on GitHub. > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z. > >. > > . > >. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:68,interoperability,share,shared,68,"`@doc_params` is used to add keyword argument descriptions that are shared among many functions to a specific docstring. it has so far not posed any problems. In the case of `dotplot`, it adds, e.g., the `{common_plot_args}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:101,interoperability,specif,specific,101,"`@doc_params` is used to add keyword argument descriptions that are shared among many functions to a specific docstring. it has so far not posed any problems. In the case of `dotplot`, it adds, e.g., the `{common_plot_args}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:297,interoperability,share,shared,297,"Yes, sorry about the confusion--I reneged the post about the decorator. (@doc_params). So I believe the pull request improves the dotplot function. Tim. On Wed, Dec 5, 2018 at 6:49 PM Alex Wolf <notifications@github.com> wrote:. > @doc_params is used to add keyword argument descriptions that are shared. > among many functions to a specific docstring. it has so far not posed any. > problems. In the case of dotplot, it adds, e.g., the {common_plot_args}. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444729429>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1mdyD6lQPepHkr7SbpovYLGU-Azks5u2IW4gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:333,interoperability,specif,specific,333,"Yes, sorry about the confusion--I reneged the post about the decorator. (@doc_params). So I believe the pull request improves the dotplot function. Tim. On Wed, Dec 5, 2018 at 6:49 PM Alex Wolf <notifications@github.com> wrote:. > @doc_params is used to add keyword argument descriptions that are shared. > among many functions to a specific docstring. it has so far not posed any. > problems. In the case of dotplot, it adds, e.g., the {common_plot_args}. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444729429>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1mdyD6lQPepHkr7SbpovYLGU-Azks5u2IW4gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:61,modifiability,deco,decorator,61,"Yes, sorry about the confusion--I reneged the post about the decorator. (@doc_params). So I believe the pull request improves the dotplot function. Tim. On Wed, Dec 5, 2018 at 6:49 PM Alex Wolf <notifications@github.com> wrote:. > @doc_params is used to add keyword argument descriptions that are shared. > among many functions to a specific docstring. it has so far not posed any. > problems. In the case of dotplot, it adds, e.g., the {common_plot_args}. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444729429>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1mdyD6lQPepHkr7SbpovYLGU-Azks5u2IW4gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:502,security,auth,authored,502,"Yes, sorry about the confusion--I reneged the post about the decorator. (@doc_params). So I believe the pull request improves the dotplot function. Tim. On Wed, Dec 5, 2018 at 6:49 PM Alex Wolf <notifications@github.com> wrote:. > @doc_params is used to add keyword argument descriptions that are shared. > among many functions to a specific docstring. it has so far not posed any. > problems. In the case of dotplot, it adds, e.g., the {common_plot_args}. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444729429>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1mdyD6lQPepHkr7SbpovYLGU-Azks5u2IW4gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/388:719,security,auth,auth,719,"Yes, sorry about the confusion--I reneged the post about the decorator. (@doc_params). So I believe the pull request improves the dotplot function. Tim. On Wed, Dec 5, 2018 at 6:49 PM Alex Wolf <notifications@github.com> wrote:. > @doc_params is used to add keyword argument descriptions that are shared. > among many functions to a specific docstring. it has so far not posed any. > problems. In the case of dotplot, it adds, e.g., the {common_plot_args}. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/388#issuecomment-444729429>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1mdyD6lQPepHkr7SbpovYLGU-Azks5u2IW4gaJpZM4ZB23Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/issues/389:112,usability,tool,tools,112,"Hm, the [code](https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188) for this looks fine: do you have less than 5 groups? ```. n_panels_x = min(n_panels_per_row, len(group_names)). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:60,deployability,version,version,60,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:60,integrability,version,version,60,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:509,integrability,sub,subscribed,509,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:60,modifiability,version,version,60,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:45,safety,test,testing,45,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:732,security,auth,auth,732,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:45,testability,test,testing,45,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:316,usability,tool,tools,316,"It is working fine for me and is part of the testing. Which version do you. have? maybe you need to use `n_panels_per_row`. On Thu, Dec 6, 2018 at 3:55 AM Alex Wolf <notifications@github.com> wrote:. > Hm, the code. > <https://github.com/theislab/scanpy/blob/21adc0c9a31fb1eebb16579aa4f41700bc939aa2/scanpy/plotting/tools/__init__.py#L180-L188>. > for this looks fine: do you have less than 5 groups? >. > n_panels_x = min(n_panels_per_row, len(group_names)). >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/389#issuecomment-444730523>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1du-PES9h_EXgZVonUcbuvlvFKdWks5u2IcsgaJpZM4ZCXJs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/issues/389:39,performance,time,time,39,"It was indeed that. Sorry for the long time for checking, I forgot about it (Christmas holiday party effect).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/389
https://github.com/scverse/scanpy/pull/390:88,performance,time,times,88,Could you please stop mentioning me? Im not a part of this project. You did it several times already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:17,usability,stop,stop,17,Could you please stop mentioning me? Im not a part of this project. You did it several times already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:201,usability,help,help,201,"Its @fidelram. The problem is probably that GitHub suggests you, @fidel, in autocompletions even though youre not a contributer. Maybe adding @fidelram to the contributors list of this project would help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:295,deployability,fail,failing,295,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:421,deployability,fail,failing,421,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:2,integrability,sub,submitted,2,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:225,integrability,sub,submission,225,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:295,reliability,fail,failing,295,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:421,reliability,fail,failing,421,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:256,safety,test,tests,256,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:285,safety,test,tests,285,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:411,safety,test,tests,411,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:367,security,modif,modified,367,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:256,testability,test,tests,256,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:285,testability,test,tests,285,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:411,testability,test,tests,411,"I submitted both the exact changes suggested by @flying-sheep and . normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax')). colors = [cmap(normalize(value)) for value in mean_flat]. as a second submission, trying to pass the tests. But it looks like the tests are failing for some other reason outside of the anndata.py file (only file modified in my commit). . I don't think the tests are failing from the changes made here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:13,availability,error,error,13,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:54,deployability,updat,update,54,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:13,performance,error,error,13,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:298,performance,time,time,298,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:13,safety,error,error,13,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:54,safety,updat,update,54,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:54,security,updat,update,54,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:13,usability,error,error,13,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py. colors = cmap(normalize(mean_flat)). ```. Its both faster and necessary: `Normalize` determines vmin and vmax from the first time its called when theyre not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh. git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883. git push -f. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:609,security,auth,auth,609,"I reset to previous commit (a4b3ccd. <https://github.com/theislab/scanpy/pull/390/commits/a4b3ccd88f0412461813838d5435ce0cc0b10883>). and pushed as instructed. Tim. On Mon, Dec 10, 2018 at 11:52 PM Philipp A. <notifications@github.com>. wrote:. > you forgot the second line. >. > colors = cmap(normalize(mean_flat)). >. > Its both mecessary and faster. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/390#issuecomment-446104893>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AACez1CSTh_Npr_Sbzw03Tl2tRfvQsd0ks5u32QngaJpZM4ZEC6l>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:19,availability,error,errors,19,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:76,availability,failur,failures,76,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:76,deployability,fail,failures,76,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:19,performance,error,errors,19,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:76,performance,failur,failures,76,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:76,reliability,fail,failures,76,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:19,safety,error,errors,19,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/pull/390:19,usability,error,errors,19,"OK great. The same errors as on master happen, so this didnt introduce any failures. Thank you very much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/issues/391:308,availability,error,error,308,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:981,integrability,sub,subscribed,981,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:308,performance,error,error,308,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:308,safety,error,error,308,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1178,security,auth,auth,1178,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:308,usability,error,error,308,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:859,usability,learn,learn,859,"It looks like your adata object is corrupted. You should be able to type. `adata.X` to get the matrix. How are you generating the adata object? On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,. >. > When running sc.pp.highly_variable_genes(adata.X) I get the following. > error:. >. > AttributeError: X not found. >. > I then ran sc.pp.highly_variable_genes(adata) and got the following:. >. > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,. > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). > You can drop duplicate edges by setting the duplicates kwarg. >. > The older sc.pp.filter_genes_dispersion(adata.X) works fine. >. > Do you know how to fix this? >. > Thank you! >. > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0. > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > louvain==0.6.1. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/391>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:146,interoperability,format,format,146,When I run adata.X I get . ```<14636x24181 sparse matrix of type '<class 'numpy.float32'>' with 16866605 stored elements in Compressed Sparse Row format>```. That looks fine?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:127,deployability,log,logarithmized,127,"Hi,. could you please try . ```. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata). ```. As highly_variable_genes expects logarithmized data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:127,safety,log,logarithmized,127,"Hi,. could you please try . ```. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata). ```. As highly_variable_genes expects logarithmized data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:127,security,log,logarithmized,127,"Hi,. could you please try . ```. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata). ```. As highly_variable_genes expects logarithmized data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:127,testability,log,logarithmized,127,"Hi,. could you please try . ```. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata). ```. As highly_variable_genes expects logarithmized data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:177,availability,error,error,177,"Hi @Koncopd, my data are indeed already normalised. @fidelram I generated the data merging a few datasets using ```bbknn```. But when I tried on a single sample, I got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:177,performance,error,error,177,"Hi @Koncopd, my data are indeed already normalised. @fidelram I generated the data merging a few datasets using ```bbknn```. But when I tried on a single sample, I got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:177,safety,error,error,177,"Hi @Koncopd, my data are indeed already normalised. @fidelram I generated the data merging a few datasets using ```bbknn```. But when I tried on a single sample, I got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:177,usability,error,error,177,"Hi @Koncopd, my data are indeed already normalised. @fidelram I generated the data merging a few datasets using ```bbknn```. But when I tried on a single sample, I got the same error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:257,reliability,Doe,Does,257,"Hm, very hard to say anything without looking at the dataset. Any negative values in the dataset? Or. ```. X = adata.X. X = np.log1p(X). X = np.expm1(X). mean = X.mean(axis=0).A1. mean[mean == 0] = 1e-12. mean = np.log1p(mean). np.any(mean == np.inf). ```. Does it show true?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:283,deployability,modul,module,283,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:87,integrability,batch,batch-removed,87,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:283,modifiability,modul,module,283,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:87,performance,batch,batch-removed,87,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:256,safety,input,input-,256,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:283,safety,modul,module,283,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:212,testability,Trace,Traceback,212,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:256,usability,input,input-,256,When I run this on the single sample I get ```False```. When I run this on the merged (batch-removed) sample I get:. ```---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-23-4cda28f741b3> in <module>. 2 X = np.log1p(X). 3 X = np.expm1(X). ----> 4 mean = X.mean(axis=0).A1. 5 mean[mean == 0] = 1e-12. 6 mean = np.log1p(mean). AttributeError: 'numpy.ndarray' object has no attribute 'A1'```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:294,availability,error,error,294,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:469,deployability,updat,updated,469,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:300,integrability,messag,message,300,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:300,interoperability,messag,message,300,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:294,performance,error,error,294,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:85,reliability,doe,does,85,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:316,reliability,doe,does,316,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:294,safety,error,error,294,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:469,safety,updat,updated,469,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:469,security,updat,updated,469,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:288,usability,clear,clear,288,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:294,usability,error,error,294,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:664,availability,error,error,664,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3217,availability,avail,available,3217,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:343,deployability,log,log,343,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1416,deployability,log,log,1416,"genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1437,deployability,log,log,1437,"ariable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1613,deployability,log,log,1613,". flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1634,deployability,log,log,1634,". ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_group",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1810,deployability,modul,module,1810," result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2985,deployability,contain,contains,2985,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3125,deployability,contain,contains,3125,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2790,energy efficiency,core,core,2790,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1195,integrability,sub,subtract,1195,"rsion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2133,integrability,sub,subset,2133,"_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3208,integrability,pub,publicly,3208,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2946,interoperability,specif,specify,2946,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3086,interoperability,specif,specify,3086,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:732,modifiability,pac,packages,732,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:903,modifiability,pac,packages,903,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1106,modifiability,pac,packages,1106,".01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1316,modifiability,pac,packages,1316,"sp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1508,modifiability,pac,packages,1508,"n_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1810,modifiability,modul,module,1810," result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1976,modifiability,pac,packages,1976,"red in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2330,modifiability,pac,packages,2330,"eprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2774,modifiability,pac,packages,2774,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:664,performance,error,error,664,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3217,reliability,availab,available,3217,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:343,safety,log,log,343,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:664,safety,error,error,664,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1416,safety,log,log,1416,"genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1437,safety,log,log,1437,"ariable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1613,safety,log,log,1613,". flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1634,safety,log,log,1634,". ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_group",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1783,safety,input,input-,1783,"rflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1810,safety,modul,module,1810," result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2974,safety,input,input,2974,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3114,safety,input,input,3114,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3217,safety,avail,available,3217,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:343,security,log,log,343,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1416,security,log,log,1416,"genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1437,security,log,log,1437,"ariable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1613,security,log,log,1613,". flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1634,security,log,log,1634,". ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_group",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3217,security,availab,available,3217,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:343,testability,log,log,343,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1416,testability,log,log,1416,"genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1437,testability,log,log,1437,"ariable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1613,testability,log,log,1613,". flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1634,testability,log,log,1634,". ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_group",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1739,testability,Trace,Traceback,1739,"py/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:5,usability,experien,experiencing,5,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:664,usability,error,error,664,"I am experiencing a similar issue with a dataset I am using. This runs fine:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.filter_genes_dispersion(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor='seurat',. log = True). ```. But this:. ```. variable_genes_min_mean = 0.01. variable_genes_max_mean = 5. variable_genes_min_disp = 0.5. sc.pp.highly_variable_genes(adata_gex, . min_mean=variable_genes_min_mean, . max_mean=variable_genes_max_mean, . min_disp=variable_genes_min_disp,. flavor = 'seurat') . ```. Throws the following error: . ```. /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scipy/sparse/data.py:135: RuntimeWarning: overflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preproce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:1783,usability,input,input-,1783,"rflow encountered in expm1. result = op(self._deduped_data()). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract. var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: overflow encountered in log. dispersion = np.log(dispersion). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85: RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:2974,usability,input,input,2974,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3114,usability,input,input,3114,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:3553,usability,user,user-images,3553,"n). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-71-69d6424effb2> in <module>. 3 max_mean=variable_genes_max_mean,. 4 min_disp=variable_genes_min_disp,. ----> 5 flavor = 'seurat') . /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 255 n_top_genes=n_top_genes,. 256 n_bins=n_bins,. --> 257 flavor=flavor). 258 else:. 259 sanitize_anndata(adata). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 90 df['dispersions'] = dispersion. 91 if flavor == 'seurat':. ---> 92 df['mean_bin'] = pd.cut(df['means'], bins=n_bins). 93 disp_grouped = df.groupby('mean_bin')['dispersions']. 94 disp_mean_bin = disp_grouped.mean(). /usr/local/anaconda3/envs/pySCENIC/lib/python3.6/site-packages/pandas/core/reshape/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates). 226 # GH 24314. 227 raise ValueError(. --> 228 ""cannot specify integer `bins` when input data contains infinity"". 229 ). 230 elif mn == mx: # adjust end points before binning. ValueError: cannot specify integer `bins` when input data contains infinity. ```. I am assuming its something wrong with the dataset (it's a publicly available one which I needed to convert from a Seurat Object), but I can't figure out what. . I have checked if there are any Inf values included in adata.X or adata.raw.X but there are not. Also both adata.X and adata.raw.X are sparse matrices. Any ideas would be greatly appreciated. . ![Screen Shot 2020-03-13 at 6 09 35 PM](https://user-images.githubusercontent.com/15019107/76643678-d6e24500-6555-11ea-88c0-c16f097432e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:120,deployability,log,logarithmized,120,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:146,deployability,log,logarithmized,146,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:37,performance,time,time,37,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:120,safety,log,logarithmized,120,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:146,safety,log,logarithmized,146,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:120,security,log,logarithmized,120,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:146,security,log,logarithmized,146,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:120,testability,log,logarithmized,120,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:146,testability,log,logarithmized,146,"Hi! . Sorry, I don't really have the time to get into this atm, but I have an idea... I think the default for expecting logarithmized data vs non-logarithmized data changed between the two functions for the `method='seurat'` case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:159,performance,disk,disk,159,"I am experiencing the same problem, and it also comes from a Seurat object that I converted to anndata with [SeuratDisk](https://github.com/mojaveazure/seurat-disk).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:5,usability,experien,experiencing,5,"I am experiencing the same problem, and it also comes from a Seurat object that I converted to anndata with [SeuratDisk](https://github.com/mojaveazure/seurat-disk).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:22,availability,error,error,22,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:252,availability,slo,slot,252,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:74,deployability,log,log,74,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:95,deployability,log,log,95,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:222,deployability,log,log,222,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:226,deployability,scale,scale,226,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:299,deployability,Updat,Update,299,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:338,deployability,contain,contains,338,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:424,deployability,updat,updating,424,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:226,energy efficiency,scale,scale,226,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:226,modifiability,scal,scale,226,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:22,performance,error,error,22,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:226,performance,scale,scale,226,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:252,reliability,slo,slot,252,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:22,safety,error,error,22,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:74,safety,log,log,74,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:95,safety,log,log,95,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:222,safety,log,log,222,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:299,safety,Updat,Update,299,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:424,safety,updat,updating,424,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:74,security,log,log,74,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:95,security,log,log,95,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:222,security,log,log,222,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:299,security,Updat,Update,299,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:424,security,updat,updating,424,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:74,testability,log,log,74,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:95,testability,log,log,95,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:222,testability,log,log,222,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:22,usability,error,error,22,"I am also getting the error `RuntimeWarning: invalid value encountered in log. dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas? Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:115,availability,error,error,115,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:318,availability,down,downstream,318,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:7,deployability,Updat,Updating,7,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:227,deployability,scale,scale,227,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:227,energy efficiency,scale,scale,227,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:227,modifiability,scal,scale,227,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:115,performance,error,error,115,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:227,performance,scale,scale,227,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:295,reliability,doe,doesn,295,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:7,safety,Updat,Updating,7,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:115,safety,error,error,115,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:7,security,Updat,Updating,7,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:115,usability,error,error,115,"FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. . I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:16,deployability,updat,update,16,"Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:16,safety,updat,update,16,"Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:16,security,updat,update,16,"Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:315,availability,error,error,315,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:394,availability,error,error,394,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:649,availability,error,error,649,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:94,deployability,log,log,94,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:50,integrability,transform,transformed,50,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:99,integrability,Sub,Subsequent,99,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:338,integrability,Transform,Transformation,338,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:50,interoperability,transform,transformed,50,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:338,interoperability,Transform,Transformation,338,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:315,performance,error,error,315,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:394,performance,error,error,394,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:649,performance,error,error,649,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:94,safety,log,log,94,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:315,safety,error,error,315,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:394,safety,error,error,394,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:649,safety,error,error,649,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:94,security,log,log,94,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:94,testability,log,log,94,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:703,testability,understand,understand,703,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:315,usability,error,error,315,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:394,usability,error,error,394,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:649,usability,error,error,649,"I have an AnnData object whose .X matrix has been transformed by size factor division, +1 and log. Subsequent ```sc.pp.highly_variable_genes(dataset, flavor='cell_ranger', n_top_genes=1000)``` yields the ```ValueError: Bin edges must be unique: ... You can drop duplicate edges by setting the 'duplicates' kwarg``` error discussed above. Transformation to a sparse matrix did not alleviate the error, and neither did any other solutions suggested. Edit: **However!** While I could not get ```flavor='cell_ranger'``` to work on the data I normalised myself, ```flavor='seurat'``` has worked okay. Therefore, I recommend people also encountering this error to stick with this second flavour, because as I understand it they utilise a similar methodology. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:187,availability,error,error,187,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:26,integrability,filter,filtering,26,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:272,integrability,batch,batch,272,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:187,performance,error,error,187,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:272,performance,batch,batch,272,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:187,safety,error,error,187,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:187,usability,error,error,187,"For me this was solved by filtering out genes that were not expressed in any cell! `sc.pp.filter_genes(adata, min_cells=1)`. If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:66,usability,help,helpful,66,"@LisaSikkema, could you please open a new issue for that? It'd be helpful if you could include a reproducible example as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:18,deployability,updat,update,18,"> Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :). Hello, Massonix, was the problem resolved?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:18,safety,updat,update,18,"> Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :). Hello, Massonix, was the problem resolved?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:18,security,updat,update,18,"> Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :). Hello, Massonix, was the problem resolved?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:117,availability,error,error,117,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:324,availability,down,downstream,324,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:499,availability,cluster,clustering,499,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:9,deployability,Updat,Updating,9,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:233,deployability,scale,scale,233,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:499,deployability,cluster,clustering,499,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:233,energy efficiency,scale,scale,233,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:420,interoperability,format,format,420,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:233,modifiability,scal,scale,233,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:474,modifiability,scal,scaling,474,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:488,modifiability,variab,variables,488,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:557,modifiability,variab,variable,557,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:117,performance,error,error,117,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:233,performance,scale,scale,233,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:301,reliability,doe,doesn,301,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:9,safety,Updat,Updating,9,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:117,safety,error,error,117,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:9,security,Updat,Updating,9,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:117,usability,error,error,117,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:588,usability,help,help,588,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error. > . > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/392:249,availability,fault,fault,249,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:14,deployability,modul,modules,14,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:249,energy efficiency,fault,fault,249,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:14,modifiability,modul,modules,14,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:249,performance,fault,fault,249,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:249,reliability,fault,fault,249,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:14,safety,modul,modules,14,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:249,safety,fault,fault,249,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:129,testability,plan,planned,129,"Both of these modules are not in the docs and not referenced in any tutorial and I never considered them mature code... I always planned on fixing these... but my bandwidth for this is limited... I should not have merged them into master, that's my fault... Won't happen again...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:184,deployability,api,api,184,"Oh, there was a PR? I missed that. I thought it was just committed directly to master. I think its even OK to merge in unstable functions as long as we dont reexport them in `scanpy.api`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:184,integrability,api,api,184,"Oh, there was a PR? I missed that. I thought it was just committed directly to master. I think its even OK to merge in unstable functions as long as we dont reexport them in `scanpy.api`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/392:184,interoperability,api,api,184,"Oh, there was a PR? I missed that. I thought it was just committed directly to master. I think its even OK to merge in unstable functions as long as we dont reexport them in `scanpy.api`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/392
https://github.com/scverse/scanpy/issues/393:308,usability,learn,learn,308,"For pca of sparse matrices i think it should work this way. 1) Create class for lazy evaluation of X - mean, i.e store X, mean separately and implement multiplication by some dense B as `sparse.csr_matrix.dot(X, B) - mean.dot(B)`. 2) Pass instance of this class to [randomized_svd](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L233)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:321,usability,learn,learn,321,"For pca of sparse matrices i think it should work this way. 1) Create class for lazy evaluation of X - mean, i.e store X, mean separately and implement multiplication by some dense B as `sparse.csr_matrix.dot(X, B) - mean.dot(B)`. 2) Pass instance of this class to [randomized_svd](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L233)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:33,availability,state,stated,33,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:33,integrability,state,stated,33,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:714,integrability,compon,component,714,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:782,integrability,compon,component,782,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:901,integrability,event,eventually,901,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:714,interoperability,compon,component,714,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:782,interoperability,compon,component,782,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:714,modifiability,compon,component,714,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:782,modifiability,compon,component,782,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:928,performance,memor,memory,928,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:1244,safety,compl,completely,1244,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:1244,security,compl,completely,1244,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:298,usability,behavi,behavior,298,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:463,usability,custom,custom,463,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:587,usability,efficien,efficiently,587,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:928,usability,memor,memory,928,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:59,availability,state,state,59,"Ah, just saw that the docs are and have been correct. They state that the default is `True`. I'm just still not used to the fact that the default value is not displayed right next to the parameter value but only in the function header...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:59,integrability,state,state,59,"Ah, just saw that the docs are and have been correct. They state that the default is `True`. I'm just still not used to the fact that the default value is not displayed right next to the parameter value but only in the function header...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:187,modifiability,paramet,parameter,187,"Ah, just saw that the docs are and have been correct. They state that the default is `True`. I'm just still not used to the fact that the default value is not displayed right next to the parameter value but only in the function header...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:354,deployability,version,version,354,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:499,deployability,observ,observation,499,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:354,integrability,version,version,354,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:364,integrability,coupl,couple,364,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:354,modifiability,version,version,354,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:364,modifiability,coupl,couple,364,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:278,reliability,doe,does,278,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:132,testability,simpl,simple,132,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:364,testability,coupl,couple,364,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:499,testability,observ,observation,499,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:132,usability,simpl,simple,132,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:428,availability,fault,fault,428,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:382,deployability,releas,release,382,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:428,energy efficiency,fault,fault,428,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:428,performance,fault,fault,428,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:428,reliability,fault,fault,428,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:428,safety,fault,fault,428,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:459,safety,test,test,459,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:510,safety,test,test,510,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:459,testability,test,test,459,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:510,testability,test,test,510,"The original line was. ```. zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True. ```. which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:12,safety,reme,remember,12,"Can't say i remember, but i don't see any reason for me to edit this block when i was adding the chunked computation... Yes, i will.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:135,deployability,releas,release,135,I fixed the bug: https://github.com/theislab/scanpy/commit/15593d532fbaa696bf1ea328d1991d31b334e175. . And I'll immediately make a new release and put a warning on the webpage... @Koncopd: Thank you for adding the tests!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:214,safety,test,tests,214,I fixed the bug: https://github.com/theislab/scanpy/commit/15593d532fbaa696bf1ea328d1991d31b334e175. . And I'll immediately make a new release and put a warning on the webpage... @Koncopd: Thank you for adding the tests!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:214,testability,test,tests,214,I fixed the bug: https://github.com/theislab/scanpy/commit/15593d532fbaa696bf1ea328d1991d31b334e175. . And I'll immediately make a new release and put a warning on the webpage... @Koncopd: Thank you for adding the tests!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:10,deployability,releas,release,10,Done with release 1.3.6 and the warning: https://github.com/theislab/scanpy/commit/35030d28bb4e1e4559449bfe41238523bee0e616,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:248,interoperability,standard,standard,248,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:234,modifiability,inherit,inherits,234,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:69,usability,learn,learn,69,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:82,usability,learn,learn,82,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:57,testability,simpl,simply,57,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to . ```. mu = M.mean(1).A1 if issparse(M) else M.mean(1). B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:57,usability,simpl,simply,57,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to . ```. mu = M.mean(1).A1 if issparse(M) else M.mean(1). B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:107,usability,learn,learn,107,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to . ```. mu = M.mean(1).A1 if issparse(M) else M.mean(1). B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:120,usability,learn,learn,120,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to . ```. mu = M.mean(1).A1 if issparse(M) else M.mean(1). B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:114,usability,learn,learn,114,"@VolkerBergen . No, similar lines also should be changed in [`randomized_range_finder`](https://github.com/scikit-learn/scikit-learn/blob/3a884c5ee507f735e2df384727340c72c5219a8e/sklearn/utils/extmath.py#L148), which is used by `randomized_svd` function. Or the whole `safe_sparse_dot` function. But copying the file extmath.py (or the part of it related to svd) and changing this lines would be sufficient, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:127,usability,learn,learn,127,"@VolkerBergen . No, similar lines also should be changed in [`randomized_range_finder`](https://github.com/scikit-learn/scikit-learn/blob/3a884c5ee507f735e2df384727340c72c5219a8e/sklearn/utils/extmath.py#L148), which is used by `randomized_svd` function. Or the whole `safe_sparse_dot` function. But copying the file extmath.py (or the part of it related to svd) and changing this lines would be sufficient, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:375,safety,test,test,375,"> which did the expected thing, @flying-sheep introduced the bug 22 days ago in ce10d02. damn, the only thing I could have done wrong there. It went into that commit because the previous code was too convoluted to understand, and I needed to understand that line to improve the docs! I ended up understanding it it but rewrote the line incorrectly. Im sorry! Did you add a test after 15593d5?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:215,testability,understand,understand,215,"> which did the expected thing, @flying-sheep introduced the bug 22 days ago in ce10d02. damn, the only thing I could have done wrong there. It went into that commit because the previous code was too convoluted to understand, and I needed to understand that line to improve the docs! I ended up understanding it it but rewrote the line incorrectly. Im sorry! Did you add a test after 15593d5?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:243,testability,understand,understand,243,"> which did the expected thing, @flying-sheep introduced the bug 22 days ago in ce10d02. damn, the only thing I could have done wrong there. It went into that commit because the previous code was too convoluted to understand, and I needed to understand that line to improve the docs! I ended up understanding it it but rewrote the line incorrectly. Im sorry! Did you add a test after 15593d5?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:296,testability,understand,understanding,296,"> which did the expected thing, @flying-sheep introduced the bug 22 days ago in ce10d02. damn, the only thing I could have done wrong there. It went into that commit because the previous code was too convoluted to understand, and I needed to understand that line to improve the docs! I ended up understanding it it but rewrote the line incorrectly. Im sorry! Did you add a test after 15593d5?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:375,testability,test,test,375,"> which did the expected thing, @flying-sheep introduced the bug 22 days ago in ce10d02. damn, the only thing I could have done wrong there. It went into that commit because the previous code was too convoluted to understand, and I needed to understand that line to improve the docs! I ended up understanding it it but rewrote the line incorrectly. Im sorry! Did you add a test after 15593d5?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:47,energy efficiency,power,power,47,"@VolkerBergen . Also appropriately implemented power method (last section of [this](http://www.cs.yale.edu/homes/el327/datamining2013aFiles/07_singular_value_decomposition.pdf), for example) for svd should be fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:32,availability,stead,steadily,32,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:160,deployability,updat,updated,160,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:174,deployability,version,version,174,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:174,integrability,version,version,174,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:174,modifiability,version,version,174,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:50,safety,test,test,50,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:160,safety,updat,updated,160,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:160,security,updat,updated,160,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:50,testability,test,test,50,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:55,testability,coverag,coverage,55,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:0,usability,Close,Closed,0,Closed by #1066,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/395:33,usability,close,close,33,"Perfect, that worked great! I'll close the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/395
https://github.com/scverse/scanpy/issues/396:218,deployability,scale,scale,218,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:538,deployability,automat,automatically,538,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:208,energy efficiency,frequenc,frequency,208,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:218,energy efficiency,scale,scale,218,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:406,integrability,sub,subpopulations,406,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:628,integrability,sub,subscribed,628,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:218,modifiability,scal,scale,218,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:527,modifiability,scal,scaling,527,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:218,performance,scale,scale,218,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:517,reliability,doe,does,517,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:825,security,auth,auth,825,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:538,testability,automat,automatically,538,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:18,usability,visual,visual,18,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:352,usability,visual,visualize,352,"Can you give me a visual example of what you want to see if I am following. you? On Thu, Dec 13, 2018 at 7:37 PM a-munoz-rojas <notifications@github.com>. wrote:. > It would be great to be able to adjust the frequency scale of the dot size. > to the min-max values of the data set, instead of 0-100% of total cells. > This is particularly important to visualize expression of genes that is. > rare in some subpopulations, that are otherwise impossible to see because. > the dots are too small. As a side note, Seurat does this scaling. > automatically and it works well. Thanks! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1QVWEqACJPiFTn47AuRIvmWwHAeKks5u4p6BgaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:70,deployability,scale,scale,70,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:163,deployability,scale,scaled,163,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:403,deployability,automat,automatically,403,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:417,deployability,scale,scales,417,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:70,energy efficiency,scale,scale,70,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:163,energy efficiency,scale,scaled,163,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:417,energy efficiency,scale,scales,417,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:70,modifiability,scal,scale,70,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:163,modifiability,scal,scaled,163,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:417,modifiability,scal,scales,417,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:70,performance,scale,scale,70,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:163,performance,scale,scaled,163,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:417,performance,scale,scales,417,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:403,testability,automat,automatically,403,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:244,usability,user,user-images,244,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:491,usability,user,user-images,491,"Sure - this is an output from a Seurat analysis. You can see that the scale of the dot size goes from 10% to 60%, such that the group with 60% expressing cells is scaled to the max dot size:. <img width=""246"" alt=""pastedgraphic-3"" src=""https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png"">. As a comparison, this is a (different) output from scanpy that automatically scales to 100% and causes the dots to be too small:. ![dotplotex](https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png). Please let me know if this makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:493,deployability,scale,scale,493,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:589,deployability,scale,scaled,589,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:826,deployability,automat,automatically,826,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:840,deployability,scale,scales,840,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:493,energy efficiency,scale,scale,493,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:589,energy efficiency,scale,scaled,589,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:840,energy efficiency,scale,scales,840,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:41,interoperability,specif,specify,41,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:143,modifiability,paramet,parameter,143,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:493,modifiability,scal,scale,493,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:589,modifiability,scal,scaled,589,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:840,modifiability,scal,scales,840,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:493,performance,scale,scale,493,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:589,performance,scale,scaled,589,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:840,performance,scale,scales,840,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:1318,security,auth,auth,1318,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:328,testability,simpl,simple,328,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:826,testability,automat,automatically,826,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:24,usability,clear,clearer,24,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:328,usability,simpl,simple,328,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:660,usability,user,user-images,660,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:926,usability,user,user-images,926,"Thanks, now is becoming clearer. Can you specify the maximum percentage. value or this is taken from the data? What would be a nice name for a parameter to add to the function? `max_fraction` ? I would not be able to work on this for the next days but at least we can. throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>. wrote:. > Sure - this is an output from a Seurat analysis. You can see that the. > scale of the dot size goes from 10% to 60%, such that the group with 60%. > expressing cells is scaled to the max dot size:. >. > [image: pastedgraphic-3]. > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>. >. > As a comparison, this is a (different) output from scanpy that. > automatically scales to 100% and causes the dots to be too small:. > [image: dotplotex]. > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>. >. > Please let me know if this makes sense! >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:32,deployability,automat,automatically,32,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:127,deployability,scale,scale,127,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:379,deployability,scale,scale,379,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:385,deployability,automat,automatically,385,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:594,deployability,scale,scale,594,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:127,energy efficiency,scale,scale,127,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:176,energy efficiency,heat,heatmaps,176,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:379,energy efficiency,scale,scale,379,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:594,energy efficiency,scale,scale,594,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:513,interoperability,specif,specify,513,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:521,interoperability,specif,specific,521,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:100,modifiability,paramet,parameters,100,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:127,modifiability,scal,scale,127,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:262,modifiability,paramet,parameter,262,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:379,modifiability,scal,scale,379,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:594,modifiability,scal,scale,594,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:127,performance,scale,scale,127,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:379,performance,scale,scale,379,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:594,performance,scale,scale,594,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:32,testability,automat,automatically,32,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:385,testability,automat,automatically,385,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:122,usability,user,user,122,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:364,usability,behavi,behavior,364,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/396:504,usability,user,user,504,"The maximum percentage value is automatically taken from the data, and the Seruat function also has parameters to let the user scale these values (similar to vmin and vmax for heatmaps). I think it would be nice to have this option in this function as well. For parameter names, a suggestion could be 'dot_min' and 'dot_max'. Maybe it would be best if the default behavior is to scale automatically to the max and min values in the data plotted (default such that dot_min and dot_max=None). This way the user can specify specific values if they want (0% to 100% for example), but otherwise the scale should be proportional to their data. Let me know what you think!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396
https://github.com/scverse/scanpy/issues/397:483,availability,down,down-regulated,483,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:40,modifiability,variab,variable,40,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:123,modifiability,variab,variable,123,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:77,performance,perform,perform,77,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:660,safety,test,test,660,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:660,testability,test,test,660,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:77,usability,perform,perform,77,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:756,usability,tool,tools,756,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```. adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]. sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'). ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:170,safety,test,testing,170,"Hi,. Thank you so much for the prompt response. I was able to make the comparisons following your method. As you suggested, I am going to try using MAST or limma for DEG testing in the future. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:170,testability,test,testing,170,"Hi,. Thank you so much for the prompt response. I was able to make the comparisons following your method. As you suggested, I am going to try using MAST or limma for DEG testing in the future. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:806,availability,state,statement,806,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:653,energy efficiency,power,powerful,653,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:806,integrability,state,statement,806,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:478,performance,perform,performs,478,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:238,safety,test,test,238,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:379,safety,test,test,379,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:754,safety,test,test,754,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1034,safety,detect,detection,1034,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1034,security,detect,detection,1034,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:129,testability,simpl,simple,129,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:238,testability,test,test,238,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:379,testability,test,test,379,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:754,testability,test,test,754,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:129,usability,simpl,simple,129,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:478,usability,perform,performs,478,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1183,availability,state,statement,1183,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:329,deployability,log,log,329,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1039,energy efficiency,model,model,1039,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:224,integrability,batch,batch,224,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:333,integrability,transform,transformed,333,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:383,integrability,batch,batch,383,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1183,integrability,state,statement,1183,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:333,interoperability,transform,transformed,333,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1019,modifiability,variab,variability,1019,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:216,performance,perform,perform,216,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:224,performance,batch,batch,224,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:383,performance,batch,batch,383,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:736,performance,perform,performs,736,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1227,performance,perform,perform,1227,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1478,performance,content,content,1478,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:123,safety,test,tests,123,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:286,safety,test,test,286,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:329,safety,log,log,329,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:406,safety,test,test,406,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:471,safety,compl,complexity,471,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:686,safety,test,testing,686,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:884,safety,test,test,884,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1140,safety,test,tests,1140,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1167,safety,test,tests,1167,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1383,safety,detect,detection,1383,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1734,safety,test,tests,1734,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1761,safety,test,tests,1761,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:329,security,log,log,329,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:471,security,compl,complexity,471,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1039,security,model,model,1039,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1383,security,detect,detection,1383,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:123,testability,test,tests,123,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:286,testability,test,test,286,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:329,testability,log,log,329,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:406,testability,test,test,406,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:587,testability,simpl,simple,587,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:686,testability,test,testing,686,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:884,testability,test,test,884,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1140,testability,test,tests,1140,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1167,testability,test,tests,1167,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1734,testability,test,tests,1734,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1761,testability,test,tests,1761,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:216,usability,perform,perform,216,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:587,usability,simpl,simple,587,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:736,usability,perform,performs,736,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1227,usability,perform,perform,1227,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:192,deployability,stack,stackexchange,192,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:442,deployability,stack,stackexchange,442,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:158,energy efficiency,power,power,158,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:635,energy efficiency,power,power,635,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1001,energy efficiency,model,models,1001,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1051,energy efficiency,model,model,1051,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:365,interoperability,standard,standard,365,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:745,interoperability,distribut,distributional,745,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:513,modifiability,paramet,parametric-test-e-g-wilcoxon-in-small-sampl,513,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1040,performance,performing model,performing model,1040,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:131,safety,test,tests,131,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:171,safety,test,tests,171,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:285,safety,test,test-,285,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:501,safety,test,test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl,501,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:778,safety,test,test,778,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:899,safety,test,tests,899,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1158,safety,test,test,1158,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1001,security,model,models,1001,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1051,security,model,model,1051,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:131,testability,test,tests,131,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:171,testability,test,tests,171,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:285,testability,test,test-,285,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:501,testability,test,test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl,501,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:571,testability,simul,simulations,571,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:778,testability,test,test,778,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:899,testability,test,tests,899,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1158,testability,test,test,1158,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:258,usability,efficien,efficiency-of-the-wilcoxon-test-,258,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:852,usability,Intuit,Intuitively,852,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:1040,usability,perform,performing,1040,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:205,availability,state,state,205,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:461,availability,cluster,cluster,461,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:542,availability,cluster,clusters,542,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:180,deployability,stack,stackexchange,180,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:461,deployability,cluster,cluster,461,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:542,deployability,cluster,clusters,542,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:575,energy efficiency,current,currently,575,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:683,energy efficiency,adapt,adapt,683,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:205,integrability,state,state,205,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:683,integrability,adapt,adapt,683,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:683,interoperability,adapt,adapt,683,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:683,modifiability,adapt,adapt,683,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:19,performance,time,time,19,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:105,safety,test,test,105,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:125,safety,test,test,125,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:145,safety,detect,detection,145,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:588,safety,test,test-overestimate-var,588,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:708,safety,test,test-overestimate-var,708,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:145,security,detect,detection,145,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:105,testability,test,test,105,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:125,testability,test,test,125,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:588,testability,test,test-overestimate-var,588,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:708,testability,test,test-overestimate-var,708,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:132,energy efficiency,adapt,adaption,132,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:132,integrability,adapt,adaption,132,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:132,interoperability,adapt,adaption,132,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:132,modifiability,adapt,adaption,132,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:156,safety,test,test,156,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:156,testability,test,test,156,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:103,performance,time,time,103,It's always a nice feeling when your understanding catches up to someone else's decision making a long time ago... ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:37,testability,understand,understanding,37,It's always a nice feeling when your understanding catches up to someone else's decision making a long time ago... ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:49,modifiability,pac,package,49,Any news when diffxpy will be up on biorxiv? The package looks super interesting and I'd like to learn more about the background to the methods you implemented.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:97,usability,learn,learn,97,Any news when diffxpy will be up on biorxiv? The package looks super interesting and I'd like to learn more about the background to the methods you implemented.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:11,performance,perform,performs,11,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:50,safety,test,tests,50,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:178,safety,test,test,178,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:128,security,ident,ident,128,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:152,security,ident,ident,152,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:50,testability,test,tests,50,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:178,testability,test,test,178,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:11,usability,perform,performs,11,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:. sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:115,energy efficiency,power,powerful,115,"This field is developing very fast, more and more advanced DE test methos are emerging, it's better to adopt these powerful methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:62,safety,test,test,62,"This field is developing very fast, more and more advanced DE test methos are emerging, it's better to adopt these powerful methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:62,testability,test,test,62,"This field is developing very fast, more and more advanced DE test methos are emerging, it's better to adopt these powerful methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:197,deployability,log,log,197,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:201,integrability,transform,transformation,201,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:201,interoperability,transform,transformation,201,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:47,safety,test,testing,47,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:197,safety,log,log,197,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:105,security,sign,significant,105,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:197,security,log,log,197,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:302,security,sign,significance,302,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:47,testability,test,testing,47,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:197,testability,log,log,197,"One of the shortcomings of scanpy's default DE testing is that p-values (or FDR) of a few genes are very significant (equal 0 or approximately 0 in some datasets), then it's impossible to execute -log transformation, even there is only one 0. The volcano plot will be not beautiful because of the high significance. @falexwolf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:131,usability,workflow,workflow,131,"@davidsebfischer is developing https://github.com/theislab/diffxpy, which is a great way to do differential expression in a scanpy workflow. We should document this! Do you think the p-values are inflated? If so, how could we propagate uncertainty to them?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:151,usability,document,document,151,"@davidsebfischer is developing https://github.com/theislab/diffxpy, which is a great way to do differential expression in a scanpy workflow. We should document this! Do you think the p-values are inflated? If so, how could we propagate uncertainty to them?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:114,safety,test,testing,114,Thanks all for the interesting discussion- did a consensus emerge on the 'best' way to do differential expression testing in scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:114,testability,test,testing,114,Thanks all for the interesting discussion- did a consensus emerge on the 'best' way to do differential expression testing in scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:184,availability,state,state,184,"We typically use `diffxpy` for this. I would check out that [repo](https://github.com/theislab/diffxpy). It's fast, and quite versatile. Documentation is also getting to a pretty good state I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:184,integrability,state,state,184,"We typically use `diffxpy` for this. I would check out that [repo](https://github.com/theislab/diffxpy). It's fast, and quite versatile. Documentation is also getting to a pretty good state I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:137,usability,Document,Documentation,137,"We typically use `diffxpy` for this. I would check out that [repo](https://github.com/theislab/diffxpy). It's fast, and quite versatile. Documentation is also getting to a pretty good state I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:141,energy efficiency,model,models,141,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:207,energy efficiency,model,model,207,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:267,safety,test,tests,267,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:280,safety,test,tests,280,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:141,security,model,models,141,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:207,security,model,model,207,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:192,testability,understand,understand,192,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:267,testability,test,tests,267,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:280,testability,test,tests,280,"Thank you @LuckyMD. Naive question, but what is the advantage of `diffxpy` over `sc.tl.rank_genes_groups`? I read comments above about noise models and technical covariates, but I don't fully understand the model fitting aspect and both methods seem to offer similar tests like T-tests and Wilcoxon rank-sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:69,deployability,version,version,69,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:198,energy efficiency,model,models,198,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:346,energy efficiency,model,model,346,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:421,energy efficiency,model,model,421,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:69,integrability,version,version,69,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:69,modifiability,version,version,69,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:187,modifiability,paramet,parametric,187,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:44,safety,test,test,44,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:108,safety,test,test,108,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:120,safety,test,tests,120,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:248,safety,test,test,248,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:323,safety,test,test,323,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:198,security,model,models,198,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:346,security,model,model,346,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:421,security,model,model,421,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:44,testability,test,test,44,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:108,testability,test,test,108,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:120,testability,test,tests,120,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:248,testability,test,test,248,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/issues/397:323,testability,test,test,323,"`sc.tl.rank_genes_groups` offers only the T-test (including a second version of this) and Wilcoxon rank-sum test. These tests are also in `diffxpy`, but there are fare more sophisticated parametric models which you can (and probably should) use to test for differential expression in different setups. Ideally, to run a DE test you would want to model raw count data as a negative binomial and then add covariates to the model (like size factors, sample, condition, etc.). This is only possible in `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397
https://github.com/scverse/scanpy/pull/398:101,deployability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,integrability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:778,integrability,batch,batch,778,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:891,integrability,sub,subscribed,891,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,interoperability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,modifiability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:778,performance,batch,batch,778,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,reliability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,security,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1111,security,auth,auth,1111,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:101,testability,integr,integrated,101,"I second this initiative. I had used the code from Brent and works quite. well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>. wrote:. > *@Marius1311* commented on this pull request. > ------------------------------. >. > In scanpy/preprocessing/combat.py. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:. >. > > @@ -0,0 +1,161 @@. > +import numpy as np. > +from scipy.sparse import issparse. > +import pandas as pd. > +import sys. > +from numpy import linalg as la. > +import patsy. > +. > +def design_mat(mod, batch_levels):. > + # require levels to make sure they are in the same order as we use in the. > + # rest of the script. > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),. >. > thanks, did that! >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,deployability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,integrability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,interoperability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,modifiability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,reliability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:195,safety,test,tests,195,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,security,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:173,testability,integr,integration,173,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:190,testability,unit,unit,190,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:195,testability,test,tests,195,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:30,usability,feedback,feedback,30,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:135,security,auth,author,135,A very happy new year! Your improvements look great! I see one small issue that I'll fix myself and I'll merge this and add you to the author list. It's a very nice comprehensive contribution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:86,deployability,modul,module,86,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1926,deployability,contain,contain,1926,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2000,deployability,contain,contain,2000,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:62,energy efficiency,adapt,adapting,62,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:331,energy efficiency,model,model,331,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:409,energy efficiency,model,model,409,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:475,energy efficiency,model,model,475,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:835,energy efficiency,Model,ModelDesc,835,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:62,integrability,adapt,adapting,62,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:62,interoperability,adapt,adapting,62,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:62,modifiability,adapt,adapting,62,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:86,modifiability,modul,module,86,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:547,modifiability,pac,packages,547,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:656,modifiability,pac,packages,656,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:765,modifiability,pac,packages,765,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:912,modifiability,pac,packages,912,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1034,modifiability,pac,packages,1034," the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1150,modifiability,pac,packages,1150," on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.genera",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1269,modifiability,pac,packages,1269,"mbat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokeniz",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1407,modifiability,pac,packages,1407,"del, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1545,modifiability,pac,packages,1545,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2442,modifiability,pac,packages,2442,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:86,safety,modul,module,86,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:174,safety,test,tests,174,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2068,safety,avoid,avoid,2068,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2078,safety,compl,complications,2078,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:331,security,model,model,331,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:409,security,model,model,409,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:475,security,model,model,475,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:835,security,Model,ModelDesc,835,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:1206,security,token,token,1206,"ver it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2078,security,compl,complications,2078,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2139,security,token,tokenize,2139,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2266,security,token,tokenize,2266,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2353,security,token,tokenize,2353,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2366,security,token,tokenize,2366,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2457,security,token,tokens,2457,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:174,testability,test,tests,174,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2331,testability,assert,assert,2331,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2387,testability,Assert,AssertionError,2387,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:2471,testability,Assert,AssertionError,2471,"ype=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully contain newlines, we'll just remove all. # the newlines up front to avoid any complications:. code = code.replace(""\n"", "" "").strip(). it = tokenize.generate_tokens(StringIO(code).readline). try:. for (pytype, string, (_, start), (_, end), code) in it:. if pytype == tokenize.ENDMARKER:. break. origin = Origin(code, start, end). > assert pytype not in (tokenize.NL, tokenize.NEWLINE). E AssertionError. ../../../miniconda3/lib/python3.6/site-packages/patsy/tokens.py:35: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:43,usability,command,command,43,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:. ```. preprocessing/_combat.py:150: in combat. s_data, design, var_pooled, stand_mean = stand_data(model, data). preprocessing/_combat.py:78: in stand_data. design = design_mat(model, batch_levels). preprocessing/_combat.py:32: in design_mat. model, return_type=""dataframe""). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix. NA_action, return_type). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design. NA_action). ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders. formula_like = ModelDesc.from_formula(formula_like). ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula. tree = parse_formula(tree_or_string). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula. _atomic_token_types). ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse. for token in token_source:. ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula. yield _read_python_expr(it, end_tokens). ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr. for pytype, token_string, origin in it:. ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next. return six.advance_iterator(self._it). _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):. # Since formulas can only contain Python expressions, and Python. # expressions cannot meaningfully ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:10,integrability,sub,substantial,10,This is a substantial contribution and I added Marius as author: https://github.com/theislab/scanpy/commit/5557338b5947f7e7317f1294151eee0c232eccdf :smile:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:57,security,auth,author,57,This is a substantial contribution and I added Marius as author: https://github.com/theislab/scanpy/commit/5557338b5947f7e7317f1294151eee0c232eccdf :smile:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:100,availability,error,error,100,"Thanks Alex! That's great, thanks also for adding me to the authors list. I haven't seen that patsy error on my Ubuntu machine either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:100,performance,error,error,100,"Thanks Alex! That's great, thanks also for adding me to the authors list. I haven't seen that patsy error on my Ubuntu machine either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:100,safety,error,error,100,"Thanks Alex! That's great, thanks also for adding me to the authors list. I haven't seen that patsy error on my Ubuntu machine either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:60,security,auth,authors,60,"Thanks Alex! That's great, thanks also for adding me to the authors list. I haven't seen that patsy error on my Ubuntu machine either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:100,usability,error,error,100,"Thanks Alex! That's great, thanks also for adding me to the authors list. I haven't seen that patsy error on my Ubuntu machine either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/issues/399:154,availability,recov,recover,154,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:397,availability,restor,restored,397,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:154,deployability,recov,recover,154,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:1046,interoperability,specif,specified,1046,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:957,modifiability,paramet,parameters,957,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:154,reliability,recov,recover,154,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:397,reliability,restor,restored,397,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:154,safety,recov,recover,154,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:390,safety,safe,safely,390,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:154,security,recov,recover,154,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:350,security,secur,security,350,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:5,usability,document,documentation,5,"[The documentation for `AnnData.write_csvs`](https://anndata.readthedocs.io/en/latest/anndata.AnnData.write_csvs.html) tells you. > It is not possible to recover the full AnnData from the output of this function. Use write() for this. Sorry for that! We thought that not having a function to read back those CSVs, we wont lull people into the false security that the AnnData object can be safely restored from CSVs. But if you have nothing else but those files, you can of course try to use [`pandas.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to read the `.obs` and `.var` dataframes and do something like. ```py. adata = AnnData(. pd.read_csv('output/X.csv').asarray(),. pd.read_csv('output/obs.csv'),. pd.read_csv('output/var.csv'),. { # adata.uns. 'some_thing': pd.read_csv('output/some_thing.csv'),. },. pd.read_csv('output/obsm.csv'),. pd.read_csv('output/varm.csv'),. ). ```. You might have to fiddle with parameters to `pandas.read_csv`, like `index_col`, and obsm/varm might not be able to be specified as data frames.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:29,availability,cluster,cluster,29,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks! ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:29,deployability,cluster,cluster,29,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks! ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:95,energy efficiency,load,load,95,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks! ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:95,performance,load,load,95,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks! ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/issues/399:271,usability,user,user-images,271,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks! ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399
https://github.com/scverse/scanpy/pull/401:329,integrability,event,event-,329,"Great! Thanks! On Wed, Dec 26, 2018 at 9:58 PM Alex Wolf <notifications@github.com> wrote:. > Merged #401 <https://github.com/theislab/scanpy/pull/401> into master. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/401#event-2045028852>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UlJw0blxwAqzG-LhWTzEXKJcnf6ks5u8-MAgaJpZM4Zaewc>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/401
https://github.com/scverse/scanpy/pull/401:210,security,auth,authored,210,"Great! Thanks! On Wed, Dec 26, 2018 at 9:58 PM Alex Wolf <notifications@github.com> wrote:. > Merged #401 <https://github.com/theislab/scanpy/pull/401> into master. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/401#event-2045028852>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UlJw0blxwAqzG-LhWTzEXKJcnf6ks5u8-MAgaJpZM4Zaewc>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/401
https://github.com/scverse/scanpy/pull/401:419,security,auth,auth,419,"Great! Thanks! On Wed, Dec 26, 2018 at 9:58 PM Alex Wolf <notifications@github.com> wrote:. > Merged #401 <https://github.com/theislab/scanpy/pull/401> into master. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/401#event-2045028852>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UlJw0blxwAqzG-LhWTzEXKJcnf6ks5u8-MAgaJpZM4Zaewc>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/401
https://github.com/scverse/scanpy/pull/403:169,interoperability,compatib,compatibility,169,"This is nice! Thank you! It appears to me that the benchmarks show that this only becomes relevant for *very* large data. So we need to be mindful to not break backward compatibility for all the small and medium-size datasets that people use (which we do by introducing the tiny difference). Don't you think that in the light of this, it would be better to leave the default as is (densifying) and have an option `sparse_pca` or something similar?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:157,performance,time,times,157,">It appears to me that the benchmarks show that this only becomes relevant for very large data. Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:354,availability,avail,available,354,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:241,deployability,resourc,resources,241,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:241,energy efficiency,resourc,resources,241,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:63,performance,time,times,63,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:227,performance,computational resourc,computational resources,227,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:354,reliability,availab,available,354,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:241,safety,resourc,resources,241,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:354,safety,avail,available,354,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:354,security,availab,available,354,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:241,testability,resourc,resources,241,"> Hm, even for my example it is 77.14 MiB vs 893.92 MiB, so 10 times difference. This seems large to me, no? Yes, it's definitely large and it's awesome that you solved this problem! I just meant that it's not hitting people's computational resources limits: your example is 60K x 2K, so quite big already, if you densify you need 800MB, which is easily available even on a laptop. That's what I meant. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:22,safety,test,test,22,Not sure what kind of test to add for this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:22,testability,test,test,22,Not sure what kind of test to add for this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,deployability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,integrability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,interoperability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,modifiability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,reliability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,security,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:35,testability,integr,integrate,35,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:62,usability,learn,learn,62,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:74,usability,learn,learn,74,Similar pull request exists already in sklearn. https://github.com/scikit-learn/scikit-learn/pull/12841. Will watch.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:87,usability,learn,learn,87,Similar pull request exists already in sklearn. https://github.com/scikit-learn/scikit-learn/pull/12841. Will watch.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:82,usability,learn,learn,82,"Hm, it was decided to suspend this pr earlier. There is an analogous pr in scikit-learn, but i'm not sure it will got forward. I'm not sure what to do with this pr...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:19,availability,state,state,19,What's the current state here? This just came out . https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1900-3. favoring sklearn's PCA implementation while stating that it cannot yet handle sparse matrices. Is that still true?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:11,energy efficiency,current,current,11,What's the current state here? This just came out . https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1900-3. favoring sklearn's PCA implementation while stating that it cannot yet handle sparse matrices. Is that still true?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:19,integrability,state,state,19,What's the current state here? This just came out . https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1900-3. favoring sklearn's PCA implementation while stating that it cannot yet handle sparse matrices. Is that still true?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:5,performance,time,time,5,"Last time i checked it was true. Need to check again, but i bet it is still the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:179,usability,learn,learn,179,"That's still the case (at least for randomized PCA @Koncopd linked above), though it looks like there may be a another path forward using other solvers: https://github.com/scikit-learn/scikit-learn/issues/12794. Still needs an implementation though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:192,usability,learn,learn,192,"That's still the case (at least for randomized PCA @Koncopd linked above), though it looks like there may be a another path forward using other solvers: https://github.com/scikit-learn/scikit-learn/issues/12794. Still needs an implementation though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:397,availability,operat,operator,397,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:458,availability,operat,operator,458,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:534,availability,operat,operator,534,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:593,availability,operat,operator,593,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:674,availability,operat,operator,674,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1536,availability,slo,slower,1536,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1587,availability,slo,slower,1587,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:217,integrability,compon,components,217,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:789,integrability,sub,subtract,789,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1349,integrability,compon,components,1349,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:217,interoperability,compon,components,217,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1349,interoperability,compon,components,1349,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:217,modifiability,compon,components,217,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1349,modifiability,compon,components,1349,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1627,performance,memor,memory,1627,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1536,reliability,slo,slower,1536,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1587,reliability,slo,slower,1587,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:748,security,modif,modify,748,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:100,usability,learn,learn,100,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1627,usability,memor,memory,1627,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:1634,usability,efficien,efficient,1634,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```. def sparse_pca(X,npcs,mu = None):. # X -- scipy sparse data matrix. # npcs -- number of principal components. # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features. if mu is None: . mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means. mmat = mdot = mu.dot . # dot product operator for the transposed means. mhmat = mhdot = mu.T.dot . # dot product operator for the data. Xmat = Xdot = X.dot . # dot product operator for the transposed data. XHmat = XHdot = X.T.conj().dot . # dot product operator for a vector of ones. ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means. def matvec(x): . return Xdot(x) - mdot(x). def matmat(x): . return Xmat(x) - mmat(x). def rmatvec(x): . return XHdot(x) - mhdot(ones(x)). def rmatmat(x): . return XHmat(x) - mhmat(ones(x)). . # construct the LinearOperator. XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,. matmat = matmat,. shape = X.shape,. rmatvec = rmatvec, rmatmat = rmatmat). . u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs). . # i like my eigenvalues sorted in decreasing order. idx = np.argsort(-s). S = np.diag(s[idx]). # principal components. pcs = u[:,idx].dot(S) . # equivalent to PCA.components_ in sklearn . components_ = v[idx,:] . return pcs,components_. ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/issues/405:44,reliability,doe,doesn,44,@fidelram Could it be that `stacked_violin` doesn't fully account for `.raw`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:513,reliability,doe,doesn,513,"Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:808,security,auth,auth,808,"Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:111,usability,indicat,indicates,111,"Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:101,availability,consist,consistently,101,That's why I meant the usual `use_raw=None` defaulting to `True` if `.raw` has been set might not be consistently implemented in `stacked_violin`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:101,usability,consist,consistently,101,That's why I meant the usual `use_raw=None` defaulting to `True` if `.raw` has been set might not be consistently implemented in `stacked_violin`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:397,integrability,Sub,Subject,397,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1007,reliability,doe,doesn,1007,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:361,security,Auth,Author,361,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:369,security,auth,author,369,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1302,security,auth,auth,1302,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1407,security,auth,authored,1407,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1611,security,auth,auth,1611,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:605,usability,indicat,indicates,605,"Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com>. Reply-To: theislab/scanpy <reply@reply.github.com>. Date: Monday, January 7, 2019 at 11:16 AM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com>. Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405). Hi, sorry for the late reply. Given that the function works for some mg. genes but not for other, this usually indicates that the gene may not be in. the matrix. Can you try to set `use_raw=True` just to check if this is the. issue (although use_raw should be True by default). Still, very suspicious. that it works with with other functions like matrixplot. On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> Could it be that stacked_violin. > doesn't fully account for .raw? >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/405#issuecomment-450221575>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5>. > . >. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/405#issuecomment-451988385>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:112,availability,consist,consistently,112,"Looking at the function code I don't see why your example does not work. @falexwolf, I checked and `use_raw` is consistently used in all plot functions. . @JuHey Can you check that `mg` is a python list? Do you mind sharing privately your data to try to identify the problem? Or even better, can you reproduce the problem using the example data in Scanpy (e.g. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:58,reliability,doe,does,58,"Looking at the function code I don't see why your example does not work. @falexwolf, I checked and `use_raw` is consistently used in all plot functions. . @JuHey Can you check that `mg` is a python list? Do you mind sharing privately your data to try to identify the problem? Or even better, can you reproduce the problem using the example data in Scanpy (e.g. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:254,security,ident,identify,254,"Looking at the function code I don't see why your example does not work. @falexwolf, I checked and `use_raw` is consistently used in all plot functions. . @JuHey Can you check that `mg` is a python list? Do you mind sharing privately your data to try to identify the problem? Or even better, can you reproduce the problem using the example data in Scanpy (e.g. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:112,usability,consist,consistently,112,"Looking at the function code I don't see why your example does not work. @falexwolf, I checked and `use_raw` is consistently used in all plot functions. . @JuHey Can you check that `mg` is a python list? Do you mind sharing privately your data to try to identify the problem? Or even better, can you reproduce the problem using the example data in Scanpy (e.g. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:411,usability,visual,visualizing-marker-genes,411,"Looking at the function code I don't see why your example does not work. @falexwolf, I checked and `use_raw` is consistently used in all plot functions. . @JuHey Can you check that `mg` is a python list? Do you mind sharing privately your data to try to identify the problem? Or even better, can you reproduce the problem using the example data in Scanpy (e.g. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1656,availability,error,error,1656,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:394,integrability,Sub,Subject,394,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1656,performance,error,error,1656,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:981,reliability,doe,doesn,981,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1656,safety,error,error,1656,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:359,security,Auth,Author,359,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:367,security,auth,author,367,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1269,security,auth,auth,1269,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1369,security,auth,authored,1369,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1591,security,auth,auth,1591,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:600,usability,indicat,indicates,600,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:1656,usability,error,error,1656,"> Hi, no worries! I tried that but by explicitly stating use_raw=True but it did not change the outcome. From: Fidel Ramirez <notifications@github.com> Reply-To: theislab/scanpy <reply@reply.github.com> Date: Monday, January 7, 2019 at 11:16 AM To: theislab/scanpy <scanpy@noreply.github.com> Cc: ""Heymann, Jurgen (NIH/NIDDK) [E]"" <heymannj@niddk.nih.gov>, Author <author@noreply.github.com> Subject: Re: [theislab/scanpy] sc.pl.stacked_violin: IndexError, list index out of range (#405) Hi, sorry for the late reply. Given that the function works for some mg genes but not for other, this usually indicates that the gene may not be in the matrix. Can you try to set `use_raw=True` just to check if this is the issue (although use_raw should be True by default). Still, very suspicious that it works with with other functions like matrixplot. > On Thu, Dec 27, 2018 at 9:07 PM Alex Wolf ***@***.***> wrote: @fidelram <https://github.com/fidelram> Could it be that stacked_violin doesn't fully account for .raw?  You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-450221575)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEu_1ftSP-OSKeDQYWV8Eu0-oRt6aXBAks5u9Sh_gaJpZM4ZiTv5> . >  You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<[#405 (comment)](https://github.com/theislab/scanpy/issues/405#issuecomment-451988385)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AioIiKYX4lsLgg91sMNygZWO1ALRDzsqks5vA3KmgaJpZM4ZiTv5>. Same error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:221,availability,error,error,221,> @fidelram Thank you for your suggestions! The example data in Scanpy worked without flaw. I will go over my code again! Actually I solved this problem by adding more markers in the marker gene list. . Alternatively The error will be gone if I `swap_axes=True`. Interesting,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:221,performance,error,error,221,> @fidelram Thank you for your suggestions! The example data in Scanpy worked without flaw. I will go over my code again! Actually I solved this problem by adding more markers in the marker gene list. . Alternatively The error will be gone if I `swap_axes=True`. Interesting,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:221,safety,error,error,221,> @fidelram Thank you for your suggestions! The example data in Scanpy worked without flaw. I will go over my code again! Actually I solved this problem by adding more markers in the marker gene list. . Alternatively The error will be gone if I `swap_axes=True`. Interesting,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:221,usability,error,error,221,> @fidelram Thank you for your suggestions! The example data in Scanpy worked without flaw. I will go over my code again! Actually I solved this problem by adding more markers in the marker gene list. . Alternatively The error will be gone if I `swap_axes=True`. Interesting,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:27,availability,error,error,27,"I'm also getting this list error, but @brianpenghe 's suggestion of using `swap_axes=True` also seems to have fixed the problem. At least it shows a plot now, although not sure if its correct yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:27,performance,error,error,27,"I'm also getting this list error, but @brianpenghe 's suggestion of using `swap_axes=True` also seems to have fixed the problem. At least it shows a plot now, although not sure if its correct yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:27,safety,error,error,27,"I'm also getting this list error, but @brianpenghe 's suggestion of using `swap_axes=True` also seems to have fixed the problem. At least it shows a plot now, although not sure if its correct yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:27,usability,error,error,27,"I'm also getting this list error, but @brianpenghe 's suggestion of using `swap_axes=True` also seems to have fixed the problem. At least it shows a plot now, although not sure if its correct yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:51,availability,error,error,51,@outlace Did you try adding more marker genes? The error is gone if you have a large number of marker genes to plot in my case.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:51,performance,error,error,51,@outlace Did you try adding more marker genes? The error is gone if you have a large number of marker genes to plot in my case.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:51,safety,error,error,51,@outlace Did you try adding more marker genes? The error is gone if you have a large number of marker genes to plot in my case.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:51,usability,error,error,51,@outlace Did you try adding more marker genes? The error is gone if you have a large number of marker genes to plot in my case.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:92,modifiability,variab,variable,92,Just made a pull request that fixes this issue. @brianpenghe . I did some debugging and the variable `num_rows` was incorrectly calculated only when `swap_axes==False` on line 880 of `_anndata.py`. Instead of `num_rows = len(categories)` it should be `num_rows = len(var_names)` . If you make that small change in your _anndata.py in `~/anaconda3/lib/site-packages/scanpy/plotting/_anndata.py` then recompile the packages using `python -m compileall .` and restart python it should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:356,modifiability,pac,packages,356,Just made a pull request that fixes this issue. @brianpenghe . I did some debugging and the variable `num_rows` was incorrectly calculated only when `swap_axes==False` on line 880 of `_anndata.py`. Instead of `num_rows = len(categories)` it should be `num_rows = len(var_names)` . If you make that small change in your _anndata.py in `~/anaconda3/lib/site-packages/scanpy/plotting/_anndata.py` then recompile the packages using `python -m compileall .` and restart python it should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:413,modifiability,pac,packages,413,Just made a pull request that fixes this issue. @brianpenghe . I did some debugging and the variable `num_rows` was incorrectly calculated only when `swap_axes==False` on line 880 of `_anndata.py`. Instead of `num_rows = len(categories)` it should be `num_rows = len(var_names)` . If you make that small change in your _anndata.py in `~/anaconda3/lib/site-packages/scanpy/plotting/_anndata.py` then recompile the packages using `python -m compileall .` and restart python it should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/pull/406:134,deployability,api,api,134,"Happy to see your review, @flying-sheep! Note that the rehauled docs are here: https://icb-scanpy.readthedocs-hosted.com/en/beyondapi/api/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:134,integrability,api,api,134,"Happy to see your review, @flying-sheep! Note that the rehauled docs are here: https://icb-scanpy.readthedocs-hosted.com/en/beyondapi/api/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:134,interoperability,api,api,134,"Happy to see your review, @flying-sheep! Note that the rehauled docs are here: https://icb-scanpy.readthedocs-hosted.com/en/beyondapi/api/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:18,safety,review,review,18,"Happy to see your review, @flying-sheep! Note that the rehauled docs are here: https://icb-scanpy.readthedocs-hosted.com/en/beyondapi/api/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:18,testability,review,review,18,"Happy to see your review, @flying-sheep! Note that the rehauled docs are here: https://icb-scanpy.readthedocs-hosted.com/en/beyondapi/api/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:97,deployability,contain,contains,97,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:156,deployability,api,api,156,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:298,deployability,api,api,298,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:49,integrability,sub,submodule,49,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:156,integrability,api,api,156,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:298,integrability,api,api,298,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:156,interoperability,api,api,156,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:298,interoperability,api,api,298,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:249,testability,simpl,simply,249,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:249,usability,simpl,simply,249,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:95,integrability,pub,public,95,can you give me access to https://icb-scanpy.readthedocs-hosted.com? I just have access to the public readthedocs.org,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:16,security,access,access,16,can you give me access to https://icb-scanpy.readthedocs-hosted.com? I just have access to the public readthedocs.org,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:81,security,access,access,81,can you give me access to https://icb-scanpy.readthedocs-hosted.com? I just have access to the public readthedocs.org,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:5,safety,test,tested,5,"I've tested this quite a bit in the past 5 days and am merging it into master. In essence, it's a superficial change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:5,testability,test,tested,5,"I've tested this quite a bit in the past 5 days and am merging it into master. In essence, it's a superficial change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:435,deployability,api,api,435,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:476,deployability,updat,update,476,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:495,deployability,api,api,495,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:500,deployability,modul,module,500,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:435,integrability,api,api,435,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:495,integrability,api,api,495,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:435,interoperability,api,api,435,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:495,interoperability,api,api,495,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:500,modifiability,modul,module,500,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:104,reliability,doe,doesn,104,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:450,safety,Except,Except,450,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:476,safety,updat,update,476,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:500,safety,modul,module,500,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:259,security,team,team,259,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:476,security,updat,update,476,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:210,testability,simpl,simply,210,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:464,testability,plan,plan,464,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:210,usability,simpl,simply,210,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:339,usability,prefer,prefer,339,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesnt seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that Im now member of the team on rtd.com (which I wasnt before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:28,deployability,updat,update,28,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:46,deployability,api,api,46,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:50,deployability,modul,module,50,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:105,deployability,api,api,105,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:46,integrability,api,api,46,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:105,integrability,api,api,105,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:46,interoperability,api,api,46,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:105,interoperability,api,api,105,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:165,interoperability,compatib,compatibility,165,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:50,modifiability,modul,module,50,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:2,safety,Except,Except,2,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:28,safety,updat,update,28,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:50,safety,modul,module,50,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:113,safety,compl,completely,113,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:28,security,updat,update,28,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:113,security,compl,completely,113,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:16,testability,plan,plan,16,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:91,testability,plan,plan,91,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:138,testability,simpl,simply,138,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/pull/406:138,usability,simpl,simply,138,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/issues/407:178,integrability,coupl,couple,178,Thank you for your suggestions! I'm very happy to improve things. We're just right now working on improvements on another branch (`beyondapi`). I'll get back to this thread in a couple days with the results...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:178,modifiability,coupl,couple,178,Thank you for your suggestions! I'm very happy to improve things. We're just right now working on improvements on another branch (`beyondapi`). I'll get back to this thread in a couple days with the results...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:178,testability,coupl,couple,178,Thank you for your suggestions! I'm very happy to improve things. We're just right now working on improvements on another branch (`beyondapi`). I'll get back to this thread in a couple days with the results...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:157,deployability,api,api,157,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:157,integrability,api,api,157,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:284,integrability,coupl,couple,284,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:157,interoperability,api,api,157,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:284,modifiability,coupl,couple,284,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:284,testability,coupl,couple,284,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:179,usability,feedback,feedback,179,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:8,usability,document,documentation,8,The new documentation index looks great! It's exactly what I was thinking about! Thanks so much for being so responsive!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/407:109,usability,responsiv,responsive,109,The new documentation index looks great! It's exactly what I was thinking about! Thanks so much for being so responsive!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/408:140,deployability,api,api,140,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:246,deployability,api,api,246,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:415,deployability,contain,contain,415,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:602,deployability,api,api,602,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:670,deployability,api,api,670,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:140,integrability,api,api,140,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:246,integrability,api,api,246,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:602,integrability,api,api,602,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:670,integrability,api,api,670,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:140,interoperability,api,api,140,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:246,interoperability,api,api,246,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:602,interoperability,api,api,602,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:670,interoperability,api,api,670,"Well, [`scanpy/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/__init__.py), [`scanpy/api/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/api/__init__.py) and [`scanpy/external/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/external/__init__.py) dont contain `.. automodule::` whereas [`scanpy/plotting/__init__.py`](https://github.com/theislab/scanpy/blob/9d599f921bcd5a0464f9db409e3ec0c58f8c8d2c/scanpy/plotting/__init__.py) and [`docs/api/index.rst`](https://github.com/theislab/scanpy/blob/master/docs/api/index.rst) do. So just removing the `automodule` lines should fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:204,deployability,api,api,204,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:284,deployability,api,api,284,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:328,deployability,api,api,328,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:360,deployability,contain,contain,360,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:499,deployability,contain,contain,499,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:204,integrability,api,api,204,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:284,integrability,api,api,284,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:328,integrability,api,api,328,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:204,interoperability,api,api,204,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:284,interoperability,api,api,284,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:328,interoperability,api,api,328,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:121,reliability,doe,doesn,121,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:352,reliability,doe,doesn,352,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:494,reliability,doe,does,494,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:227,safety,compl,completely,227,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:227,security,compl,completely,227,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:4,testability,simpl,simply,4,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:4,usability,simpl,simply,4,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:400,usability,document,documenting,400,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:520,usability,document,documenting,520,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:113,modifiability,extens,extensions,113,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:227,modifiability,extens,extensions,227,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:31,reliability,doe,doesn,31,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:465,reliability,doe,doesn,465,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:419,testability,understand,understand,419,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:545,testability,simpl,simply,545,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:545,usability,simpl,simply,545,"I see! OK, so `.. automodule` [doesnt by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we dont have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I dont understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:156,deployability,api,api,156,"Hmm, maybe it has to do with some docstring bugs, lets check. e.g. this is pretty borked, Ill fix it: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.external.exporting.cellbrowser.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:156,integrability,api,api,156,"Hmm, maybe it has to do with some docstring bugs, lets check. e.g. this is pretty borked, Ill fix it: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.external.exporting.cellbrowser.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:156,interoperability,api,api,156,"Hmm, maybe it has to do with some docstring bugs, lets check. e.g. this is pretty borked, Ill fix it: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.external.exporting.cellbrowser.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:228,availability,cluster,clustermap,228,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:40,deployability,fail,fail,40,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:119,deployability,api,api,119,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:144,deployability,contain,containing,144,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:228,deployability,cluster,clustermap,228,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:119,integrability,api,api,119,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:119,interoperability,api,api,119,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/408:40,reliability,fail,fail,40,"Docutils says that all kinds of imports fail in `plotting`, hmm. /edit: ah, of course. 1. autosummary generates `docs/api/scanpy.plotting.rst` containing a header and. ```rst. .. automodule:: scanpy.plotting. .. autosummary::. clustermap. ... ```. 2. the `automodule` directive renders the docstring from `plotting/__init__.py` and then the autogenerated list of functions. To fix this, we need to create `scanpy.plotting.rst` ourselves. Will do so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/409:16,availability,slo,slow,16,"Sorry about the slow response, I'll look into this early this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:16,reliability,slo,slow,16,"Sorry about the slow response, I'll look into this early this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:136,availability,error,error,136,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1599,availability,error,error,1599,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1877,availability,error,error,1877,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:524,deployability,modul,module,524,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1625,deployability,Updat,Update,1625,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:766,energy efficiency,heat,heatmap,766,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1088,integrability,sub,subplots,1088,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1428,integrability,sub,substituting,1428,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:524,modifiability,modul,module,524,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:615,modifiability,pac,packages,615,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:973,modifiability,pac,packages,973,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:136,performance,error,error,136,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:751,performance,time,time,751,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1599,performance,error,error,1599,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1715,performance,perform,performing,1715,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1877,performance,error,error,1877,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:136,safety,error,error,136,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:382,safety,except,exception,382,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:430,safety,except,exception,430,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:497,safety,input,input-,497,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:524,safety,modul,module,524,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1599,safety,error,error,1599,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1625,safety,Updat,Update,1625,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1877,safety,error,error,1877,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1625,security,Updat,Update,1625,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:261,testability,Trace,Traceback,261,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:453,testability,Trace,Traceback,453,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:136,usability,error,error,136,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:497,usability,input,input-,497,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1599,usability,error,error,1599,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1658,usability,command,command,1658,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1715,usability,perform,performing,1715,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:1877,usability,error,error,1877,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-48-abf5bf78cb77> in <module>. ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap). 159 if as_heatmap:. 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d. --> 161 timeseries_as_heatmap(. 162 adata.X[adata.obs['dpt_order_indices'].values],. 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map). 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)). 198 ax.imshow(. --> 199 np.array(X, dtype=np.float_),. 200 aspect='auto',. 201 interpolation='nearest',. ValueError: setting an array element with a sequence. ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance! Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:115,availability,error,error,115,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:40,deployability,version,version,40,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:40,integrability,version,version,40,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:40,modifiability,version,version,40,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:115,performance,error,error,115,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:115,safety,error,error,115,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/issues/409:115,usability,error,error,115,@falexwolf This issue still persists in version 1.9.1. and the work around suggested by @Xparx results in the same error that @haskankaya has reported. Any advice on how to get around this or a potential fix?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409
https://github.com/scverse/scanpy/pull/412:23,usability,user,users,23,looks good! also tells users not to use any of those things,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/412
https://github.com/scverse/scanpy/issues/415:19,availability,consist,consistent,19,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:89,safety,test,test,89,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:149,safety,test,tests,149,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:31,testability,simpl,simple,31,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:89,testability,test,test,89,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:149,testability,test,tests,149,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:19,usability,consist,consistent,19,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:31,usability,simpl,simple,31,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/417:488,integrability,batch,batch-correct,488,"I think I figured it out, just had to convert the matrix (`adata.X`) todense in order to get the DataFrame I wanted: . <img width=""1102"" alt=""screen shot 2019-01-09 at 4 09 20 pm"" src=""https://user-images.githubusercontent.com/8352840/50928530-f00ad580-1428-11e9-97f3-37cd6aa8b4d6.png"">. I'm trying to export a DataFrame for use with [clustergrammer2](https://github.com/ismms-himc/clustergrammer2). See example notebook on nbviewer: https://nbviewer.jupyter.org/github/ismms-himc/scanpy-batch-correct/blob/export_df/notebooks/1.0_Batch_Correction_Example.ipynb. Is there a way to export a dense DataFrame directly from adata?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/417
https://github.com/scverse/scanpy/issues/417:488,performance,batch,batch-correct,488,"I think I figured it out, just had to convert the matrix (`adata.X`) todense in order to get the DataFrame I wanted: . <img width=""1102"" alt=""screen shot 2019-01-09 at 4 09 20 pm"" src=""https://user-images.githubusercontent.com/8352840/50928530-f00ad580-1428-11e9-97f3-37cd6aa8b4d6.png"">. I'm trying to export a DataFrame for use with [clustergrammer2](https://github.com/ismms-himc/clustergrammer2). See example notebook on nbviewer: https://nbviewer.jupyter.org/github/ismms-himc/scanpy-batch-correct/blob/export_df/notebooks/1.0_Batch_Correction_Example.ipynb. Is there a way to export a dense DataFrame directly from adata?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/417
https://github.com/scverse/scanpy/issues/417:193,usability,user,user-images,193,"I think I figured it out, just had to convert the matrix (`adata.X`) todense in order to get the DataFrame I wanted: . <img width=""1102"" alt=""screen shot 2019-01-09 at 4 09 20 pm"" src=""https://user-images.githubusercontent.com/8352840/50928530-f00ad580-1428-11e9-97f3-37cd6aa8b4d6.png"">. I'm trying to export a DataFrame for use with [clustergrammer2](https://github.com/ismms-himc/clustergrammer2). See example notebook on nbviewer: https://nbviewer.jupyter.org/github/ismms-himc/scanpy-batch-correct/blob/export_df/notebooks/1.0_Batch_Correction_Example.ipynb. Is there a way to export a dense DataFrame directly from adata?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/417
https://github.com/scverse/scanpy/issues/417:139,usability,Document,Documentation,139,Thank you for pointing out that bug. It's fixed here: https://github.com/theislab/anndata/commit/2d11f9fb1cb9059a178c7d11ffa595b49b742342. Documentation is here: https://anndata.readthedocs.io/en/latest/anndata.AnnData.to_df.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/417
https://github.com/scverse/scanpy/issues/418:101,modifiability,paramet,parameter,101,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:117,reliability,doe,does,117,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:275,safety,test,test,275,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:323,safety,test,test,323,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:275,testability,test,test,275,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:323,testability,test,test,323,"@Xparx Thanks for reporting the problem an a potential solution. . Each plotting function has a save parameter which does:. ```. pl.savefig(filename, dpi=dpi, bbox_inches='tight'). ```. So, instead of calling `fig.savefig()`, what you can do in your example is to add `save='test.png'`:. ```. sc.pl.matrixplot(adata, save='test.png', var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:154,deployability,api,api,154,"Cool! :smile:. The only thing to add to this, you can change the directory where you want figures to end up here: https://scanpy.readthedocs.io/en/latest/api/index.html#settings",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:0,energy efficiency,Cool,Cool,0,"Cool! :smile:. The only thing to add to this, you can change the directory where you want figures to end up here: https://scanpy.readthedocs.io/en/latest/api/index.html#settings",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:154,integrability,api,api,154,"Cool! :smile:. The only thing to add to this, you can change the directory where you want figures to end up here: https://scanpy.readthedocs.io/en/latest/api/index.html#settings",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:154,interoperability,api,api,154,"Cool! :smile:. The only thing to add to this, you can change the directory where you want figures to end up here: https://scanpy.readthedocs.io/en/latest/api/index.html#settings",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:185,usability,workflow,workflow,185,Didn't know about the setting settings! I will try to use that. I noticed the `save` figure argument but couldn't connect it to saving where I wanted and how to incorporate it in to my workflow. With the settings functionality it is probably doable. . Thanks for your responses.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/419:186,energy efficiency,heat,heatmap,186,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:808,energy efficiency,draw,drawbacks,808,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:588,interoperability,standard,standard,588,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:657,modifiability,paramet,parameter,657,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:224,safety,test,testing,224,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:308,safety,test,test,308,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:358,safety,test,test,358,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:224,testability,test,testing,224,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:308,testability,test,test,308,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:358,testability,test,test,358,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:423,usability,visual,visualizing-marker-genes,423,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:719,usability,behavi,behaviour,719,"All plotting functions should return an ax or an ax list if `show=False`. `_rank_genes_groups_plot` returns whatever the internally called function returns (which could be tracksplot or heatmap etc). However, I don't recall testing this output in all cases. . Can you provide a non working example using the test data? Here you can see how to use one of the test datasets: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. I think that @falexwolf can comment on the original reason to set `show=False` to return the axes. . I agree with @Xparx that is more standard to always return the axes as you usually don't dig into the parameter list for this functionality. However, changing this behaviour now could break some code so I don't know if the benefits are greater than the drawbacks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:315,availability,state,statements,315,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:607,availability,state,statement,607,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:461,energy efficiency,heat,heatmap,461,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:315,integrability,state,statements,315,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:607,integrability,state,statement,607,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:56,safety,test,test,56,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:56,testability,test,test,56,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:70,usability,visual,visualization,70,"Hi thinks for the answer and thanks for the link on the test data and visualization, I will try to use that going forward. I will cook up a non working example if needed, however just looking at the code https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302 there is missing return statements for a few of the plotting functions in the `_rank_genes_groups_plot` unless I missed something they will then not return an axes? The [heatmap]( https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044) function itself return an axis but there is no return statement from the `_rank_genes_groups_plot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:429,availability,state,statements,429,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:734,availability,state,statement,734,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:583,energy efficiency,heat,heatmap,583,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:429,integrability,state,statements,429,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:734,integrability,state,statement,734,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:153,safety,test,test,153,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:1033,security,auth,auth,1033,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:153,testability,test,test,153,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:170,usability,visual,visualization,170,"I will take a look. On Fri, Jan 11, 2019 at 6:08 PM Andreas <notifications@github.com> wrote:. > Hi thinks for the answer and thanks for the link on the test data and. > visualization, I will try to use that going forward. >. > I will cook up a non working example if needed, however just looking at. > the code. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L302. > there is missing return statements for a few of the plotting functions in. > the _rank_genes_groups_plot unless I missed something they will then not. > return an axes? >. > The heatmap. > <https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1044>. > function itself return an axis but there is no return statement from the. > _rank_genes_groups_plot. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453587853>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1aJAKobdjZYdCil5CcJ3vJz8h-2nks5vCMUmgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:475,deployability,automat,automatically,475,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:687,deployability,api,api,687,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:270,integrability,pub,publication-quality,270,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:687,integrability,api,api,687,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:687,interoperability,api,api,687,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:128,modifiability,pac,packages,128,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:1103,reliability,Doe,Does,1103,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:475,testability,automat,automatically,475,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:290,usability,visual,visualization,290,"Hey! Thank you for the thoughtful considerations! Yes, by default, Scanpy plotting functions return `None`, other than in other packages. Scanpy's plotting should be seen as being even more high-level, than, say Seaborn. A single one-line function call should display a publication-quality visualization in a notebook and no ugly string representation of a returned Axes should clutter the output. This is why `settings.autoshow` defaults to `True` and any plotting function automatically calls `pl.show()`, other than Seaborn... If you don't want that, you can either pass `show=False` in the function or globally set [`settings.autoshow=False`](https://scanpy.readthedocs.io/en/latest/api/index.html#settings). In that case, every plotting function should return an Axes or a collection of Axes. . Typically, the Scanpy plotting functions are so high level that you don't even need to postprocess a returned axis object. And if you really want to, calling functions on the `matplotlib.pyplot` namespace comes usually handier. If you then really need the axes, you can also get it via `ax = pl.gca()`. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:135,deployability,api,api,135,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:267,deployability,modul,modules,267,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:135,integrability,api,api,135,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:135,interoperability,api,api,135,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:79,modifiability,pac,package,79,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:267,modifiability,modul,modules,267,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:267,safety,modul,modules,267,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:436,safety,avoid,avoid,436,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:325,usability,workflow,workflow,325,"@falexwolf Thanks for the explanation. I see now that it's an issue of how the package should be used, i.e. the philosophy behind your api, and I seem to have tried to use it in a different perhaps more low level way. I will look in to using more low level functions/modules if I need it and work with your original intended workflow which i think make sense. I have missed the whole settings functionality which seems really useful to avoid some of the issues I have had.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:352,deployability,api,api,352,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:490,deployability,modul,modules,490,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:352,integrability,api,api,352,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:352,interoperability,api,api,352,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:293,modifiability,pac,package,293,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:490,modifiability,modul,modules,490,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:490,safety,modul,modules,490,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:665,safety,avoid,avoid,665,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:957,security,auth,auth,957,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/419:551,usability,workflow,workflow,551,"Nevertheless, some of the plotting functions are returning ax by default. I. will need to change that. On Mon, Jan 14, 2019 at 12:44 AM Andreas <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> Thanks for the explanation. > I see now that it's an issue of how the package should be used, i.e. the. > philosophy behind your api, and I seem to have tried to use it in a. > different perhaps more low level way. I will look in to using more low. > level functions/modules if I need it and work with your original intended. > workflow which i think make sense. I have missed the whole settings. > functionality which seems really useful to avoid some of the issues I have. > had. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/419#issuecomment-453876840>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1YINwJpe8e_BJavkUulUdzp6IMItks5vC8TLgaJpZM4Z4pAD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/420:91,availability,cluster,clusters,91,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:128,availability,cluster,cluster,128,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:62,deployability,updat,updated,62,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:91,deployability,cluster,clusters,91,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:128,deployability,cluster,cluster,128,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:254,deployability,updat,updated,254,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:62,safety,updat,updated,62,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:161,safety,avoid,avoid,161,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:254,safety,updat,updated,254,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:62,security,updat,updated,62,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:254,security,updat,updated,254,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:161,availability,avail,available,161,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:54,deployability,updat,updated,54,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:221,deployability,automat,automatically,221,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:161,reliability,availab,available,161,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:54,safety,updat,updated,54,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:161,safety,avail,available,161,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:54,security,updat,updated,54,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:161,security,availab,available,161,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:221,testability,automat,automatically,221,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/420:15,usability,hint,hint,15,Thanks for the hint. Someone else asked a similar question and answered to him.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420
https://github.com/scverse/scanpy/issues/421:166,deployability,version,versions,166,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:402,deployability,modul,module,402,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:5,energy efficiency,load,load,5,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:166,integrability,version,versions,166,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:830,interoperability,format,formatting,830,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:166,modifiability,version,versions,166,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:402,modifiability,modul,module,402,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:511,modifiability,pac,packages,511,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:819,modifiability,variab,variables,819,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:990,modifiability,pac,packages,990,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:5,performance,load,load,5,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:148,safety,test,tested,148,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:374,safety,input,input-,374,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:402,safety,modul,module,402,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:148,testability,test,tested,148,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:330,testability,Trace,Traceback,330,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:374,usability,input,input-,374,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```. In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) . ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-385-66af52bcd3f3> in <module>. ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace). 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(. 71 obs_metrics[""total_{expr_type}""]). ---> 72 proportions = top_segment_proportions(X, percent_top). 73 # Since there are local loop variables, formatting must occur in their scope. 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns). 182 if not isspmatrix_csr(mtx):. 183 mtx = csr_matrix(mtx). --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns). 185 else:. 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0. ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:92,availability,error,errors,92,"Just checking to make sure we're working with the same data, since there could be different errors coming from that. The PR should do the right thing on `pbmc3k`. Could you check if it works for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:92,performance,error,errors,92,"Just checking to make sure we're working with the same data, since there could be different errors coming from that. The PR should do the right thing on `pbmc3k`. Could you check if it works for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:92,safety,error,errors,92,"Just checking to make sure we're working with the same data, since there could be different errors coming from that. The PR should do the right thing on `pbmc3k`. Could you check if it works for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/421:92,usability,error,errors,92,"Just checking to make sure we're working with the same data, since there could be different errors coming from that. The PR should do the right thing on `pbmc3k`. Could you check if it works for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/422:38,integrability,coupl,couple,38,Scanpy expects string categories in a couple of places. Didn't you get a warning at some point when trying to set integer categories?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:38,modifiability,coupl,couple,38,Scanpy expects string categories in a couple of places. Didn't you get a warning at some point when trying to set integer categories?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:38,testability,coupl,couple,38,Scanpy expects string categories in a couple of places. Didn't you get a warning at some point when trying to set integer categories?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:628,availability,error,error,628,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:35,deployability,version,versions,35,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:152,deployability,contain,contains,152,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:144,energy efficiency,load,loading,144,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:210,energy efficiency,load,load,210,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:35,integrability,version,versions,35,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:35,modifiability,version,versions,35,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:166,modifiability,variab,variable,166,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:144,performance,load,loading,144,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:210,performance,load,load,210,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:628,performance,error,error,628,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:390,reliability,doe,does,390,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:628,safety,error,error,628,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:203,testability,simpl,simply,203,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:612,testability,understand,understand,612,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:203,usability,simpl,simply,203,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:628,usability,error,error,628,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```. adata = sc.read_loom(lf). adata.obs.columns = [""cellid"", ""hpf""]. adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'). ```. This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly. Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:70,integrability,coupl,couple,70,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:252,interoperability,standard,standard,252,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:70,modifiability,coupl,couple,70,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:221,performance,perform,performance,221,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:412,reliability,doe,doesn,412,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:80,safety,sanit,sanity,80,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:80,security,sanit,sanity,80,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:70,testability,coupl,couple,70,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:221,usability,perform,performance,221,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:300,energy efficiency,heat,heatmap,300,"This seems to be working now. As these seem equivalent enough:. ```python. import scanpy as sc, pandas as pd. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""louvain_int""] = pd.Categorical(pbmc.obs[""louvain""].cat.codes). sc.pl.heatmap(pbmc, genes, groupby=""louvain_int""). sc.pl.heatmap(pbmc, genes, groupby=""louvain""). ```. Not sure if this ever got a test case though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:351,energy efficiency,heat,heatmap,351,"This seems to be working now. As these seem equivalent enough:. ```python. import scanpy as sc, pandas as pd. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""louvain_int""] = pd.Categorical(pbmc.obs[""louvain""].cat.codes). sc.pl.heatmap(pbmc, genes, groupby=""louvain_int""). sc.pl.heatmap(pbmc, genes, groupby=""louvain""). ```. Not sure if this ever got a test case though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:425,safety,test,test,425,"This seems to be working now. As these seem equivalent enough:. ```python. import scanpy as sc, pandas as pd. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""louvain_int""] = pd.Categorical(pbmc.obs[""louvain""].cat.codes). sc.pl.heatmap(pbmc, genes, groupby=""louvain_int""). sc.pl.heatmap(pbmc, genes, groupby=""louvain""). ```. Not sure if this ever got a test case though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/issues/422:425,testability,test,test,425,"This seems to be working now. As these seem equivalent enough:. ```python. import scanpy as sc, pandas as pd. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""louvain_int""] = pd.Categorical(pbmc.obs[""louvain""].cat.codes). sc.pl.heatmap(pbmc, genes, groupby=""louvain_int""). sc.pl.heatmap(pbmc, genes, groupby=""louvain""). ```. Not sure if this ever got a test case though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/pull/423:131,availability,redund,redundancy,131,"No, you're right. It's fine to continue having this. Initially, I wanted to get rid of it at some point... but also numpy has this redundancy between many array attributes which pop up as numpy level functions everywhere. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/pull/423:31,deployability,continu,continue,31,"No, you're right. It's fine to continue having this. Initially, I wanted to get rid of it at some point... but also numpy has this redundancy between many array attributes which pop up as numpy level functions everywhere. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/pull/423:131,deployability,redundan,redundancy,131,"No, you're right. It's fine to continue having this. Initially, I wanted to get rid of it at some point... but also numpy has this redundancy between many array attributes which pop up as numpy level functions everywhere. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/pull/423:131,reliability,redundan,redundancy,131,"No, you're right. It's fine to continue having this. Initially, I wanted to get rid of it at some point... but also numpy has this redundancy between many array attributes which pop up as numpy level functions everywhere. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/pull/423:131,safety,redund,redundancy,131,"No, you're right. It's fine to continue having this. Initially, I wanted to get rid of it at some point... but also numpy has this redundancy between many array attributes which pop up as numpy level functions everywhere. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/pull/424:50,modifiability,exten,extended,50,"@ivirshup Do you mind adding as part of the PR an extended description of the function? I don't think that everyone is familiar with `calculateQCMetrics` and thus, the output of this function is unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:93,usability,help,help,93,"Are there any particular areas you'd like to see expanded? Otherwise, maybe an example would help? I'm thinking something like:. ```python. Example. -------. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. or something longer (though I think this is in one of the tutorials):. ```python. Example. -------. >>> adata = sc.datasets.pbmc3k(). >>> adata.var[""mito""] = adata.var_names.str.startswith(""MT-""). >>> sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""pct_counts_mito"", kind=""hex""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:131,deployability,version,versions,131,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:263,deployability,version,version,263,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:131,integrability,version,versions,131,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:146,integrability,topic,topics,146,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:263,integrability,version,version,263,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:115,modifiability,pac,packages,115,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:131,modifiability,version,versions,131,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:263,modifiability,version,version,263,"A summary of the output from this page would be too difficult or maybe add the link?https://www.rdocumentation.org/packages/scater/versions/1.0.4/topics/calculateQCMetrics (btw, is this the right function?). Adding an example is a good idea. Could be the shorter version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:65,deployability,version,version,65,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:235,deployability,Updat,Updated,235,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:928,deployability,Contain,Container,928,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1098,deployability,Contain,Container,1098,"o date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1409,deployability,Depend,Depending,1409,"alculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1480,deployability,updat,updates,1480,"r an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1515,deployability,Observ,Observation,1515,"turns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:57,energy efficiency,current,current,57,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:608,energy efficiency,Current,Currently,608,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:2478,energy efficiency,measur,measured,2478,": `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`. E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does . not appear in. . Example. -------. Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). """""". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:65,integrability,version,version,65,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1409,integrability,Depend,Depending,1409,"alculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:531,interoperability,specif,specifics,531,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:65,modifiability,version,version,65,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:669,modifiability,Paramet,Parameters,669,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:902,modifiability,variab,variables,902,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1016,modifiability,variab,variables,1016,"on, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total numb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1409,modifiability,Depend,Depending,1409,"alculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:2038,modifiability,variab,variabes,2038,": `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`. E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does . not appear in. . Example. -------. Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). """""". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:2210,modifiability,Variab,Variable,2210,": `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`. E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does . not appear in. . Example. -------. Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). """""". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:2586,reliability,doe,does,2586,": `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`. E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does . not appear in. . Example. -------. Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). """""". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:235,safety,Updat,Updated,235,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1409,safety,Depend,Depending,1409,"alculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1480,safety,updat,updates,1480,"r an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:235,security,Updat,Updated,235,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:431,security,control,control,431,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1007,security,ident,identify,1007,"ght function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1046,security,control,control,1046," date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1480,security,updat,updates,1480,"r an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:431,testability,control,control,431,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1046,testability,control,control,1046," date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1409,testability,Depend,Depending,1409,"alculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:1515,testability,Observ,Observation,1515,"turns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:626,usability,efficien,efficient,626,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>. <summary>Updated docstring</summary>. ```python. def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),. percent_top=(50, 100, 200, 500), inplace=False):. """""". Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section . Returns for specifics. Largely based on `calculateQCMetrics` from scater. [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters. ----------. adata : :class:`~anndata.AnnData`. Annotated data matrix. expr_type : `str`, optional (default: `""counts""`). Name of kind of values in X. var_type : `str`, optional (default: `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_cou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:2653,usability,visual,visualization,2653,": `""genes""`). The kind of thing the variables are. qc_vars : `Container`, optional (default: `()`). Keys for boolean columns of `.var` which identify variables you could . want to control for (e.g. ""ERCC"" or ""mito""). percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`). Which proportions of top genes to cover. If empty or `None` don't. calculate. inplace : bool, optional (default: `False`). Whether to place calculated metrics in `.obs` and `.var`. Returns. -------. Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]. Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or. updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`. E.g. ""total_genes_by_counts"". Number of genes with positive counts . in a cell. * `total_{expr_type}`. E.g. ""total_counts"". Total number of counts for a cell. * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`. E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts . for 50 most expressed genes in a cell. * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""total_counts_mito"". Total number of counts for variabes in . `qc_vars`. * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`. E.g. ""pct_counts_mito"". Proportion of total counts for a cell which . are mitochondrial. Variable level metrics include:. * `total_{expr_type}`. E.g. ""total_counts"". Sum of counts for a gene. * `mean_{expr_type}`. E.g. ""mean counts"". Mean expression over all cells. * `n_cells_by_{expr_type}`. E.g. ""n_cells_by_counts"". Number of cells this expression is . measured in. * `pct_dropout_by_{expr_type}`. E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does . not appear in. . Example. -------. Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). """""". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:17,deployability,updat,updated,17,I think that the updated docstring is much better.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:17,safety,updat,updated,17,I think that the updated docstring is much better.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/424:17,security,updat,updated,17,I think that the updated docstring is much better.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424
https://github.com/scverse/scanpy/pull/425:406,availability,cluster,clustering,406,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:620,availability,state,states,620,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:688,availability,cluster,clustering,688,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1020,availability,cluster,clustering,1020,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1280,availability,cluster,cluster,1280,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1343,availability,cluster,clusters,1343,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:406,deployability,cluster,clustering,406,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:420,deployability,observ,observations,420,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:520,deployability,API,API,520,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:688,deployability,cluster,clustering,688,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:959,deployability,observ,observations,959,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1020,deployability,cluster,clustering,1020,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1111,deployability,stage,stage,1111,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1280,deployability,cluster,cluster,1280,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:1343,deployability,cluster,clusters,1343,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:520,integrability,API,API,520,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:620,integrability,state,states,620,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:520,interoperability,API,API,520,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:649,interoperability,specif,specific,649,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:794,interoperability,standard,standard,794,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:468,modifiability,variab,variables,468,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:904,modifiability,exten,extend,904,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:637,reliability,doe,does,637,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:420,testability,observ,observations,420,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:959,testability,observ,observations,959,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:851,usability,clear,clear,851,"Sorry for the late response! This seems to have come just after I went through the issues last weekend... . It looks great! :smile:. Some small notes:. * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...). * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:887,availability,cluster,clustering,887,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:887,deployability,cluster,clustering,887,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:318,energy efficiency,Current,Currently,318,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:227,modifiability,exten,extend,227,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:584,modifiability,paramet,parameter,584,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:652,performance,network,network,652,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:615,reliability,doe,does,615,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:707,reliability,doe,does,707,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:152,security,expos,expose,152,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:652,security,network,network,652,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:26,usability,feedback,feedback,26,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:422,usability,tool,tools,422,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:252,deployability,scale,scale,252,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:378,deployability,contain,contained,378,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:252,energy efficiency,scale,scale,252,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:252,modifiability,scal,scale,252,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:74,performance,network,network,74,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:252,performance,scale,scale,252,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:286,performance,time,time,286,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:349,performance,network,network,349,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:449,performance,network,network,449,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:500,performance,time,time,500,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:37,reliability,doe,does,37,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:129,reliability,doe,does,129,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:74,security,network,network,74,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:349,security,network,network,349,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:449,security,network,network,449,"Great! :smile:. > What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. That is fine: if you just compute distances on the medians/medoids that's a small-scale problem that won't cost any time. If Seurat uses medians, they will not use the underlying network, because they aren't contained in there. If they use medoids, they might use the underlying network, but it will be negligible save in compute time... So don't worry about that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:16,availability,error,error,16,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:80,deployability,version,version,80,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:61,integrability,sub,submitting,61,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:80,integrability,version,version,80,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:80,modifiability,version,version,80,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:16,performance,error,error,16,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:16,safety,error,error,16,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:16,usability,error,error,16,I am getting an error elsewhere that I want to revise before submitting a final version. Hopefully tomorrow,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:40,deployability,updat,updated,40,I think it is ok to merge now. . I also updated some of the plotting functions to accept a `gene_symbol` column: . ![image](https://user-images.githubusercontent.com/4964309/52279718-85cb4f00-295a-11e9-99e9-f9b8648609a6.png). What is missing is `sc.pl.rank_genes_groups` and `sc.pl.violin` any volunteers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:40,safety,updat,updated,40,I think it is ok to merge now. . I also updated some of the plotting functions to accept a `gene_symbol` column: . ![image](https://user-images.githubusercontent.com/4964309/52279718-85cb4f00-295a-11e9-99e9-f9b8648609a6.png). What is missing is `sc.pl.rank_genes_groups` and `sc.pl.violin` any volunteers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:40,security,updat,updated,40,I think it is ok to merge now. . I also updated some of the plotting functions to accept a `gene_symbol` column: . ![image](https://user-images.githubusercontent.com/4964309/52279718-85cb4f00-295a-11e9-99e9-f9b8648609a6.png). What is missing is `sc.pl.rank_genes_groups` and `sc.pl.violin` any volunteers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:132,usability,user,user-images,132,I think it is ok to merge now. . I also updated some of the plotting functions to accept a `gene_symbol` column: . ![image](https://user-images.githubusercontent.com/4964309/52279718-85cb4f00-295a-11e9-99e9-f9b8648609a6.png). What is missing is `sc.pl.rank_genes_groups` and `sc.pl.violin` any volunteers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:55,deployability,log,logging,55,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:80,energy efficiency,current,currently,80,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:168,performance,time,time,168,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:55,safety,log,logging,55,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:176,safety,review,review,176,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:55,security,log,logging,55,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:55,testability,log,logging,55,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:176,testability,review,review,176,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:102,usability,learn,learned,102,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didnt do it yet. I didnt have time to review the whole thing, but if yall want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:152,deployability,log,logging,152,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:179,energy efficiency,current,currently,179,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:270,performance,time,time,270,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:152,safety,log,logging,152,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:278,safety,review,review,278,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:152,security,log,logging,152,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:597,security,auth,auth,597,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:152,testability,log,logging,152,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:278,testability,review,review,278,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:201,usability,learn,learned,201,"Please go ahead! On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes. > Alex is currently a bit ill I learned, which is why he probably didnt do. > it yet. I didnt have time to review the whole thing, but if yall want I. > can do that too. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:98,deployability,updat,updated,98,Thanks a lot. All of these new features are what we need! I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:98,safety,updat,updated,98,Thanks a lot. All of these new features are what we need! I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:98,security,updat,updated,98,Thanks a lot. All of these new features are what we need! I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:236,usability,tool,tools,236,Thanks a lot. All of these new features are what we need! I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:28,deployability,updat,updated,28,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:410,deployability,updat,updated,410,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:28,safety,updat,updated,28,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:410,safety,updat,updated,410,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:28,security,updat,updated,28,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:410,security,updat,updated,410,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:896,security,auth,auth,896,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:107,usability,visual,visualizing-marker-genes,107,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/pull/425:554,usability,tool,tools,554,"Recently, this tutorial was updated with what you need:. https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from. https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>. wrote:. > Thanks a lot. All of these new features are what we need! >. > I notice that the tutorial has not been updated yet (such as. > sc.tl.filter_rank_genes_groups( ) and rna velocity function in. > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find. > these features occasionally. Could you add them in scanpy tutorial ? >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425
https://github.com/scverse/scanpy/issues/426:143,modifiability,layer,layers,143,"Yeah, AnnData doesnt serialize arbitrary attributes to disk. I assume the output of `fit_transform` is cellgene? Then you could do `all_data.layers['magic'] = ...`. If the output is `celly` with `y != n_genes` then you should do `all_data.obsm['magic'] = ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:56,performance,disk,disk,56,"Yeah, AnnData doesnt serialize arbitrary attributes to disk. I assume the output of `fit_transform` is cellgene? Then you could do `all_data.layers['magic'] = ...`. If the output is `celly` with `y != n_genes` then you should do `all_data.obsm['magic'] = ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:14,reliability,doe,doesn,14,"Yeah, AnnData doesnt serialize arbitrary attributes to disk. I assume the output of `fit_transform` is cellgene? Then you could do `all_data.layers['magic'] = ...`. If the output is `celly` with `y != n_genes` then you should do `all_data.obsm['magic'] = ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:254,integrability,sub,subscribed,254,"FYI, you can use magic within scanpy using `sc.pp.magic`. On Tue, Jan 15, 2019 at 9:25 AM Samuele Soraggi <notifications@github.com>. wrote:. > Closed #426 <https://github.com/theislab/scanpy/issues/426>. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/426#event-2073913727>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RuGVf_KelSo-2UQap_4vXsAW5Qyks5vDZB-gaJpZM4Z-cf8>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:381,integrability,event,event-,381,"FYI, you can use magic within scanpy using `sc.pp.magic`. On Tue, Jan 15, 2019 at 9:25 AM Samuele Soraggi <notifications@github.com>. wrote:. > Closed #426 <https://github.com/theislab/scanpy/issues/426>. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/426#event-2073913727>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RuGVf_KelSo-2UQap_4vXsAW5Qyks5vDZB-gaJpZM4Z-cf8>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:471,security,auth,auth,471,"FYI, you can use magic within scanpy using `sc.pp.magic`. On Tue, Jan 15, 2019 at 9:25 AM Samuele Soraggi <notifications@github.com>. wrote:. > Closed #426 <https://github.com/theislab/scanpy/issues/426>. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/426#event-2073913727>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RuGVf_KelSo-2UQap_4vXsAW5Qyks5vDZB-gaJpZM4Z-cf8>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/426:144,usability,Close,Closed,144,"FYI, you can use magic within scanpy using `sc.pp.magic`. On Tue, Jan 15, 2019 at 9:25 AM Samuele Soraggi <notifications@github.com>. wrote:. > Closed #426 <https://github.com/theislab/scanpy/issues/426>. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/426#event-2073913727>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RuGVf_KelSo-2UQap_4vXsAW5Qyks5vDZB-gaJpZM4Z-cf8>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426
https://github.com/scverse/scanpy/issues/428:355,deployability,updat,updated,355,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:409,deployability,api,api,409,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:409,integrability,api,api,409,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:409,interoperability,api,api,409,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:355,safety,updat,updated,355,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:355,security,updat,updated,355,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/429:21,deployability,version,versions,21,"@falexwolf . Hm, new versions don't have `counts_per_cell ` at all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:21,integrability,version,versions,21,"@falexwolf . Hm, new versions don't have `counts_per_cell ` at all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:21,modifiability,version,versions,21,"@falexwolf . Hm, new versions don't have `counts_per_cell ` at all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:268,deployability,version,version,268,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:268,integrability,version,version,268,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:268,modifiability,version,version,268,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:170,testability,simpl,simpler,170,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:170,usability,simpl,simpler,170,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:111,deployability,modul,module,111,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:285,integrability,filter,filtering,285,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:111,modifiability,modul,module,111,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:111,safety,modul,module,111,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:133,security,modif,modifying,133,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:203,testability,context,context,203,"Totally fine. Was just wondering whether we can make it work as expected for the deprecated normalize_per_cell module, which is just modifying a single line (PR?). My use case: I have found that, in the context of RNA velocity analysis, normalizing by initial sizes (cell sizes before filtering) can be quite important. That is where I used to pass the `counts_per_cell` attribute. Here, I made it the default:. https://github.com/theislab/scvelo/blob/c86e4530485e6e62a055c5a9285177b9554613ce/scvelo/preprocessing/utils.py#L256",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:352,integrability,filter,filtering,352,"I find normalising by initial cell sizes quite important. That's the only use case for me where I use `counts_per_cell`. And I believe that applies to many others as well. Thus, one might think of having a bool option such as `by_initial_size` instead. What do you think, @falexwolf ? In that case, one would have to store the initial cell size before filtering (either when doing the gene filtering or even when reading the data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:390,integrability,filter,filtering,390,"I find normalising by initial cell sizes quite important. That's the only use case for me where I use `counts_per_cell`. And I believe that applies to many others as well. Thus, one might think of having a bool option such as `by_initial_size` instead. What do you think, @falexwolf ? In that case, one would have to store the initial cell size before filtering (either when doing the gene filtering or even when reading the data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:78,energy efficiency,estimat,estimate,78,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:589,energy efficiency,estimat,estimator,589,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:724,testability,simpl,simple,724,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:724,usability,simpl,simple,724,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:343,deployability,log,logical,343,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:68,integrability,filter,filtering,68,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:147,integrability,filter,filter,147,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:243,integrability,filter,filtering,243,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:309,integrability,filter,filtering,309,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:343,safety,log,logical,343,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:343,security,log,logical,343,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:343,testability,log,logical,343,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:278,usability,minim,minimal,278,"I was wondering if using the initial `total_counts` versus the post-filtering `total_counts` really matter that much. In the end we typically only filter out genes that have very few counts, so that the difference between the initial and post-filtering `total_counts` should be minimal. Principally using pre-filtering values is probably more logical, although I'm not sure it really changes anything. I wonder how hard it would be to put scran's size factor calculation into python... that might be a good HiWi project.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:129,integrability,filter,filtered,129,"It does not matter much in most cases. Just in the mentioned use case of RNA velocity analysis it does as many cells usually get filtered out because of low unspliced count nums. Whatsoever, it is no effort to compute initial sizes beforehand and you are ensured to do the ""right thing"", no matter how much you've filtered out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:314,integrability,filter,filtered,314,"It does not matter much in most cases. Just in the mentioned use case of RNA velocity analysis it does as many cells usually get filtered out because of low unspliced count nums. Whatsoever, it is no effort to compute initial sizes beforehand and you are ensured to do the ""right thing"", no matter how much you've filtered out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:3,reliability,doe,does,3,"It does not matter much in most cases. Just in the mentioned use case of RNA velocity analysis it does as many cells usually get filtered out because of low unspliced count nums. Whatsoever, it is no effort to compute initial sizes beforehand and you are ensured to do the ""right thing"", no matter how much you've filtered out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:98,reliability,doe,does,98,"It does not matter much in most cases. Just in the mentioned use case of RNA velocity analysis it does as many cells usually get filtered out because of low unspliced count nums. Whatsoever, it is no effort to compute initial sizes beforehand and you are ensured to do the ""right thing"", no matter how much you've filtered out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:30,integrability,filter,filtering,30,Also new functions don't have filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:54,testability,simpl,simply,54,"That's good to know! That means, `filter_genes` would simply annotate the genes kept just like `highly_variable_genes`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:54,usability,simpl,simply,54,"That's good to know! That means, `filter_genes` would simply annotate the genes kept just like `highly_variable_genes`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:24,integrability,filter,filtering,24,"@VolkerBergen If you're filtering out cells, you wouldn't have different total counts per cell, right? Only if you filter out genes. Or is that what you meant?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:115,integrability,filter,filter,115,"@VolkerBergen If you're filtering out cells, you wouldn't have different total counts per cell, right? Only if you filter out genes. Or is that what you meant?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:33,integrability,filter,filtering,33,"@VolkerBergen . Hm, i also meant filtering of cells. No filtering of cells in `normalize_{total, quantile}`. What do you mean by filtering of genes in relation to normalization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:56,integrability,filter,filtering,56,"@VolkerBergen . Hm, i also meant filtering of cells. No filtering of cells in `normalize_{total, quantile}`. What do you mean by filtering of genes in relation to normalization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:129,integrability,filter,filtering,129,"@VolkerBergen . Hm, i also meant filtering of cells. No filtering of cells in `normalize_{total, quantile}`. What do you mean by filtering of genes in relation to normalization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:0,integrability,Filter,Filtering,0,"Filtering out genes would obviously change the cell sizes. Hence, with the argument `by_initial_size` it was meant that initial sizes (before filtering) are used. But when `filter_genes` only annotates instead of actually filtering, it does not matter indeed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:142,integrability,filter,filtering,142,"Filtering out genes would obviously change the cell sizes. Hence, with the argument `by_initial_size` it was meant that initial sizes (before filtering) are used. But when `filter_genes` only annotates instead of actually filtering, it does not matter indeed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:222,integrability,filter,filtering,222,"Filtering out genes would obviously change the cell sizes. Hence, with the argument `by_initial_size` it was meant that initial sizes (before filtering) are used. But when `filter_genes` only annotates instead of actually filtering, it does not matter indeed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:236,reliability,doe,does,236,"Filtering out genes would obviously change the cell sizes. Hence, with the argument `by_initial_size` it was meant that initial sizes (before filtering) are used. But when `filter_genes` only annotates instead of actually filtering, it does not matter indeed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:90,testability,simpl,simple,90,"@LuckyMD: Yes, scran's size factor calculation would be very nice-to-have and should be a simple task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:90,usability,simpl,simple,90,"@LuckyMD: Yes, scran's size factor calculation would be very nice-to-have and should be a simple task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:48,integrability,topic,topics,48,I will put it on my long list of potential HiWi topics...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:138,modifiability,paramet,parameter,138,"Hi, I am new to scanpy and trying to use scanpy.pp.normalize_total to normalize my dataset. I saw in the documentation saying 'key_added' parameter will store the normalization factor (size factor). However, the result in the new column stores the same value as total counts before normalization. . Are this function using similar strategy as scran? Do we have normalization factors stored somewhere else? . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:105,usability,document,documentation,105,"Hi, I am new to scanpy and trying to use scanpy.pp.normalize_total to normalize my dataset. I saw in the documentation saying 'key_added' parameter will store the normalization factor (size factor). However, the result in the new column stores the same value as total counts before normalization. . Are this function using similar strategy as scran? Do we have normalization factors stored somewhere else? . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:329,reliability,doe,doesn,329,"Hi @saberyzy,. The size factor for counts per million normalization (the default for `sc.pp.normalize_total`) is the total counts. That value is used to compute the normalized values (X/total_counts * factor; where the factor can be 10,000 or the median total counts across cells), so it is the correct size factor. The function doesn't do anything fancier at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/issues/429:183,energy efficiency,current,current,183,"Hi, @LuckyMD,. Thanks for the answer! In your tutorial paper you recommended to use scran for normalization. It seems like scanpy do not have a similar normalization function. So the current best way is still use a docker to use scran to normalize the data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429
https://github.com/scverse/scanpy/pull/430:136,testability,simpl,simply,136,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430
https://github.com/scverse/scanpy/pull/430:34,usability,help,helpful,34,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430
https://github.com/scverse/scanpy/pull/430:82,usability,user,user,82,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430
https://github.com/scverse/scanpy/pull/430:136,usability,simpl,simply,136,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430
https://github.com/scverse/scanpy/issues/432:20,availability,error,error,20,I ran into the same error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:20,performance,error,error,20,I ran into the same error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:20,safety,error,error,20,I ran into the same error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:20,usability,error,error,20,I ran into the same error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:107,availability,error,error,107,"I have larger number of cells than 50. but sc.tl.pca(adata, use_highly_variable_genes = False) resolved my error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:107,performance,error,error,107,"I have larger number of cells than 50. but sc.tl.pca(adata, use_highly_variable_genes = False) resolved my error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:107,safety,error,error,107,"I have larger number of cells than 50. but sc.tl.pca(adata, use_highly_variable_genes = False) resolved my error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:107,usability,error,error,107,"I have larger number of cells than 50. but sc.tl.pca(adata, use_highly_variable_genes = False) resolved my error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:31,integrability,compon,components,31,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:31,interoperability,compon,components,31,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:31,modifiability,compon,components,31,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:113,modifiability,variab,variable,113,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/434:273,energy efficiency,load,loaded,273,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:310,modifiability,pac,package,310,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:24,performance,memor,memory,24,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:273,performance,load,loaded,273,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:285,performance,memor,memory,285,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:344,performance,memor,memory,344,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:382,reliability,doe,doesn,382,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:95,safety,test,test,95,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:245,safety,except,except,245,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:95,testability,test,test,95,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:24,usability,memor,memory,24,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:193,usability,behavi,behavior,193,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:285,usability,memor,memory,285,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:344,usability,memor,memory,344,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:359,usability,custom,custom,359,"Well, but the amount of memory should be a lot smaller than if you used. ```. adata = sc.read('test.h5ad'). ```. There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:41,integrability,coupl,couple,41,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:87,interoperability,standard,standard,87,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:41,modifiability,coupl,couple,41,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:330,performance,memor,memory,330,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:377,performance,memor,memory,377,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:424,performance,memor,memory,424,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:471,performance,memor,memory,471,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:518,performance,memor,memory,518,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:565,performance,memor,memory,565,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:612,performance,memor,memory,612,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:659,performance,memor,memory,659,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:706,performance,memor,memory,706,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:753,performance,memor,memory,753,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:917,performance,memor,memory,917,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:964,performance,memor,memory,964,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1011,performance,memor,memory,1011,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1058,performance,memor,memory,1058,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1105,performance,memor,memory,1105,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1152,performance,memor,memory,1152,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1199,performance,memor,memory,1199,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1246,performance,memor,memory,1246,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1293,performance,memor,memory,1293,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1340,performance,memor,memory,1340,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1405,performance,memor,memory,1405,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1617,performance,memor,memory,1617,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1757,performance,memor,memory,1757,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:96,safety,test,test,96,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1554,security,session,session,1554,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1675,security,session,session,1675,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:41,testability,coupl,couple,41,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:96,testability,test,test,96,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:18,usability,behavi,behavior,18,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:330,usability,memor,memory,330,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:377,usability,memor,memory,377,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:424,usability,memor,memory,424,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:471,usability,memor,memory,471,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:518,usability,memor,memory,518,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:565,usability,memor,memory,565,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:612,usability,memor,memory,612,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:659,usability,memor,memory,659,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:706,usability,memor,memory,706,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:753,usability,memor,memory,753,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:917,usability,memor,memory,917,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:964,usability,memor,memory,964,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1011,usability,memor,memory,1011,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1058,usability,memor,memory,1058,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1105,usability,memor,memory,1105,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1152,usability,memor,memory,1152,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1199,usability,memor,memory,1199,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1246,usability,memor,memory,1246,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1293,usability,memor,memory,1293,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1340,usability,memor,memory,1340,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1405,usability,memor,memory,1405,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1617,usability,memor,memory,1617,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1757,usability,memor,memory,1757,"I've noticed this behavior too. Here's a couple examples:. ```python. # First, using a standard test dataset. In [1]: %load_ext memory_profiler . In [2]: import scanpy as sc . In [3]: adatas = [] . In [4]: adatas_backed = [] . In [5]: for i in range(10): . ...: %memit adatas.append(sc.read(""data/pbmc3k_raw.h5ad"")) . ...: . peak memory: 223.52 MiB, increment: 48.47 MiB. peak memory: 275.66 MiB, increment: 52.14 MiB. peak memory: 319.36 MiB, increment: 43.71 MiB. peak memory: 361.41 MiB, increment: 42.04 MiB. peak memory: 403.64 MiB, increment: 42.24 MiB. peak memory: 446.02 MiB, increment: 42.37 MiB. peak memory: 488.57 MiB, increment: 42.56 MiB. peak memory: 530.31 MiB, increment: 41.74 MiB. peak memory: 573.53 MiB, increment: 43.21 MiB. peak memory: 615.81 MiB, increment: 42.29 MiB. In [6]: for i in range(10): . ...: %memit adatas_backed.append(sc.read(""data/pbmc3k_raw.h5ad"", backed=""r"")) . ...: . peak memory: 658.04 MiB, increment: 42.07 MiB. peak memory: 700.22 MiB, increment: 42.19 MiB. peak memory: 742.49 MiB, increment: 42.27 MiB. peak memory: 784.62 MiB, increment: 42.14 MiB. peak memory: 827.00 MiB, increment: 42.38 MiB. peak memory: 869.21 MiB, increment: 42.21 MiB. peak memory: 911.36 MiB, increment: 42.14 MiB. peak memory: 953.34 MiB, increment: 41.98 MiB. peak memory: 996.37 MiB, increment: 43.03 MiB. peak memory: 1038.57 MiB, increment: 42.20 MiB. In [7]: %memit . peak memory: 1038.62 MiB, increment: -0.09 MiB. In [8]: sc.__version__ . Out[8]: '1.3.7+59.ge0d2ea6'. ```. Using a larger dataset:. ```python. # In a new session:. In [4]: %memit adata = sc.read(""tmp_bm.h5ad"") . peak memory: 2975.57 MiB, increment: 2799.25 MiB. # In another session:. In [4]: %memit adata_backed = sc.read(""tmp_bm.h5ad"", backed=""r"") . peak memory: 2969.57 MiB, increment: 2794.57 MiB. ```. While making those examples I got a range of results, though what I've posted are the ones I saw most often. It's enough to make me think there's something strange going on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:272,deployability,log,log,272,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1713,deployability,version,version,1713,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1781,deployability,log,logging,1781,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1713,integrability,version,version,1713,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1713,modifiability,version,version,1713,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:104,performance,memor,memory,104,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:504,performance,memor,memory,504,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:618,performance,memor,memory,618,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:724,performance,memor,memory,724,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:272,safety,log,log,272,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:358,safety,test,test,358,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:486,safety,test,test,486,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:588,safety,test,test,588,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:706,safety,test,test,706,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1781,safety,log,logging,1781,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:85,security,sign,significantly,85,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:272,security,log,log,272,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1781,security,log,logging,1781,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:272,testability,log,log,272,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:358,testability,test,test,358,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:486,testability,test,test,486,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:588,testability,test,test,588,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:706,testability,test,test,706,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1240,testability,verif,verification,1240,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1781,testability,log,logging,1781,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:58,usability,custom,custom,58,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:104,usability,memor,memory,104,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:504,usability,memor,memory,504,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:618,usability,memor,memory,618,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:724,usability,memor,memory,724,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1257,usability,custom,custom,1257,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:1886,usability,learn,learn,1886,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```. adata = sc.read_10x_h5(""source.h5"", ""GRCh38""). adata.obs['n_counts'] = adata.X.sum(1). adata.obs['log_counts'] = np.log(adata.obs['n_counts']). adata.obs['n_genes'] = (adata.X > 0).sum(1). adata.write(""test.h5ad"", compression='gzip', compression_opts=1). ```. Then read it three different ways:. ```. In [7]: %memit a1 = sc.read(""test.h5ad""). peak memory: 2333.84 MiB, increment: 2170.77 MiB. ```. ```. In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""). peak memory: 4400.07 MiB, increment: 2137.85 MiB. ```. ```. In [9]: %memit a3 = custom_read(""test.h5ad""). peak memory: 4390.11 MiB, increment: 66.90 MiB. ```. where `custom_read` is defined as:. ```. def custom_read(filename):. adata = AnnData(). hf = h5py.File(filename). adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]. adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]. adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]. adata.obs_names = [x[0] for x in hf['obs'][()]]. adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]. adata.var_names = [x[0] for x in hf['var'][()]]. return adata. ```. and here's some verification the custom read is actually reading the data. ```. In [16]: a2.obs_keys(). Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(). Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs). Out[18]: 384000. In [19]: len(a3.obs). Out[19]: 384000. In [20]: a2.var_keys(). Out[20]: ['gene_ids']. In [21]: a3.var_keys(). Out[21]: ['gene_ids']. In [22]: len(a2.var). Out[22]: 33694. In [23]: len(a3.var). Out[23]: 33694. ```. Here's some version info I neglected in the original comment:. ```. In [28]: sc.logging.print_versions(). scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:46,deployability,observ,observation,46,"@Koncopd, this seems to be not related to the observation you mentioned a couple of days ago: fixed-length strings in categories... Can you please look into it? Something seems to be fundamentally wrong (very likely I'm responsible for that).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:74,integrability,coupl,couple,74,"@Koncopd, this seems to be not related to the observation you mentioned a couple of days ago: fixed-length strings in categories... Can you please look into it? Something seems to be fundamentally wrong (very likely I'm responsible for that).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:74,modifiability,coupl,couple,74,"@Koncopd, this seems to be not related to the observation you mentioned a couple of days ago: fixed-length strings in categories... Can you please look into it? Something seems to be fundamentally wrong (very likely I'm responsible for that).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:46,testability,observ,observation,46,"@Koncopd, this seems to be not related to the observation you mentioned a couple of days ago: fixed-length strings in categories... Can you please look into it? Something seems to be fundamentally wrong (very likely I'm responsible for that).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:74,testability,coupl,couple,74,"@Koncopd, this seems to be not related to the observation you mentioned a couple of days ago: fixed-length strings in categories... Can you please look into it? Something seems to be fundamentally wrong (very likely I'm responsible for that).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:157,performance,memor,memory,157,"Thank you, both! It very likely wasn't there in the beginning and I probably messed it up at some point. It seems that I should make a test that checks that memory usage behaves properly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:135,safety,test,test,135,"Thank you, both! It very likely wasn't there in the beginning and I probably messed it up at some point. It seems that I should make a test that checks that memory usage behaves properly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:135,testability,test,test,135,"Thank you, both! It very likely wasn't there in the beginning and I probably messed it up at some point. It seems that I should make a test that checks that memory usage behaves properly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:157,usability,memor,memory,157,"Thank you, both! It very likely wasn't there in the beginning and I probably messed it up at some point. It seems that I should make a test that checks that memory usage behaves properly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/435:200,availability,error,error,200,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:283,availability,error,error,283,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:12,deployability,manag,managed,12,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:31,deployability,log,log,31,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:12,energy efficiency,manag,managed,12,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:311,energy efficiency,Load,Load,311,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:418,integrability,filter,filter,418,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:510,integrability,Filter,Filter,510,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:200,performance,error,error,200,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:283,performance,error,error,283,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:311,performance,Load,Load,311,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:165,reliability,doe,does,165,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:12,safety,manag,managed,12,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:31,safety,log,log,31,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:200,safety,error,error,200,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:283,safety,error,error,283,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:31,security,log,log,31,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
