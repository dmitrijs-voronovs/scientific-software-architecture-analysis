id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/96:345,energy efficiency,model,model,345,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,integrability,version,version-,94,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:241,interoperability,compatib,compatible,241,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,modifiability,version,version-,94,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:119,modifiability,concern,concerned,119,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:140,safety,updat,updates,140,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:140,security,updat,updates,140,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:345,security,model,model,345,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:119,testability,concern,concerned,119,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:78,energy efficiency,model,model,78,"Thanks! We're submitting a paper soon, and I'm hoping to incorporate a stable model into it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:14,integrability,sub,submitting,14,"Thanks! We're submitting a paper soon, and I'm hoping to incorporate a stable model into it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:78,security,model,model,78,"Thanks! We're submitting a paper soon, and I'm hoping to incorporate a stable model into it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,availability,avail,available,94,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:4,energy efficiency,current,current,4,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:12,energy efficiency,model,model,12,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:154,energy efficiency,model,model,154,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:226,energy efficiency,model,model-independent,226,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,reliability,availab,available,94,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,safety,avail,available,94,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:12,security,model,model,12,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:94,security,availab,available,94,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:154,security,model,model,154,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:226,security,model,model-independent,226,"The current model is stable and has been successfully used in many instances. It will also be available in Scanpy 1.2. In addition, there will be another model. General improvements only regard the ease of use of PAGA and are model-independent anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:13,energy efficiency,model,model,13,Will the new model also be stable and described in the manuscript/documentation? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:13,security,model,model,13,Will the new model also be stable and described in the manuscript/documentation? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:66,usability,document,documentation,66,Will the new model also be stable and described in the manuscript/documentation? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/97:427,availability,sli,slight,427,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:463,availability,redund,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:42,deployability,instal,installed,42,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:330,deployability,instal,installed,330,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:454,deployability,API,API,454,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:463,deployability,redundan,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:376,energy efficiency,current,currently,376,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:454,integrability,API,API,454,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:454,interoperability,API,API,454,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:13,performance,network,networkx,13,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:341,performance,network,networkx,341,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:427,reliability,sli,slight,427,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:463,reliability,redundan,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:463,safety,redund,redundant,463,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:13,security,network,networkx,13,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:341,security,network,networkx,341,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:257,usability,document,document,257,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:161,deployability,instal,installation,161,Graph_tool library would be even better and it also implements SBM and many other things that may be useful in graph analysis of single cells. Unfortunately its installation is a painful experience.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:187,usability,experien,experience,187,Graph_tool library would be even better and it also implements SBM and many other things that may be useful in graph analysis of single cells. Unfortunately its installation is a painful experience.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:61,deployability,instal,installation,61,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:126,deployability,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:126,integrability,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:126,modifiability,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:126,safety,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:126,testability,depend,dependency,126,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:25,deployability,modul,module,25,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:99,deployability,build,builds,99,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:25,modifiability,modul,module,25,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:109,performance,network,networkx,109,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:184,performance,Network,NetworkX,184,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:25,safety,modul,module,25,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:109,security,network,networkx,109,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:184,security,Network,NetworkX,184,"@flying-sheep [community module](http://python-louvain.readthedocs.io/en/latest/index.html), which builds on networkx, has a more than decent implementation of Louvain's method. Also, NetworkX 2.1 has been improved over its predecessors. Why dropping igraph isn't an option?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:619,energy efficiency,draw,drawing,619,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:791,modifiability,pac,package,791,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:663,performance,network,networkx,663,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:699,performance,network,networkx,699,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:731,performance,perform,performant,731,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:674,safety,compl,completely,674,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:663,security,network,networkx,663,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:674,security,compl,completely,674,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:699,security,network,networkx,699,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:258,testability,simpl,simply,258,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:806,testability,simpl,simply,806,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:258,usability,simpl,simply,258,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:440,usability,tool,tools,440,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:731,usability,perform,performant,731,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:806,usability,simpl,simply,806,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:131,availability,avail,available,131,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:516,availability,avail,available,516,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:330,deployability,modul,module,330,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:460,deployability,modul,modules,460,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:376,interoperability,specif,specific,376,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:221,modifiability,paramet,parameter,221,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:330,modifiability,modul,module,330,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:460,modifiability,modul,modules,460,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:434,performance,network,networkx,434,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:131,reliability,availab,available,131,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:516,reliability,availab,available,516,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:131,safety,avail,available,131,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:330,safety,modul,module,330,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:460,safety,modul,modules,460,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:516,safety,avail,available,516,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:131,security,availab,available,131,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:434,security,network,networkx,434,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:516,security,availab,available,516,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:353,testability,understand,understand,353,"Hello again, some months later. Unfortunately I haven’t used scanpy for a while and now I’m back I found that only two flavors are available for Louvain processing, `igraph` and `vtraag`. The latter allows for resolution parameter but relies on `python-louvain`, right? the reason for my “but” is that I’m having issues with that module on OSX, which I understand it is not a specific scanpy issue. I’ll ask again: what’s wrong with `networkx` and `community` modules? I’m asking because flavor `taynaud` is no more available",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:221,deployability,modul,modules,221,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:305,deployability,scale,scale,305,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:428,deployability,instal,installed,428,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:496,deployability,instal,installation,496,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:578,deployability,instal,installation,578,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:305,energy efficiency,scale,scale,305,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:221,modifiability,modul,modules,221,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:261,modifiability,paramet,parameter,261,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:305,modifiability,scal,scale,305,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:212,performance,network,networkx,212,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:305,performance,scale,scale,305,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:221,safety,modul,modules,221,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:212,security,network,networkx,212,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:135,usability,tool,tools,135,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:104,deployability,modul,modules,104,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:116,deployability,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:166,deployability,instal,installation,166,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:116,integrability,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:35,modifiability,paramet,parameter,35,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:104,modifiability,modul,modules,104,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:116,modifiability,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:104,safety,modul,modules,104,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:116,safety,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:116,testability,depend,dependencies,116,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/98:5,energy efficiency,cool,cool,5,"Yes, cool, let's do this as soon as possible. First need to finish scanpy 1.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/98
https://github.com/scverse/scanpy/pull/100:113,availability,down,downsampling,113,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:126,deployability,observ,observations,126,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:168,deployability,observ,observations,168,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:156,integrability,sub,subsampling,156,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:126,testability,observ,observations,126,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:168,testability,observ,observations,168,"thank you very much! one thing that we could consider is renaming this to ""downsample_counts""; for some people, ""downsampling observations"" is an alias to ""subsampling observations"" and for these the function name is not descriptive enough. what do you think? can I make this change?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:33,availability,error,error,33,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:185,availability,error,error,185,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:33,performance,error,error,33,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:185,performance,error,error,185,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:33,safety,error,error,33,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:185,safety,error,error,185,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:33,usability,error,error,33,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:185,usability,error,error,185,"Also, you may want to change the error that is shown if a non-AnnData object is passed to the function. I put it as TypeError, because I didn't find how you have previously called that error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/issues/101:259,deployability,releas,released,259,"Yes, I know about an issue that is probably related to that: At some point in `add_or_update_graph_in_adata`, numpy takes more cores than it's supposed to - that's the only instance in the whole of Scanpy. Otherwise it's well-behaved. When Scanpy 1.0 will be released in the next few days, this will be resolved. Do you think this will do the job for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/101:127,energy efficiency,core,cores,127,"Yes, I know about an issue that is probably related to that: At some point in `add_or_update_graph_in_adata`, numpy takes more cores than it's supposed to - that's the only instance in the whole of Scanpy. Otherwise it's well-behaved. When Scanpy 1.0 will be released in the next few days, this will be resolved. Do you think this will do the job for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/101:71,deployability,version,version,71,"Yes that will solve the problem. Thank you, looking forward to the new version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/101:71,integrability,version,version,71,"Yes that will solve the problem. Thank you, looking forward to the new version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/101:71,modifiability,version,version,71,"Yes that will solve the problem. Thank you, looking forward to the new version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/102:61,deployability,contain,contain,61,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:160,deployability,version,version,160,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:160,integrability,version,version,160,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:187,integrability,coupl,couple,187,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:160,modifiability,version,version,160,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:187,modifiability,coupl,couple,187,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:187,testability,coupl,couple,187,"Sorry, that's probably due to the fact that your annotations contain a column with a categorical data and a *single* category. We'll solve this bug in the next version of anndata, just a couple of days. As workaround, remove the annotation with a single cateogry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:18,usability,close,close,18,thank you. please close this issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:36,deployability,version,version,36,do you also have the newest anndata version?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:36,integrability,version,version,36,do you also have the newest anndata version?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:36,modifiability,version,version,36,do you also have the newest anndata version?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:414,availability,error,error,414,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:592,deployability,modul,module,592,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1361,deployability,log,logg,1361,"d'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:4030,deployability,version,versions,4030,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1569,energy efficiency,load,load,1569,"-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2044,energy efficiency,core,core,2044,"anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __ini",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2328,energy efficiency,core,core,2328,"backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2698,energy efficiency,core,core,2698,"structor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3016,energy efficiency,core,core,3016,"3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3294,energy efficiency,core,core,3294,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3592,energy efficiency,core,core,3592,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:420,integrability,messag,message,420,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:4030,integrability,version,versions,4030,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:420,interoperability,messag,message,420,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3807,interoperability,format,format,3807,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:592,modifiability,modul,module,592,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:690,modifiability,pac,packages,690,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1088,modifiability,pac,packages,1088,"port scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1473,modifiability,pac,packages,1473,"---------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1789,modifiability,pac,packages,1789,"p_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2027,modifiability,pac,packages,2027,"ct. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dty",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2312,modifiability,pac,packages,2312,"5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:2682,modifiability,pac,packages,2682,"). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3000,modifiability,pac,packages,3000,"xpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3278,modifiability,pac,packages,3278,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3576,modifiability,pac,packages,3576,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3726,modifiability,Paramet,Parameter,3726,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:3949,modifiability,Paramet,Parameter,3949,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:4030,modifiability,version,versions,4030,"_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ordered). 325 . 326 return dtype. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered). 224 . 225 def __init__(self, categories=None, ordered=None):. --> 226 self._finalize(categories, ordered, fastpath=False). 227 . 228 @classmethod. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath). 333 if categories is not None:. 334 categories = self.validate_categories(categories,. --> 335 fastpath=fastpath). 336 . 337 self._categories = categories. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath). 502 if not fastpath and not is_list_like(categories):. 503 msg = ""Parameter 'categories' must be list-like, was {!r}"". --> 504 raise TypeError(msg.format(categories)). 505 elif not isinstance(categories, ABCIndexClass):. 506 categories = Index(categories, tupleize_cols=False). TypeError: Parameter 'categories' must be list-like, was 1. ```. I am running the following versions:. ``` . 0.24.2 #pandas. 1.4.4.post1 #scanpy. 0.6.22.post1 #anndata. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:414,performance,error,error,414,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:800,performance,cach,cache,800,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:964,performance,cach,cache,964,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:970,performance,cach,cache,970,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1199,performance,cach,cache,1199,"data = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1569,performance,load,load,1569,"-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1590,performance,memor,memory,1590,"odule>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories mus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:414,safety,error,error,414,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:566,safety,input,input-,566,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:592,safety,modul,module,592,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1361,safety,log,logg,1361,"d'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1361,security,log,logg,1361,"d'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:522,testability,Trace,Traceback,522,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1361,testability,log,logg,1361,"d'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:10,usability,experien,experiencing,10,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:414,usability,error,error,414,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:566,usability,input,input-,566,"I am also experiencing this issue. Running the following code:. ```. import pandas as pd. import scanpy as sc. import anndata. print(pd.__version__). print(sc.__version__). print(anndata.__version__). adata = sc.datasets.pbmc68k_reduced(). adata.obs[""single_cat""] = 1. adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']). adata.write('/tmp/adata.h5ad'). sc.read('/tmp/adata.h5ad'). ```. Returns this error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-adde38d13544> in <module>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1590,usability,memor,memory,1590,"odule>. ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories mus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/102:1910,usability,close,close,1910,"irst_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 500 if not backed:. 501 f.close(). --> 502 return AnnData._args_from_dict(d). 503 . 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata). 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(. 2183 codes=d_true_keys[ann][k_stripped].values,. -> 2184 categories=v,. 2185 ). 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype). 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,. 639 ordered=ordered,. --> 640 dtype=dtype). 641 if dtype.categories is None:. 642 msg = (""The categories must be provided in 'categories' or "". /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py in _from_values_or_dtype(cls, values, categories, ordered, dtype). 322 # Note: This could potentially have categories=None and. 323 # ordered=None. --> 324 dtype = CategoricalDtype(categories, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/103:139,deployability,api,api,139,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:312,deployability,observ,observations,312,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:338,deployability,observ,observations,338,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:139,integrability,api,api,139,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:139,interoperability,api,api,139,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:541,modifiability,paramet,parameters,541,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:731,modifiability,exten,extend,731,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:312,testability,observ,observations,312,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:338,testability,observ,observations,338,"Hi Dustin! Thank you for providing a template. You can easily read that template in with two additional lines of code:. ```. import scanpy.api as sc. import pandas as pd. adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T. adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'). adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'). ```. I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:80,interoperability,standard,standard,80,"Your example above works just fine. Thank you! . My template is not a universal standard, at least not to my knowledge. For my use case, I'm just trying to provide users a template to populate with their data. At upload I'll convert the xlsx into an AnnData object. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/issues/103:164,usability,user,users,164,"Your example above works just fine. Thank you! . My template is not a universal standard, at least not to my knowledge. For my use case, I'm just trying to provide users a template to populate with their data. At upload I'll convert the xlsx into an AnnData object. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103
https://github.com/scverse/scanpy/pull/104:44,deployability,log,logarithm,44,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:153,deployability,log,logarithm,153,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:44,safety,log,logarithm,44,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:153,safety,log,logarithm,153,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:44,security,log,logarithm,44,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:153,security,log,logarithm,153,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:44,testability,log,logarithm,44,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/pull/104:153,testability,log,logarithm,153,"Thank you! :smile: There is also taking the logarithm of the means involved and I'll add this for clarifcation, but of course you're right... there's no logarithm of the variance taken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/104
https://github.com/scverse/scanpy/issues/106:65,deployability,roll,rolling,65,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:97,interoperability,standard,standard,97,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:125,testability,plan,planned,125,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:13,usability,feedback,feedback,13,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:165,availability,replic,replicate,165,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:193,availability,replic,replicate,193,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:319,availability,replic,replicate,319,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:344,availability,replic,replicate,344,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:428,availability,replic,replicate,428,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:516,availability,replic,replicate,516,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:575,availability,replic,replicate,575,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:840,availability,replic,replicate-data-in-pandas,840,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:86,deployability,api,api,86,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:138,deployability,toggl,toggleswitch,138,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:794,deployability,stack,stackoverflow,794,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:258,energy efficiency,alloc,allocate,258,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:86,integrability,api,api,86,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:86,interoperability,api,api,86,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:271,performance,memor,memory,271,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:249,reliability,doe,does,249,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:712,reliability,Doe,Does,712,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:271,usability,memor,memory,271,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:304,usability,efficien,efficient,304,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:746,usability,Visual,Visualize,746,"Thank you for the reminder, Joshua! :smile:. How about doing this? ```. import scanpy.api as sc. import pandas as pd. adata = sc.datasets.toggleswitch(). adata.obs['replicate'] = 0. adata.obs['replicate'].loc[100:] = 1. df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient. df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made. df_grouped = df.groupby('replicate'). print(df_grouped.mean()). print(df_grouped.std()). ```. outputs. ```. 0 1. replicate . 0 0.510177 0.135317. 1 0.152043 0.439836. 0 1. replicate . 0 0.293965 0.162549. 1 0.153663 0.271669. ```. Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question? PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:483,availability,replic,replicates,483,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:558,availability,replic,replicates,558,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:787,availability,replic,replicates,787,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1072,availability,replic,replicates,1072,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1322,availability,replic,replicates,1322,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:338,integrability,interfac,interfaces,338,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:338,interoperability,interfac,interfaces,338,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1189,interoperability,specif,specifically,1189,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1501,interoperability,prox,proxy,1501,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:338,modifiability,interfac,interfaces,338,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:425,modifiability,concern,concerns,425,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:312,performance,time,time,312,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:97,reliability,doe,does,97,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:817,reliability,Doe,Does,817,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:580,safety,input,input,580,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:392,security,access,access,392,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:57,testability,understand,understand,57,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:425,testability,concern,concerns,425,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:741,testability,Simpl,Simple,741,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:402,usability,prefer,preferred,402,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:580,usability,input,input,580,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:648,usability,user,user-images,648,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:741,usability,Simpl,Simple,741,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:907,usability,user,user-images,907,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/107:16,energy efficiency,current,currently,16,"Hi David,. it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107
https://github.com/scverse/scanpy/issues/107:290,reliability,doe,doesn,290,"Hi David,. it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107
https://github.com/scverse/scanpy/issues/107:144,usability,learn,learning-based,144,"Hi David,. it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107
https://github.com/scverse/scanpy/issues/107:55,usability,close,close,55,Sounds interesting. Looking forward to seeing it! I'll close this in the meantime. Thanks Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107
https://github.com/scverse/scanpy/issues/108:111,deployability,api,api,111,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:122,deployability,api,api,122,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:233,deployability,api,api,233,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:244,deployability,api,api,244,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:280,deployability,automat,automatically,280,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:543,deployability,automat,automatic,543,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:21,energy efficiency,current,currently,21,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:111,integrability,api,api,111,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:122,integrability,api,api,122,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:233,integrability,api,api,233,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:244,integrability,api,api,244,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:111,interoperability,api,api,111,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:122,interoperability,api,api,122,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:233,interoperability,api,api,233,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:244,interoperability,api,api,244,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:488,performance,time,time,488,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:385,reliability,doe,doesn,385,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:410,security,hack,hack,410,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:280,testability,automat,automatically,280,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:543,testability,automat,automatic,543,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:615,usability,user,user,615,"Yes, agreed. You can currently workaround with [set_rcParams_Defaults](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Defaults.html). Upon import, [set_rcParams_Scanpy](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pl.set_rcParams_Scanpy.html) is automatically called. I had the impression that seaborn still changes the rcParams also after 0.8. If it doesn't I can remove the hack of resetting the params after calling `seaborn.violinplot` for the first time. I agree that in Scanpy 1.0, we should remove the automatic call of set_rcParams_Scanpy and instead require this from the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:48,reliability,doe,doesn,48,"Great! Thanks! :). Just doing a quick check, it doesn't seem like Seaborn affects things:. ![image](https://user-images.githubusercontent.com/668803/37688750-cc1b3d70-2c5e-11e8-937e-f40c6c1776e3.png). Scanpy on the other hand makes the text larger and puts in a grid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:108,usability,user,user-images,108,"Great! Thanks! :). Just doing a quick check, it doesn't seem like Seaborn affects things:. ![image](https://user-images.githubusercontent.com/668803/37688750-cc1b3d70-2c5e-11e8-937e-f40c6c1776e3.png). Scanpy on the other hand makes the text larger and puts in a grid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:79,deployability,version,versions,79,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:169,deployability,build,building,169,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:79,integrability,version,versions,79,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:79,modifiability,version,versions,79,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:14,usability,behavi,behavior,14,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:157,usability,behavi,behavior,157,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:196,usability,behavi,behavior,196,"Hm, yes, such behavior is really frustrating. I was probably using old seaborn versions when coming up with the brilliant idea of fighting back its annoying behavior by building the same annoying behavior into scanpy. Will be resolved in 1.0. Thank you for pointing it out again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:293,deployability,configurat,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it? <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:293,integrability,configur,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it? <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:293,modifiability,configur,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it? <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:293,security,configur,configuration,293,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it? <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:376,usability,user,user-images,376,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it? <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:114,performance,time,time,114,"Ah ok, just looked it up. It's not a bug... :wink: I don't know why I misunderstood this... Must have been a long time ago...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:147,deployability,api,api,147,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:106,energy efficiency,current,current,106,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:147,integrability,api,api,147,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:147,interoperability,api,api,147,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:64,safety,avoid,avoid,64,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:38,testability,context,context,38,"Mm I think I you're supposed to use a context (`with ... :`) to avoid changing things globally. Anyway my current workaround of doing `from scanpy.api import preprocessing as scpp` is not too cumbersome, so should be okay until 1.0 hits :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/108:34,usability,close,close,34,"Scanpy 1.0 is out and hence I can close this, I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/109:348,availability,state,stated,348,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:733,availability,error,error,733,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:823,deployability,observ,observe,823,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:348,integrability,state,stated,348,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:745,integrability,filter,filtering,745,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:733,performance,error,error,733,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:787,reliability,Doe,Does,787,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:733,safety,error,error,733,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:538,security,access,access,538,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:19,testability,understand,understand,19,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:823,testability,observ,observe,823,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:733,usability,error,error,733,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:797,usability,help,help,797,"Dear Olivia,. as I understand, you get a. ```. KeyError: 'Wfdc18'. ```. when calling. ```. adata1 = adata[:, filter_result.gene_subset]. adata1.var.ix['Wfdc18']. ```. Right? So, 'Wfdc18' is no longer `adata1.var_names`. You can also check by typing. ```. print('Wfdc18' in adata1.var_names). ```. which should print `False`. Regarding plotting: as stated in the [basic tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) in box [22], you have to pass `use_raw=False` if you want to access `adata.X` in plotting if `adata.raw` has been set, otherwise it assumes that you want to plot the raw data. ```. sc.pl.pca(adata1, color='Wfdc18', use_raw=False). ```. which will throw an error after filtering if 'Wfdc18' is no longer there. Does this help and explain what you observe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:20,reliability,doe,does,20,"Dear Alex, . Yes it does! . Thank you for your input! O.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:47,safety,input,input,47,"Dear Alex, . Yes it does! . Thank you for your input! O.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:47,usability,input,input,47,"Dear Alex, . Yes it does! . Thank you for your input! O.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/110:203,deployability,releas,release,203,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week. Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/issues/110:217,deployability,version,version,217,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week. Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/issues/110:53,energy efficiency,current,currently,53,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week. Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/issues/110:217,integrability,version,version,217,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week. Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/issues/110:217,modifiability,version,version,217,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week. Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/pull/114:91,availability,robust,robust,91,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:91,reliability,robust,robust,91,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:35,safety,test,test,35,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:91,safety,robust,robust,91,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:35,testability,test,test,35,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:40,testability,coverag,coverage,40,No problem. I think increasing the test coverage should be prioritised to make scanpy more robust.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:97,integrability,abstract,abstraction,97,"Yes... you're of course right. For now, I added the call of diffmaps to a notebook... When graph abstraction is finished, I'll think about systematically adding a lot more tests...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:97,modifiability,abstract,abstraction,97,"Yes... you're of course right. For now, I added the call of diffmaps to a notebook... When graph abstraction is finished, I'll think about systematically adding a lot more tests...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:172,safety,test,tests,172,"Yes... you're of course right. For now, I added the call of diffmaps to a notebook... When graph abstraction is finished, I'll think about systematically adding a lot more tests...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/114:172,testability,test,tests,172,"Yes... you're of course right. For now, I added the call of diffmaps to a notebook... When graph abstraction is finished, I'll think about systematically adding a lot more tests...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/114
https://github.com/scverse/scanpy/pull/116:66,integrability,interfac,interface,66,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:66,interoperability,interfac,interface,66,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:66,modifiability,interfac,interface,66,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:150,integrability,filter,filter,150,"The rendering being broken is a consequence of using HTML to make the readme look nicer on GitHub, and that won’t improve by using markdown. (if they filter HTML-in-rst, they’ll also filter HTML-in-md, right?). Also, what advantage would the switch have? PS: I think for a small, mostly-html-anyway README, md is a good choice, but it’s crap for technical documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:183,integrability,filter,filter,183,"The rendering being broken is a consequence of using HTML to make the readme look nicer on GitHub, and that won’t improve by using markdown. (if they filter HTML-in-rst, they’ll also filter HTML-in-md, right?). Also, what advantage would the switch have? PS: I think for a small, mostly-html-anyway README, md is a good choice, but it’s crap for technical documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:356,usability,document,documentation,356,"The rendering being broken is a consequence of using HTML to make the readme look nicer on GitHub, and that won’t improve by using markdown. (if they filter HTML-in-rst, they’ll also filter HTML-in-md, right?). Also, what advantage would the switch have? PS: I think for a small, mostly-html-anyway README, md is a good choice, but it’s crap for technical documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:47,reliability,doe,doesn,47,"Yeah, actually I don't get why even the header doesn't get rendered properly. I thought it's an rst issue, but you're right maybe md also doesn't make a difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:138,reliability,doe,doesn,138,"Yeah, actually I don't get why even the header doesn't get rendered properly. I thought it's an rst issue, but you're right maybe md also doesn't make a difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:541,deployability,version,version,541,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:590,deployability,version,version,590,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:380,integrability,transform,transform,380,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:541,integrability,version,version,541,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:590,integrability,version,version,590,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:380,interoperability,transform,transform,380,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:240,modifiability,reu,reused,240,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:474,modifiability,reu,reused,474,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:541,modifiability,version,version,541,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:590,modifiability,version,version,590,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/116:651,performance,time,time,651,"Thank you for the note Gökcen! Yes, all this is due to the html snippets... indeed, on GitHub, html in Markdown renders properly and respects, e.g., `width` and would enable the same layout as on http://scanpy.rtfd.io, where the readme is [reused](https://raw.githubusercontent.com/theislab/scanpy/master/docs/index.rst). I don't know about PyPI. In any case, it would be ugly to transform between rst and md. On the other hand, the important thing is that the narrative is reused and the image could be embedded separately using a markdown version for GitHub and PyPi and the present html version for the docs... I'll do this when there is some more time. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116
https://github.com/scverse/scanpy/pull/119:82,deployability,api,api,82,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:0,energy efficiency,Cool,Cool,0,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:82,integrability,api,api,82,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:82,interoperability,api,api,82,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:221,testability,simpl,simpler,221,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:221,usability,simpl,simpler,221,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:51,deployability,api,api,51,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:568,energy efficiency,current,current,568,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:647,energy efficiency,optim,optimally,647,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:719,energy efficiency,load,loaded,719,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:51,integrability,api,api,51,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:51,interoperability,api,api,51,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:359,modifiability,paramet,parameter,359,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:442,modifiability,paramet,parameter,442,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:476,modifiability,extens,extension,476,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:697,modifiability,extens,extension,697,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:795,modifiability,extens,extension,795,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:719,performance,load,loaded,719,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:343,security,hack,hack,343,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:531,security,hack,hack,531,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:661,security,hack,hack,661,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:757,security,hack,hack,757,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:110,testability,simpl,simple,110,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:110,usability,simpl,simple,110,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:312,deployability,api,api,312,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:673,deployability,stage,stage,673,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:312,integrability,api,api,312,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:312,interoperability,api,api,312,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:281,safety,except,except,281,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:228,security,hack,hacks,228,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:763,testability,simpl,simply,763,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:87,usability,person,person,87,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:701,usability,person,person,701,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:763,usability,simpl,simply,763,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:810,usability,learn,learn,810,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:318,deployability,version,version,318,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:423,deployability,api,api,423,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:318,integrability,version,version,318,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:423,integrability,api,api,423,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:423,interoperability,api,api,423,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:834,interoperability,format,format,834,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:318,modifiability,version,version,318,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:1090,modifiability,paramet,parameter,1090,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:393,safety,except,except,393,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:625,safety,prevent,prevented,625,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:281,security,hack,hack,281,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:625,security,preven,prevented,625,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:947,testability,simpl,simply,947,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:89,usability,person,person,89,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:947,usability,simpl,simply,947,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:994,usability,learn,learn,994,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/issues/120:95,deployability,api,api,95,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:106,deployability,api,api,106,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:133,deployability,api,api,133,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:95,integrability,api,api,95,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:106,integrability,api,api,106,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:133,integrability,api,api,133,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:95,interoperability,api,api,95,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:106,interoperability,api,api,106,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:133,interoperability,api,api,133,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:19,modifiability,extens,extensive,19,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:374,modifiability,paramet,parameter,374,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:543,reliability,Doe,Does,543,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:523,testability,simpl,simple-pseudotime,523,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:523,usability,simpl,simple-pseudotime,523,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:553,usability,help,help,553,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:163,modifiability,paramet,parameter,163,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:467,modifiability,pac,package,467,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:266,reliability,doe,doesn,266,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:310,reliability,Doe,Does,310,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:359,reliability,doe,doesn,359,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:338,safety,test,tests,338,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:443,safety,test,tests,443,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:147,security,control,control,147,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:34,testability,understand,understanding,34,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:147,testability,control,control,147,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:338,testability,test,tests,338,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:443,testability,test,tests,443,"Thanks for the quick response. My understanding is that diffusion maps are extremely sensitive to sigma, so I'm curious why you don't allow direct control of this parameter? I also notice the following comment in your source code:. ""choose sigma, the heuristic here doesn't seem to make much of a difference"". Does this mean that in your tests, varying sigma doesn't make much of a difference? If so, I'm curious why this is because in my own tests using the destiny package, choice of sigma matters quite a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:9,usability,document,documentation,9,"From the documentation, it sounds like the default is to use an exponential kernel. Is this correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1385,availability,consist,consistency,1385,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:740,modifiability,paramet,parameters,740,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:807,modifiability,paramet,parameters,807,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1312,modifiability,paramet,parameter,1312,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1078,reliability,doe,does,1078,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1610,reliability,Doe,Does,1610,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:190,safety,compl,completely,190,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:376,safety,compl,completely,376,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:190,security,compl,completely,190,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:376,security,compl,completely,376,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:950,security,control,controls,950,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:950,testability,control,controls,950,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1361,testability,understand,understand,1361,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:795,usability,clear,clear,795,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:842,usability,effectiv,effective,842,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1300,usability,user,user,1300,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1385,usability,consist,consistency,1385,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:1620,usability,help,help,1620,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:272,energy efficiency,measur,measuring,272,"Ah, and yes, the default is the ""exponential kernel"" of UMAP [McInnes18]. While this is again kind of a detail and your results will probably be qualitatively the same when you change the kernel, we have made better experiences with the more recent thoughts of McInnes on measuring connectivity of manifolds...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:216,usability,experien,experiences,216,"Ah, and yes, the default is the ""exponential kernel"" of UMAP [McInnes18]. While this is again kind of a detail and your results will probably be qualitatively the same when you change the kernel, we have made better experiences with the more recent thoughts of McInnes on measuring connectivity of manifolds...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:64,usability,help,help,64,Ah got it. Thanks for the quick response! Really appreciate the help. I am marking this as closed now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/120:91,usability,close,closed,91,Ah got it. Thanks for the quick response! Really appreciate the help. I am marking this as closed now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120
https://github.com/scverse/scanpy/issues/121:3,availability,error,error,3,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:129,availability,error,error,129,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:87,integrability,filter,filter,87,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:3,performance,error,error,3,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:129,performance,error,error,129,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:3,safety,error,error,3,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:129,safety,error,error,129,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:3,usability,error,error,3,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/121:129,usability,error,error,129,"An error jumped out when normalizing, turns out I had a bad dataset mixed in and after filter there were no cell left, hence the error. Thanks!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/121
https://github.com/scverse/scanpy/issues/122:146,availability,avail,available,146,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/122:146,reliability,availab,available,146,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/122:312,reliability,Doe,Does,312,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/122:146,safety,avail,available,146,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/122:146,security,availab,available,146,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/122:322,usability,help,help,322,"Hey! We do it as they do in Seurat. See [here](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), available through the [examples page](http://scanpy.readthedocs.io/en/latest/examples.html):. ```. adata = adata[adata.obs['percent_mito'] < 0.05, :]. ```. in Box 8. Does this help you or do you need more? Best,. lex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/123:240,integrability,sub,subsetting,240,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:350,integrability,sub,submit,350,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:384,integrability,coupl,couple,384,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:384,modifiability,coupl,couple,384,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:277,security,access,access,277,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:384,testability,coupl,couple,384,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:129,usability,learn,learned,129,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:77,deployability,instal,install,77,"If it's what I suspect it to be, it should be fixed with anndata 0.5.9. `pip install anndata==0.5.9`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/pull/125:4,deployability,build,build,4,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:37,deployability,instal,installed,37,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:61,deployability,fail,failed,61,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:22,reliability,doe,doesn,22,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:61,reliability,fail,failed,61,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:40,deployability,contain,contained,40,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:62,deployability,instal,install,62,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:137,deployability,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:246,deployability,instal,installation,246,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:864,deployability,API,API,864,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,deployability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:137,integrability,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:452,integrability,repositor,repository,452,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:556,integrability,coupl,couple,556,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:776,integrability,bridg,bridge,776,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:864,integrability,API,API,864,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,integrability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:452,interoperability,repositor,repository,452,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:776,interoperability,bridg,bridge,776,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:864,interoperability,API,API,864,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,interoperability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:103,modifiability,maintain,maintain,103,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:137,modifiability,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:325,modifiability,maintain,maintainability,325,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:401,modifiability,maintain,maintained,401,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:440,modifiability,extens,extensions,440,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:475,modifiability,pac,package,475,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:556,modifiability,coupl,couple,556,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:662,modifiability,paramet,parametric,662,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,modifiability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,reliability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:103,safety,maintain,maintain,103,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:137,safety,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:288,safety,risk,risk,288,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:325,safety,maintain,maintainability,325,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:401,safety,maintain,maintained,401,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:288,security,risk,risk,288,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:520,security,team,team,520,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,security,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:137,testability,depend,dependencies,137,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:556,testability,coupl,couple,556,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:880,testability,integr,integration,880,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:74,usability,user,users,74,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:216,usability,user,users,216,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:300,usability,user,user,300,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:305,usability,experien,experience,305,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:954,availability,mainten,maintenance,954,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:158,deployability,contain,contained,158,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:210,deployability,API,API,210,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:226,deployability,depend,dependencies,226,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:379,deployability,instal,installation,379,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:81,integrability,coupl,couple,81,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:120,integrability,sub,submodule,120,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:168,integrability,wrap,wrapper,168,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:210,integrability,API,API,210,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:226,integrability,depend,dependencies,226,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:306,integrability,wrap,wrapper,306,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:570,integrability,wrap,wrappers,570,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:631,integrability,interfac,interfaces,631,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:694,integrability,repositor,repository,694,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:830,integrability,repositor,repository,830,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:168,interoperability,wrapper,wrapper,168,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:210,interoperability,API,API,210,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:306,interoperability,wrapper,wrapper,306,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:570,interoperability,wrapper,wrappers,570,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:631,interoperability,interfac,interfaces,631,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:694,interoperability,repositor,repository,694,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:830,interoperability,repositor,repository,830,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:81,modifiability,coupl,couple,81,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:226,modifiability,depend,dependencies,226,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:425,modifiability,pac,packages,425,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:631,modifiability,interfac,interfaces,631,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:810,modifiability,maintain,maintaining,810,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:131,reliability,rto,rtools,131,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:932,reliability,doe,doesn,932,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:954,reliability,mainten,maintenance,954,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:0,safety,Compl,Completely,0,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:226,safety,depend,dependencies,226,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:554,safety,test,tests,554,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:810,safety,maintain,maintaining,810,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:0,security,Compl,Completely,0,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:81,testability,coupl,couple,81,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:226,testability,depend,dependencies,226,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:554,testability,test,tests,554,"Completely agree, Gökcen! How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:131,integrability,interfac,interface,131,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:403,integrability,sub,submodule,403,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:431,integrability,wrap,wrappers,431,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:131,interoperability,interfac,interface,131,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:431,interoperability,wrapper,wrappers,431,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:131,modifiability,interfac,interface,131,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:495,modifiability,maintain,maintain,495,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2,safety,compl,completely,2,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:495,safety,maintain,maintain,495,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2,security,compl,completely,2,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:89,usability,user,user,89,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:94,usability,experien,experience,94,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:241,usability,efficien,efficient,241,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:356,usability,Person,Personally,356,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:375,usability,prefer,prefer,375,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1209,availability,mainten,maintenance,1209," API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1719,availability,mainten,maintenance,1719,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:164,deployability,contain,contained,164,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:216,deployability,API,API,216,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:232,deployability,depend,dependencies,232,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:385,deployability,instal,installation,385,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1120,deployability,instal,installation,1120,"a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:89,integrability,coupl,couple,89,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:128,integrability,sub,submodule,128,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:174,integrability,wrap,wrapper,174,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:216,integrability,API,API,216,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:232,integrability,depend,dependencies,232,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:312,integrability,wrap,wrapper,312,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1106,integrability,wrap,wrappers,1106,"uld we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-49",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1339,integrability,wrap,wrappers,1339,"g for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1400,integrability,interfac,interfaces,1400,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1463,integrability,repositor,repository,1463,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1597,integrability,repositor,repository,1597,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:174,interoperability,wrapper,wrapper,174,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:216,interoperability,API,API,216,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:312,interoperability,wrapper,wrapper,312,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1106,interoperability,wrapper,wrappers,1106,"uld we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-49",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1339,interoperability,wrapper,wrappers,1339,"g for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1400,interoperability,interfac,interfaces,1400,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1463,interoperability,repositor,repository,1463,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1597,interoperability,repositor,repository,1597,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:89,modifiability,coupl,couple,89,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:232,modifiability,depend,dependencies,232,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:431,modifiability,pac,packages,431,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:642,modifiability,pac,packages,642,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:740,modifiability,pac,packages,740,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:993,modifiability,maintain,maintain,993,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1138,modifiability,pac,packages,1138,"ls? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1248,modifiability,pac,packages,1248," would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There wil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1400,modifiability,interfac,interfaces,1400,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1577,modifiability,maintain,maintaining,1577,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1925,modifiability,pac,packages,1925,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2196,modifiability,pac,packages,2196,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:138,reliability,rto,rtools,138,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1209,reliability,mainten,maintenance,1209," API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1697,reliability,doe,doesn,1697,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1719,reliability,mainten,maintenance,1719,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2,safety,Compl,Completely,2,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:232,safety,depend,dependencies,232,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:993,safety,maintain,maintain,993,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1029,safety,risk,risk,1029," > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1323,safety,test,tests,1323,"s only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1577,safety,maintain,maintaining,1577,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2,security,Compl,Completely,2,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1029,security,risk,risk,1029," > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:89,testability,coupl,couple,89,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:232,testability,depend,dependencies,232,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1323,testability,test,tests,1323,"s only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:932,usability,user,users,932,"> Completely agree, Gökcen! > . > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ref",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1238,usability,support,supported,1238,"s of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:2057,usability,user,user-images,2057,"people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. > . > What do you think? Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:200,deployability,API,API,200,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:843,deployability,instal,installing,843,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:31,integrability,sub,submodule,31,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:100,integrability,sub,submodule,100,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:200,integrability,API,API,200,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:224,integrability,interfac,interfaces,224,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:621,integrability,wrap,wrappers,621,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:756,integrability,sub,submodule,756,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1019,integrability,sub,submodule,1019,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:200,interoperability,API,API,200,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:224,interoperability,interfac,interfaces,224,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:621,interoperability,wrapper,wrappers,621,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:149,modifiability,pac,package,149,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:224,modifiability,interfac,interfaces,224,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:260,modifiability,maintain,maintainers,260,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:640,modifiability,pac,packages,640,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:789,modifiability,maintain,maintaining,789,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:873,modifiability,pac,package,873,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:42,reliability,rto,rtools,42,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:513,reliability,doe,does,513,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:605,reliability,rto,rtools,605,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:748,reliability,rto,rtools,748,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1011,reliability,rto,rtools,1011,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:260,safety,maintain,maintainers,260,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:789,safety,maintain,maintaining,789,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:950,safety,compl,completely,950,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:950,security,compl,completely,950,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:241,usability,tool,tools,241,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:281,usability,tool,tools,281,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:291,usability,help,help,291,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:826,usability,user,user,826,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,availability,error,error,229,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:82,deployability,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:256,deployability,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:277,deployability,instal,installed,277,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:54,integrability,interfac,interface,54,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:82,integrability,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:235,integrability,messag,messages,235,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:256,integrability,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:54,interoperability,interfac,interface,54,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:235,interoperability,messag,messages,235,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:20,modifiability,pac,packages,20,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:54,modifiability,interfac,interface,54,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:82,modifiability,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:256,modifiability,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,performance,error,error,229,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:82,safety,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,safety,error,error,229,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:256,safety,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:82,testability,depend,dependencies,82,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:171,testability,simpl,simply,171,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:256,testability,depend,dependencies,256,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:171,usability,simpl,simply,171,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:223,usability,clear,clear,223,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,usability,error,error,229,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:58,deployability,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:58,integrability,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:76,integrability,interfac,interface,76,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:135,integrability,interfac,interface,135,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:76,interoperability,interfac,interface,76,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:135,interoperability,interfac,interface,135,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:58,modifiability,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:76,modifiability,interfac,interface,76,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:135,modifiability,interfac,interface,135,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:151,reliability,rto,rtools,151,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:58,safety,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:58,testability,depend,dependency,58,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:154,deployability,api,api,154,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:154,integrability,api,api,154,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:154,interoperability,api,api,154,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:31,reliability,rto,rtools,31,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:108,reliability,rto,rtools,108,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:170,safety,test,test,170,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:170,testability,test,test,170,I merged it and moved it into `rtools`. There is a reexport so that you should be able to call it using `sc.rtools.mnn_concatenate` where `sc` is `scanpy.api`. Could you test whether this works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,deployability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:989,deployability,version,version,989,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1186,energy efficiency,current,currently,1186,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,integrability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:204,integrability,batch,batches,204,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:989,integrability,version,version,989,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1254,integrability,repositor,repository,1254,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,interoperability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:165,interoperability,platform,platforms,165,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:906,interoperability,compatib,compatible,906,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1254,interoperability,repositor,repository,1254,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,modifiability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:989,modifiability,version,version,989,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1224,modifiability,pac,pack,1224,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:204,performance,batch,batches,204,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:576,performance,time,time,576,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,reliability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:28,safety,test,test,28,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:70,safety,test,tested,70,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:658,safety,compl,complete,658,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:685,safety,compl,complete,685,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,security,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:658,security,compl,complete,658,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:685,security,compl,complete,685,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:28,testability,test,test,28,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:70,testability,test,tested,70,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:122,testability,integr,integrating,122,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1006,testability,plan,planning,1006,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:297,usability,user,user-images,297,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:704,usability,support,support,704,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:789,usability,user,user-images,789,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:1089,usability,tool,toolbox,1089,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing... ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation. ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png). I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:0,energy efficiency,Cool,Cool,0,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:99,energy efficiency,cool,cooler,99,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:523,energy efficiency,current,currently,523,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:164,integrability,interfac,interface,164,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:164,interoperability,interfac,interface,164,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:164,modifiability,interfac,interface,164,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:409,modifiability,pac,package,409,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:384,testability,plan,planning,384,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:242,usability,tool,tools,242,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:410,deployability,version,version,410,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:203,energy efficiency,optim,optimize,203,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:402,energy efficiency,current,current,402,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:410,integrability,version,version,410,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:410,modifiability,version,version,410,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:203,performance,optimiz,optimize,203,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:274,performance,time,time,274,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:507,reliability,rto,rtools,507,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,safety,test,tests,229,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:229,testability,test,tests,229,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:261,deployability,api,api,261,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:0,energy efficiency,Cool,Cool,0,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:52,integrability,interfac,interface,52,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:261,integrability,api,api,261,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:321,integrability,wrap,wrap,321,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:52,interoperability,interfac,interface,52,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:261,interoperability,api,api,261,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:52,modifiability,interfac,interface,52,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:334,modifiability,pac,packages,334,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:227,reliability,rto,rtools,227,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:192,testability,simpl,simple,192,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:192,usability,simpl,simple,192,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/125:349,usability,user,user,349,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/126:74,energy efficiency,Current,Currently,74,"Hey Ron, do you still plan to upload or link a small example for pypairs? Currently, this links to https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle. I'm happy to put your notebook there. . PS: As you're based on dataframes, you don't accept sparse matrices, right? I'll add a warning for this...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/126
https://github.com/scverse/scanpy/pull/126:22,testability,plan,plan,22,"Hey Ron, do you still plan to upload or link a small example for pypairs? Currently, this links to https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle. I'm happy to put your notebook there. . PS: As you're based on dataframes, you don't accept sparse matrices, right? I'll add a warning for this...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/126
https://github.com/scverse/scanpy/issues/129:120,testability,Simpl,Simply,120,"Oh yes, it's no longer created on purpose since 1.0. But the plotting function obviously still assumes it. I'll fix it. Simply use the 'dpt_pseudotime' annotation instead. `pl.dpt_timeseries` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:120,usability,Simpl,Simply,120,"Oh yes, it's no longer created on purpose since 1.0. But the plotting function obviously still assumes it. I'll fix it. Simply use the 'dpt_pseudotime' annotation instead. `pl.dpt_timeseries` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/132:248,usability,command,command,248,"Hey, this is really strange... Can you, in your notebook, run. ```. !h5ls ./molecule_info.h5. ```. This should give you the output `GRCm38` if it's in the file. I have only seen 10x files that have the default genome `mm10` up to now... . PS: On a command line, you'd do. ```. h5ls ./molecule_info.h5. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/132
https://github.com/scverse/scanpy/issues/132:115,interoperability,format,format,115,Hi and thank you for your answer ! Actually I made a confusion between molecule_info.h5 and the count matrix in h5 format ! That's why it doesn't work !! It works fine with the count matrix if I provide GRCm38 as the genome ID !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/132
https://github.com/scverse/scanpy/issues/132:138,reliability,doe,doesn,138,Hi and thank you for your answer ! Actually I made a confusion between molecule_info.h5 and the count matrix in h5 format ! That's why it doesn't work !! It works fine with the count matrix if I provide GRCm38 as the genome ID !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/132
https://github.com/scverse/scanpy/pull/136:637,deployability,API,API,637,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:1091,deployability,depend,dependencies,1091,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:209,integrability,wrap,wrapper,209,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:238,integrability,wrap,wrapper,238,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:529,integrability,wrap,wrapper,529,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:637,integrability,API,API,637,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:1091,integrability,depend,dependencies,1091,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:209,interoperability,wrapper,wrapper,209,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:238,interoperability,wrapper,wrapper,238,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:529,interoperability,wrapper,wrapper,529,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:637,interoperability,API,API,637,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:789,modifiability,pac,package,789,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:1091,modifiability,depend,dependencies,1091,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:537,performance,parallel,parallelizes,537,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:641,reliability,doe,does,641,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:735,safety,compl,complicated,735,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:1091,safety,depend,dependencies,1091,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:735,security,compl,complicated,735,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:1091,testability,depend,dependencies,1091,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:228,usability,tool,tool,228,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:282,usability,visual,visualization,282,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:362,usability,tool,tools,362,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:442,usability,tool,tools,442,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:595,usability,user,users,595,"Hi Scott! Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function. > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518. > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:55,deployability,instal,installing,55,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:120,integrability,interfac,interface,120,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:120,interoperability,interfac,interface,120,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:120,modifiability,interfac,interface,120,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:173,modifiability,pac,package,173,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:102,performance,time,time,102,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:21,safety,test,test,21,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:21,testability,test,test,21,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:153,testability,simpl,simply,153,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:153,usability,simpl,simply,153,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:77,deployability,API,API,77,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:77,integrability,API,API,77,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:132,integrability,interfac,interface,132,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:77,interoperability,API,API,77,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:132,interoperability,interfac,interface,132,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:132,modifiability,interfac,interface,132,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/issues/137:394,deployability,contain,containing,394,"Dear Francesco, . sorry about the late response... These days are incredibly busy. Currently this is not on the list of urgent things. If you find an elegant way of introducing this in all the plotting functions at once, happy to discuss some prototype code. For now, you can just use your method. ```py. for i in range(nrow*ncol):. if i < len(genes):. gene = genes[i]. # df is the numpy array containing tSNE. ax = sc.pl.scatter(ax=axs[i], show=False). ```. What do you think? Alex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:83,energy efficiency,Current,Currently,83,"Dear Francesco, . sorry about the late response... These days are incredibly busy. Currently this is not on the list of urgent things. If you find an elegant way of introducing this in all the plotting functions at once, happy to discuss some prototype code. For now, you can just use your method. ```py. for i in range(nrow*ncol):. if i < len(genes):. gene = genes[i]. # df is the numpy array containing tSNE. ax = sc.pl.scatter(ax=axs[i], show=False). ```. What do you think? Alex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:243,usability,prototyp,prototype,243,"Dear Francesco, . sorry about the late response... These days are incredibly busy. Currently this is not on the list of urgent things. If you find an elegant way of introducing this in all the plotting functions at once, happy to discuss some prototype code. For now, you can just use your method. ```py. for i in range(nrow*ncol):. if i < len(genes):. gene = genes[i]. # df is the numpy array containing tSNE. ax = sc.pl.scatter(ax=axs[i], show=False). ```. What do you think? Alex.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:86,performance,time,time,86,"Thanks @falexwolf ,. No worries. I'd like to contribute when I can, but I'd need some time to understand how the code is structured. For now I'll be using your suggestion. If I have time I'll provide a PR. Thanks again for the library,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:182,performance,time,time,182,"Thanks @falexwolf ,. No worries. I'd like to contribute when I can, but I'd need some time to understand how the code is structured. For now I'll be using your suggestion. If I have time I'll provide a PR. Thanks again for the library,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:94,testability,understand,understand,94,"Thanks @falexwolf ,. No worries. I'd like to contribute when I can, but I'd need some time to understand how the code is structured. For now I'll be using your suggestion. If I have time I'll provide a PR. Thanks again for the library,. Francesco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:149,availability,error,error,149,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:116,modifiability,variab,variable,116,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:149,performance,error,error,149,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:149,safety,error,error,149,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:253,testability,simpl,simple,253,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:149,usability,error,error,149,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:253,usability,simpl,simple,253,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python. if ax is None:. axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3). else:. axs = [ax]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/138:123,deployability,instal,installation,123,I noticed now that the library is `python-igraph` and not `igraph` as explained in https://scanpy.readthedocs.io/en/latest/installation.html . I am closing this issue since I've solved through the steps in the documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:210,usability,document,documentation,210,I noticed now that the library is `python-igraph` and not `igraph` as explained in https://scanpy.readthedocs.io/en/latest/installation.html . I am closing this issue since I've solved through the steps in the documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:115,availability,cluster,clustering,115,"I've tried to work through the documentation, but am having the same issue as soon as I try to go past the Louvain clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:115,deployability,cluster,clustering,115,"I've tried to work through the documentation, but am having the same issue as soon as I try to go past the Louvain clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:31,usability,document,documentation,31,"I've tried to work through the documentation, but am having the same issue as soon as I try to go past the Louvain clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:29,deployability,instal,installing,29,The above issue was fixed by installing python-igraph and not igraph. Just run. `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:85,deployability,instal,install,85,The above issue was fixed by installing python-igraph and not igraph. Just run. `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:74,deployability,instal,install,74,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:529,deployability,instal,installing,529,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:584,deployability,instal,install,584,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:111,energy efficiency,core,core,111,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:445,integrability,Sub,Subject,445,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:46,performance,time,times,46,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:990,security,auth,auth,990,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]. Sent: Friday, May 24, 2019 3:54 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run. pip install python-igraph. —. You are receiving this because you commented. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:61,deployability,instal,install,61,"@RicedeKrispy . Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:70,availability,error,error,70,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:108,deployability,updat,update,108,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:202,deployability,version,version,202,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:284,deployability,instal,install,284,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:326,deployability,instal,install,326,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:475,deployability,instal,installed,475,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:933,deployability,instal,install,933,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:202,integrability,version,version,202,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:785,integrability,Sub,Subject,785,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:202,modifiability,version,version,202,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:70,performance,error,error,70,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:70,safety,error,error,70,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:108,safety,updat,update,108,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:108,security,updat,update,108,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:178,security,ident,identifying,178,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1423,security,auth,auth,1423,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:70,usability,error,error,70,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:137,usability,user,users,137,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:397,usability,indicat,indicates,397,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]. Sent: Saturday, May 25, 2019 2:15 PM. To: theislab/scanpy <scanpy@noreply.github.com>. Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>. Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>. Hi, if you are using Windows, you can try to install python-igraph from the wheel here. https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —. You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:145,availability,error,errors,145,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:433,availability,down,download,433,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:704,availability,down,download,704,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:479,deployability,Version,Version,479,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:549,deployability,Version,Version,549,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:624,deployability,build,builds,624,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:649,deployability,version,version,649,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:683,deployability,version,version,683,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:366,energy efficiency,core,core,366,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:460,energy efficiency,core,core,460,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:503,energy efficiency,core,core,503,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:578,energy efficiency,core,core,578,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:644,energy efficiency,core,core-version,644,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:731,energy efficiency,core,core,731,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:215,integrability,buffer,buffer,215,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:479,integrability,Version,Version,479,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:549,integrability,Version,Version,549,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:649,integrability,version,version,649,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:683,integrability,version,version,683,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:479,modifiability,Version,Version,479,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:549,modifiability,Version,Version,549,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:649,modifiability,version,version,649,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:683,modifiability,version,version,683,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:65,performance,memor,memory,65,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:145,performance,error,errors,145,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:145,safety,error,errors,145,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:65,usability,memor,memory,65,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:145,usability,error,errors,145,"Hello,. I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated! Skipping optional fixer: buffer. Skipping optional fixer: idioms. Skipping optional fixer: set_literal. Skipping optional fixer: ws_comma. running build_ext. Cannot find the C core of igraph on this system using pkg-config. We will now try to download and compile the C core from scratch. Version number of the C core: 0.7.1.post6. We will also try: 0.7.1. . Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds. Use the --c-core-version switch to try a different version. . Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/pull/139:48,deployability,releas,release,48,"No worries, I would have done this for the next release... . Btw: cool that you made your actual `fit_transform` accept `AnnData` objects... Would not have been necessary, but nice! :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:66,energy efficiency,cool,cool,66,"No worries, I would have done this for the next release... . Btw: cool that you made your actual `fit_transform` accept `AnnData` objects... Would not have been necessary, but nice! :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:94,integrability,interfac,interface,94,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:94,interoperability,interfac,interface,94,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:94,modifiability,interfac,interface,94,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:49,usability,user,user,49,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/139:148,usability,interact,interact,148,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139
https://github.com/scverse/scanpy/pull/141:107,testability,simpl,simple,107,"Nice! Thank you! That's certainly very meaningful. It should just go somewhere else, not in `preprocessing.simple`... Let me think about where to put these queries... We'll have more of this sort of thing in the future! I'll certainly merge this and move it around... However, we'll not make bioservices a hard requirement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:107,usability,simpl,simple,107,"Nice! Thank you! That's certainly very meaningful. It should just go somewhere else, not in `preprocessing.simple`... Let me think about where to put these queries... We'll have more of this sort of thing in the future! I'll certainly merge this and move it around... However, we'll not make bioservices a hard requirement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:2,deployability,updat,updated,2,"I updated the commits, making bioservices optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:2,safety,updat,updated,2,"I updated the commits, making bioservices optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:2,security,updat,updated,2,"I updated the commits, making bioservices optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:511,availability,error,error,511,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:15,deployability,updat,update,15,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,deployability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,integrability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,interoperability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,modifiability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:442,modifiability,variab,variable,442,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:645,modifiability,paramet,parameters,645,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:780,modifiability,paramet,parameter,780,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:511,performance,error,error,511,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,reliability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:15,safety,updat,update,15,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:511,safety,error,error,511,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:15,security,updat,update,15,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,security,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:61,testability,regress,regression,61,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:355,testability,integr,integrated,355,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:511,usability,error,error,511,"Hopefully last update on this PR. What I did:. - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated. - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones. - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:109,integrability,sub,subsequently,109,"Sorry for taking so long with this, I'm merging this now... I'll add a few more commits on the master branch subsequently. Thank you again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/issues/142:44,modifiability,paramet,parameter,44,I'd make it a preprocessing step that has a parameter that allows to save the latent and/or the imputed representation of X. The latent one would go into `.obsm` as `'X_dca'` and would be an alternative to `'X_pca'` when analyzing the manifold using `neighbors` with param `use_rep='X_dca'`. The imputed one would directly overwrite `.X`. Meaningful default in my opinion: only store the late representation... but you're free to do this however you like.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:324,modifiability,paramet,parameter,324,"Maybe it's also in idea to call the latent representation 'Z_dca'... until now, the `Z` nomenclature wasn't really necessary... but thinking about how to explain this in the docs of the dca tool, I guess it would add clarity. Even though, of course, we don't adopt this notation for PCA, for instance. You can make a `mode` parameter: `mode='latent'`, etc... But it requires some thinking about how to do this in the most meaningful way...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:190,usability,tool,tool,190,"Maybe it's also in idea to call the latent representation 'Z_dca'... until now, the `Z` nomenclature wasn't really necessary... but thinking about how to explain this in the docs of the dca tool, I guess it would add clarity. Even though, of course, we don't adopt this notation for PCA, for instance. You can make a `mode` parameter: `mode='latent'`, etc... But it requires some thinking about how to do this in the most meaningful way...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:217,usability,clarit,clarity,217,"Maybe it's also in idea to call the latent representation 'Z_dca'... until now, the `Z` nomenclature wasn't really necessary... but thinking about how to explain this in the docs of the dca tool, I guess it would add clarity. Even though, of course, we don't adopt this notation for PCA, for instance. You can make a `mode` parameter: `mode='latent'`, etc... But it requires some thinking about how to do this in the most meaningful way...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/143:42,deployability,updat,update,42,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:42,safety,updat,update,42,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:42,security,updat,update,42,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:53,usability,document,documentation,53,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/144:326,deployability,automat,automatically,326,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:87,integrability,sub,subsetting,87,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:299,performance,memor,memory,299,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:435,safety,except,except,435,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:326,testability,automat,automatically,326,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:299,usability,memor,memory,299,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/144:383,usability,user,user,383,"Hm, very strange... Did you copy your AnnData somewhere on the way between? Also, when subsetting, internally a view of AnnData is generated. This view listens to whether you want to add an additional annotation to the object... if you do so, a copy is made and the view becomes an actual object in memory... All this happens automatically and you do not need to worry about it as a user... I have not encountered any problem with it, except when you want to change the data matrix of the view... this will be resolved soon. Could it be that something like this happened? Otherwise, no one reported that issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/145:77,modifiability,variab,variable,77,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145
https://github.com/scverse/scanpy/issues/145:199,reliability,Doe,Does,199,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145
https://github.com/scverse/scanpy/issues/145:209,usability,help,help,209,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145
https://github.com/scverse/scanpy/issues/146:32,energy efficiency,load,loading,32,Is this still fixed? I see that loading a 36GB h5ad file requires me to use a machine with 128GB RAM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:32,performance,load,loading,32,Is this still fixed? I see that loading a 36GB h5ad file requires me to use a machine with 128GB RAM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:208,performance,memor,memory,208,"@sjfleming I can't reproduce this behaviour locally anymore. For example:. ```python. In [1]: import anndata as ad. %. In [2]: %load_ext memory_profiler. In [3]: %memit adata = ad.read_h5ad(""tmp.h5ad""). peak memory: 17805.42 MiB, increment: 17595.69 MiB. In [4]: !du -hs tmp.h5ad. 18G	tmp.h5ad. ```. But you would see this if the file is compressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:34,usability,behavi,behaviour,34,"@sjfleming I can't reproduce this behaviour locally anymore. For example:. ```python. In [1]: import anndata as ad. %. In [2]: %load_ext memory_profiler. In [3]: %memit adata = ad.read_h5ad(""tmp.h5ad""). peak memory: 17805.42 MiB, increment: 17595.69 MiB. In [4]: !du -hs tmp.h5ad. 18G	tmp.h5ad. ```. But you would see this if the file is compressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:208,usability,memor,memory,208,"@sjfleming I can't reproduce this behaviour locally anymore. For example:. ```python. In [1]: import anndata as ad. %. In [2]: %load_ext memory_profiler. In [3]: %memit adata = ad.read_h5ad(""tmp.h5ad""). peak memory: 17805.42 MiB, increment: 17595.69 MiB. In [4]: !du -hs tmp.h5ad. 18G	tmp.h5ad. ```. But you would see this if the file is compressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/148:81,deployability,version,version,81,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:102,deployability,updat,updating,102,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:158,deployability,instal,installation,158,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:212,deployability,instal,installation,212,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:230,deployability,instal,installing-miniconda,230,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:81,integrability,version,version,81,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:81,modifiability,version,version,81,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:296,modifiability,pac,packages,296,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:102,safety,updat,updating,102,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:102,security,updat,updating,102,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:36,testability,trace,trace,36,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:32,deployability,instal,install,32,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:42,deployability,upgrad,upgrade,42,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:42,modifiability,upgrad,upgrade,42,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:49,deployability,Upgrad,Upgrading,49,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:118,deployability,instal,installation,118,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:49,modifiability,Upgrad,Upgrading,49,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:147,usability,document,documentation,147,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/150:4,deployability,updat,updated,4,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150
https://github.com/scverse/scanpy/issues/150:4,safety,updat,updated,4,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150
https://github.com/scverse/scanpy/issues/150:4,security,updat,updated,4,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150
https://github.com/scverse/scanpy/issues/153:67,deployability,api,api,67,"Oh, yes... I moved the reexports from `scanpy.plotting` to `scanpy.api.plotting`... one needs to . `import filter_genes_dispersion from scanpy.plotting.preprocessing`, I guess... pull request is of course welcome... Could also fix this myself, if you prefer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:67,integrability,api,api,67,"Oh, yes... I moved the reexports from `scanpy.plotting` to `scanpy.api.plotting`... one needs to . `import filter_genes_dispersion from scanpy.plotting.preprocessing`, I guess... pull request is of course welcome... Could also fix this myself, if you prefer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:67,interoperability,api,api,67,"Oh, yes... I moved the reexports from `scanpy.plotting` to `scanpy.api.plotting`... one needs to . `import filter_genes_dispersion from scanpy.plotting.preprocessing`, I guess... pull request is of course welcome... Could also fix this myself, if you prefer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:251,usability,prefer,prefer,251,"Oh, yes... I moved the reexports from `scanpy.plotting` to `scanpy.api.plotting`... one needs to . `import filter_genes_dispersion from scanpy.plotting.preprocessing`, I guess... pull request is of course welcome... Could also fix this myself, if you prefer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/154:221,deployability,api,api,221,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:232,deployability,api,api,232,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:267,deployability,api,api,267,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:221,integrability,api,api,221,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:232,integrability,api,api,232,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:267,integrability,api,api,267,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:221,interoperability,api,api,221,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:232,interoperability,api,api,232,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:267,interoperability,api,api,267,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:47,testability,Simpl,Simply,47,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:47,usability,Simpl,Simply,47,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:150,usability,document,documentation,150,You're calling this function in the wrong way. Simply type. ```. adata.write_loom('myfilename.loom'). ```. if `adata` is an `AnnData` object. See the documentation of the [function](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.write_loom.html#scanpy.api.AnnData.write_loom).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:150,usability,user,user-images,150,"Hi Alex, . Thank you for your prompt response. I tried what you suggested: . <img width=""735"" alt=""screen shot 2018-05-15 at 7 07 10 pm"" src=""https://user-images.githubusercontent.com/6422882/40088043-36452540-5873-11e8-9e56-04f47cf31d25.png"">. As I have shown you in my previous post adata is an AnnData object. Olivia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:183,integrability,filter,filtered,183,"`sc.AnnData` is the class. You need to call the method of an *object* of that class, like:. ```py. adata = AnnData(...) # Create object with class AnnData. [...]. adata.write_loom('./filtered...loom') # Call write_loom method on that object. ```. `AnnData.write_loom` is a *function* that takes the parameters `self, filename`. `adata.write_loom` is a *method* of the `adata` object with the one parameter `filename`. therefore you can call `AnnData.write_loom(adata, 'file.loom')` or `adata.write_loom('file.loom')`, but you do neither. I hope that was enlightening!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:299,modifiability,paramet,parameters,299,"`sc.AnnData` is the class. You need to call the method of an *object* of that class, like:. ```py. adata = AnnData(...) # Create object with class AnnData. [...]. adata.write_loom('./filtered...loom') # Call write_loom method on that object. ```. `AnnData.write_loom` is a *function* that takes the parameters `self, filename`. `adata.write_loom` is a *method* of the `adata` object with the one parameter `filename`. therefore you can call `AnnData.write_loom(adata, 'file.loom')` or `adata.write_loom('file.loom')`, but you do neither. I hope that was enlightening!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:396,modifiability,paramet,parameter,396,"`sc.AnnData` is the class. You need to call the method of an *object* of that class, like:. ```py. adata = AnnData(...) # Create object with class AnnData. [...]. adata.write_loom('./filtered...loom') # Call write_loom method on that object. ```. `AnnData.write_loom` is a *function* that takes the parameters `self, filename`. `adata.write_loom` is a *method* of the `adata` object with the one parameter `filename`. therefore you can call `AnnData.write_loom(adata, 'file.loom')` or `adata.write_loom('file.loom')`, but you do neither. I hope that was enlightening!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:67,availability,error,error,67,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:428,deployability,modul,module,428,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1522,deployability,log,logging,1522,".6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1704,deployability,log,logging,1704,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:248,interoperability,format,format,248,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:428,modifiability,modul,module,428,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:534,modifiability,pac,packages,534,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:739,modifiability,pac,packages,739,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:975,modifiability,pac,packages,975,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1355,modifiability,pac,packages,1355,"raceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1766,modifiability,pac,packages,1766,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:2069,modifiability,pac,packages,2069,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:2319,modifiability,pac,packages,2319,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:67,performance,error,error,67,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:67,safety,error,error,67,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:402,safety,input,input-,402,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:428,safety,modul,module,428,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1522,safety,log,logging,1522,".6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1704,safety,log,logging,1704,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1522,security,log,logging,1522,".6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1704,security,log,logging,1704,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:358,testability,Trace,Traceback,358,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1522,testability,log,logging,1522,".6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:1704,testability,log,logging,1704,"ename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 1037 for key, vals in col_attrs.items():. /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in set_attr(self, name, values, axis, dtype). 561 . 562 self.delete_attr(name, axis, raise_on_missing=False). --> 563 self._save_attr(name, values, axis). 564 self._load_attr(name, axis). 565 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _save_attr(self, name, values, axis). 166 if self.mode != ""r+"":. 167 raise IOError(""Cannot save attributes when connected in read-only mode""). --> 168 if values.dtype.type is np.str_:. 169 values = np.array([x.encode('ascii', 'ignore') for x in values]). 170 . AttributeError: 'list' object has no attribute 'dtype'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:67,usability,error,error,67,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:402,usability,input,input-,402,"Indeed it was enlightening and useful...However, still gives me an error... ```pytb. type(adata). anndata.base.AnnData. adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). ... writing to '.loom' file densifies sparse matrix. Converting to csc format. Creating. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-8-333dccc1e180> in <module>(). ----> 1 adata.write_loom('./filtered_gene_bc_matrices_h5.loom'). /anaconda3/lib/python3.6/site-packages/anndata/base.py in write_loom(self, filename). 1736 """""". 1737 from .readwrite.write import write_loom. -> 1738 write_loom(filename, self). 1739 . 1740 @staticmethod. /anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in write_loom(filename, adata). 71 if os.path.exists(filename):. 72 os.remove(filename). ---> 73 create(filename, X, row_attrs=row_attrs, col_attrs=col_attrs). 74 . 75 . /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1019 . 1020 if scipy.sparse.issparse(matrix):. -> 1021 return _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1022 . 1023 # Create the file (empty). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in _create_sparse(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 982 if ds is None:. 983 logging.info(""Creating""). --> 984 ds = create(filename, matrix[:, ix:ix + window].toarray(), row_attrs, ca, file_attrs, chunks, chunk_cache, dtype, compression_opts). 985 else:. 986 logging.info(""Adding columns""). /anaconda3/lib/python3.6/site-packages/loompy/loompy.py in create(filename, matrix, row_attrs, col_attrs, file_attrs, chunks, chunk_cache, dtype, compression_opts). 1033 . 1034 for key, vals in row_attrs.items():. -> 1035 ds.set_attr(key, vals, axis=0). 1036 . 103",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:24,availability,error,error,24,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:49,availability,error,error,49,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:110,deployability,version,version,110,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:177,deployability,updat,update,177,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:184,deployability,instal,installation,184,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:110,integrability,version,version,110,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:110,modifiability,version,version,110,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:24,performance,error,error,24,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:49,performance,error,error,49,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:24,safety,error,error,24,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:49,safety,error,error,49,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:177,safety,updat,update,177,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:177,security,updat,update,177,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:24,usability,error,error,24,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/154:49,usability,error,error,49,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154
https://github.com/scverse/scanpy/issues/156:411,deployability,version,version,411,"Hi! The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:411,integrability,version,version,411,"Hi! The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:32,modifiability,variab,variable,32,"Hi! The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:411,modifiability,version,version,411,"Hi! The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:110,security,modif,modify,110,"Hi! The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:90,availability,avail,available,90,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:55,modifiability,variab,variable,55,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:90,reliability,availab,available,90,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:90,safety,avail,available,90,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:90,security,availab,available,90,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:73,usability,confirm,confirm,73,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:184,modifiability,exten,extend,184,"Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:115,usability,user,user,115,"Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:81,energy efficiency,current,currently,81,@falexwolf @fidelram how to we change the color palette for numerical variables? currently setting `palette = 'Oranges'` only works for the categorical ones,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:70,modifiability,variab,variables,70,@falexwolf @fidelram how to we change the color palette for numerical variables? currently setting `palette = 'Oranges'` only works for the categorical ones,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,availability,avail,available,46,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:342,energy efficiency,current,currently,342,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:331,modifiability,variab,variables,331,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,reliability,availab,available,46,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,safety,avail,available,46,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,security,availab,available,46,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:677,security,auth,auth,677,"use `cmap`. In general you can use any option available for. `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram. > <https://github.com/fidelram> how to we change the color palette for. > numerical variables? currently setting palette = 'Oranges' only works for. > the categorical ones. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:27,usability,close,closed,27,"@falexwolf, should this be closed? It looks like the provided color palette is used now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:243,availability,cluster,clusters,243,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. > . > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:243,deployability,cluster,clusters,243,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. > . > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:192,modifiability,exten,extend,192,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. > . > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:123,usability,user,user,123,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. > . > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,reliability,doe,doesn,46,"You can’t distinguish any 120 colors, so that doesn’t make sense. The 64 in “godsnot_64” are already really stretching it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:174,availability,cluster,clusters,174,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:174,deployability,cluster,clusters,174,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:698,integrability,event,event-,698,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:788,security,auth,auth,788,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:101,usability,clear,clear,101,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:137,usability,help,helps,137,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:195,usability,visual,visualize,195,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:309,usability,Close,Closed,309,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will. give you a gradient from a clear to a darker color. Maybe that helps but I. agree with Philipp, 120 clusters is a lot to visualize with different. colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:51,availability,cluster,clusters,51,How can I manually change the colors of my louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:51,deployability,cluster,clusters,51,How can I manually change the colors of my louvain clusters?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:84,deployability,contain,containing,84,"If you take a look at `adata.uns['louvain_colors']` you will see this is an ndarray containing hex codes for the colours. So you can change colours manually by e.g., `adata.uns['louvain_colors'][0] = '#ffff00'`. The array is created the first time you call `sc.pl.umap(adata, color='louvain')` or any other plotting function with `color='louvain'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:243,performance,time,time,243,"If you take a look at `adata.uns['louvain_colors']` you will see this is an ndarray containing hex codes for the colours. So you can change colours manually by e.g., `adata.uns['louvain_colors'][0] = '#ffff00'`. The array is created the first time you call `sc.pl.umap(adata, color='louvain')` or any other plotting function with `color='louvain'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:48,reliability,doe,doesn,48,"> You can’t distinguish any 120 colors, so that doesn’t make sense. The 64 in “godsnot_64” are already really stretching it. Yes, I totally agree with you! Also I think gradient color is a great idea, thanks!!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:174,availability,cluster,clusters,174,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:174,deployability,cluster,clusters,174,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:597,integrability,event,event-,597,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:681,security,auth,auth,681,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:102,usability,clear,clear,102,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:138,usability,help,helps,138,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:195,usability,visual,visualize,195,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:303,usability,Close,Closed,303,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors. > […](#). > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> . > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/158:15,availability,error,error,15,The same exact error also happens using the docker image suggested on the web site. fastgenomics/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:15,performance,error,error,15,The same exact error also happens using the docker image suggested on the web site. fastgenomics/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:15,safety,error,error,15,The same exact error also happens using the docker image suggested on the web site. fastgenomics/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:15,usability,error,error,15,The same exact error also happens using the docker image suggested on the web site. fastgenomics/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:444,availability,error,error,444,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:462,availability,error,error,462,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:218,deployability,version,versions,218,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:250,deployability,version,version,250,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:505,deployability,version,version,505,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:218,integrability,version,versions,218,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:250,integrability,version,version,250,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:505,integrability,version,version,505,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:218,modifiability,version,versions,218,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:250,modifiability,version,version,250,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:505,modifiability,version,version,505,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:444,performance,error,error,444,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:462,performance,error,error,462,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:444,safety,error,error,444,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:462,safety,error,error,462,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:363,usability,learn,learn,363,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:444,usability,error,error,444,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:462,usability,error,error,462,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think? But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run? The latest version of the tutorial says. ```. scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:10,deployability,instal,install,10,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:50,deployability,version,version,50,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:141,deployability,version,versions,141,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:50,integrability,version,version,50,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:141,integrability,version,versions,141,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:113,interoperability,compatib,compatible,113,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:50,modifiability,version,version,50,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:141,modifiability,version,versions,141,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:94,energy efficiency,current,current,94,"Unfortunately, we had a prerelease with a bug in `rank_genes_groups_violin`... on PyPI... the current prelease `1.1a2` should run almost stable... 1.1 will be out in the next couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:175,integrability,coupl,couple,175,"Unfortunately, we had a prerelease with a bug in `rank_genes_groups_violin`... on PyPI... the current prelease `1.1a2` should run almost stable... 1.1 will be out in the next couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:175,modifiability,coupl,couple,175,"Unfortunately, we had a prerelease with a bug in `rank_genes_groups_violin`... on PyPI... the current prelease `1.1a2` should run almost stable... 1.1 will be out in the next couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:175,testability,coupl,couple,175,"Unfortunately, we had a prerelease with a bug in `rank_genes_groups_violin`... on PyPI... the current prelease `1.1a2` should run almost stable... 1.1 will be out in the next couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:103,usability,learn,learn,103,"Hi, here my settings:. scanpy==1.0.4 anndata==0.6.1 numpy==1.14.2 scipy==1.1.0 pandas==0.23.0 . scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1. Ivan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:140,deployability,instal,install,140,"Ok, it's related to pandas 0.23 - runs on pandas 0.22. Don't know what happened but I'll fix this tomorrow. In the meanwhile you could `pip install pandas==0.22.0`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:64,deployability,instal,installing,64,"> Unfortunately, we had a prerelease with a bug. the dangers of installing prereleases :wink:. the docker image is on 1.1a1 though :innocent:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:112,performance,time,time,112,"I was wrong. The pandas 0.23 issue is not related to the prerelease... Unfortunately, I still haven't found the time to figure out what is going wrong... but it's on top of the list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:16,availability,down,downgrade,16,The solution do downgrade to pandas=0.22 worked for me. Thanks,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/159:7,energy efficiency,current,currently,7,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:129,modifiability,exten,extend,129,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:17,reliability,doe,doesn,17,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:115,safety,test,test,115,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:149,safety,test,testing,149,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:115,testability,test,test,115,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:149,testability,test,testing,149,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:157,safety,test,testing,157,Just wanted to bring this question back up - it would be great if we could get fold changes and p-values returned from the relevant methods for differential testing in scanpy. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:157,testability,test,testing,157,Just wanted to bring this question back up - it would be great if we could get fold changes and p-values returned from the relevant methods for differential testing in scanpy. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,deployability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:195,deployability,log,log-fold,195,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:25,energy efficiency,power,powerful,25,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,integrability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,interoperability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:55,modifiability,pac,package,55,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,modifiability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,reliability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:47,safety,test,testing,47,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:195,safety,log,log-fold,195,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:218,safety,test,test,218,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,security,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:195,security,log,log-fold,195,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:47,testability,test,testing,47,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:105,testability,integr,integrates,105,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:195,testability,log,log-fold,195,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:218,testability,test,test,218,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:180,integrability,sub,submit,180,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:40,modifiability,pac,package,40,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:148,modifiability,pac,package,148,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:120,usability,learn,learning,120,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:86,availability,avail,available,86,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:183,modifiability,pac,package,183,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:86,reliability,availab,available,86,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:86,safety,avail,available,86,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:108,safety,detect,detection,108,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:86,security,availab,available,86,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:108,security,detect,detection,108,@falexwolf Is there anyplace where we can read into `diffxpy`? I've been benchmarking available marker gene detection algorithms and am interested to see what is included in this new package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:85,deployability,instal,install,85,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:130,deployability,instal,installation,130,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:143,deployability,fail,failed,143,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:56,modifiability,pac,package,56,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:108,modifiability,pac,package,108,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:143,reliability,fail,failed,143,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:68,integrability,pub,public,68,@falexwolf you were a little quick: @davidsebfischer didn’t turn it public yet. maybe david would like to send it to you for beta testing?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:130,safety,test,testing,130,@falexwolf you were a little quick: @davidsebfischer didn’t turn it public yet. maybe david would like to send it to you for beta testing?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:130,testability,test,testing,130,@falexwolf you were a little quick: @davidsebfischer didn’t turn it public yet. maybe david would like to send it to you for beta testing?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:45,integrability,pub,public,45,"Yes, he told me just yesterday that it's not public yet, but it will soon be.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:35,integrability,pub,public,35,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:110,safety,valid,validation,110,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:110,security,validat,validation,110,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/159:142,usability,help,help,142,"Hi @a-munoz-rojas, diffxpy will be public very soon, once we finished running all benchmarks that we need for validation. I would be happy to help you set it up if you still want to give it a go then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/160:33,availability,error,error,33,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:344,availability,error,error,344,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:785,availability,error,error,785,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:426,deployability,api,api,426,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:867,deployability,api,api,867,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:575,energy efficiency,core,core,575,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:1014,energy efficiency,core,core,1014,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:426,integrability,api,api,426,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:867,integrability,api,api,867,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:426,interoperability,api,api,426,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:867,interoperability,api,api,867,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:558,modifiability,pac,packages,558,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:997,modifiability,pac,packages,997,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:33,performance,error,error,33,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:344,performance,error,error,344,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:785,performance,error,error,785,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:33,safety,error,error,33,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:344,safety,error,error,344,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:785,safety,error,error,785,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:261,testability,trace,tracebacks,261,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:443,testability,Trace,Traceback,443,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:884,testability,Trace,Traceback,884,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:33,usability,error,error,33,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:344,usability,error,error,344,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:580,usability,interact,interactiveshell,580,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:785,usability,error,error,785,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:1019,usability,interact,interactiveshell,1019,"Hi! Thanks for using scanpy! The error is in [`anndata`](https://github.com/theislab/anndata) so filing a bug there would be more fitting. I did it for you: https://github.com/theislab/anndata/issues/23. ---. Lastly, please do ```` ```pytb ```` for your python tracebacks blocks so we get syntax highlighting:. > ````markdown. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-> packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```. > ````. This would look like this:. > I also get the error when I try to use it with jupyter notebook:. > . > ```pytb. > import scanpy.api as sc. > . > Traceback (most recent call last):. > . > File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code. > exec(code_obj, self.user_global_ns, self.user_ns). > . > […]. > . > SyntaxError: invalid syntax. > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:132,deployability,releas,release,132,fixed in https://github.com/theislab/anndata/commit/555c15c8a170944b762ba7ce1d8c0b41f4e4dfbe. the fix should be in the next anndata release (0.6.2 or 0.7),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:45,deployability,instal,install,45,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:147,deployability,instal,installation,147,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:168,deployability,instal,install,168,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:194,deployability,instal,install,194,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:267,deployability,version,version,267,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:267,integrability,version,version,267,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:267,modifiability,version,version,267,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:55,deployability,updat,update,55,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:88,deployability,instal,install,88,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:129,deployability,version,version,129,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:129,integrability,version,version,129,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:129,modifiability,version,version,129,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:55,safety,updat,update,55,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:55,security,updat,update,55,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/pull/161:20,deployability,fail,fail,20,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/pull/161:40,deployability,fail,fail,40,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/pull/161:20,reliability,fail,fail,20,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/pull/161:40,reliability,fail,fail,40,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/pull/161:9,safety,test,tests,9,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/pull/161:9,testability,test,tests,9,"Only the tests that fail on master also fail here, so this is fine and can be merged",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/issues/162:51,availability,error,error,51,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:57,integrability,messag,message,57,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:57,interoperability,messag,message,57,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:51,performance,error,error,51,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:51,safety,error,error,51,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:51,usability,error,error,51,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:6,safety,Test,Tests,6,Done! Tests fixed in 478e3dcb4706328bb3726fb674473e490f353a33,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:6,testability,Test,Tests,6,Done! Tests fixed in 478e3dcb4706328bb3726fb674473e490f353a33,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/163:526,energy efficiency,current,current,526,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]! How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:534,interoperability,standard,standards,534,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]! How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:467,performance,bottleneck,bottleneck,467,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]! How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:292,reliability,doe,does,292,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]! How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:390,security,rotat,rotated,390,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]! How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:322,energy efficiency,current,current,322,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:127,performance,performance issu,performance issues,127,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:232,safety,test,tested,232,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:232,testability,test,tested,232,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:127,usability,perform,performance,127,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:228,availability,slo,slower,228,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:651,deployability,releas,releases,651,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:86,energy efficiency,current,current,86,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:195,energy efficiency,current,current,195,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:245,energy efficiency,alloc,allocation,245,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:168,modifiability,interm,intermediate,168,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:51,performance,memor,memory,51,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:124,performance,memor,memory,124,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:561,performance,memor,memory,561,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:228,reliability,slo,slower,228,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:51,usability,memor,memory,51,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:124,usability,memor,memory,124,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:561,usability,memor,memory,561,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:721,usability,user,user-images,721,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python. def lessalloc_dense(X):. mean = X.mean(axis=0). mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X). var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))). return mean, var. ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/pull/164:44,performance,memor,memory,44,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:127,performance,memor,memory,127,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:31,safety,test,test,31,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:31,testability,test,test,31,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:88,testability,regress,regressors,88,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:44,usability,memor,memory,44,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:127,usability,memor,memory,127,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:134,usability,efficien,efficient,134,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:169,usability,minim,minimal,169,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/issues/165:359,deployability,automat,automatically,359,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:332,security,modif,modify,332,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:454,security,modif,modifying,454,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:94,testability,Simpl,Simply,94,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:359,testability,automat,automatically,359,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:94,usability,Simpl,Simply,94,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/165:257,usability,user,user,257,"Hi Huidong,. thank you for the kind words! `adata_subset` is a `view` on an `AnnData` object. Simply check the output when printing it. You can do. ```. adata_subset = adata[ list_of_barcodes, :].copy(). ```. to get an actual `AnnData`. Of course, you as a user shouldn't have to care about making this copy. As soon as you want to modify the view, it should automatically make this copy itself. This is why all the attributes *listen* to whether you're modifying them or not. I just forgot to add the listener to the *full* `.obs` attribute. I'll do that very soon. In the meanwhile, call `.copy` yourself. ;). Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/166:229,availability,error,error,229,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:407,deployability,version,version,407,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:407,integrability,version,version,407,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:407,modifiability,version,version,407,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:229,performance,error,error,229,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:229,safety,error,error,229,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:229,usability,error,error,229,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:235,usability,hint,hinting,235,"Yes, this is related to the fact that `sanitize_anndata` cannot be meaningfully applied to a view of `AnnData`. You're right that one should also account for this case... I'll give it a thought. At least there should be a proper error hinting people to call `sc.utils.sanitize_anndata` when trying the call you mention. Thank you very much for pointing this out. :smile: It should have happened also before version 1.1, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:229,deployability,fail,fails,229,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:356,deployability,modul,module,356,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:356,modifiability,modul,module,356,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:229,reliability,fail,fails,229,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:328,safety,input,input-,328,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:356,safety,modul,module,356,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:98,testability,assert,assert,98,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:174,testability,assert,assert,174,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:249,testability,assert,assert,249,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:269,testability,Assert,AssertionError,269,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:284,testability,Trace,Traceback,284,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:367,testability,assert,assert,367,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:453,testability,assert,assert,453,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:508,testability,Assert,AssertionError,508,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:328,usability,input,input-,328,"I have something that might be related:. ```python. ad = ad[ad.obs['cell type'] != 'nan'].copy(). assert np.all(ad.obs['cell type'] != 'nan'). sc.utils.sanitize_anndata(ad). assert np.all(ad.obs['cell type'] != 'nan'). ```. This fails in the second assert:. ```python. AssertionError Traceback (most recent call last). <ipython-input-103-2f44e51fdcae> in <module>. 8 assert np.all(ad.obs['cell type'] != 'nan'). 9 sc.utils.sanitize_anndata(ad). ---> 10 assert np.all(ad.obs['cell type'] != 'nan'). 11 . 12 . AssertionError: . ```. It's really black magic, any ideas? PS: `nan`s are really string, not proper NaNs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:141,availability,error,error,141,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:141,performance,error,error,141,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:141,safety,error,error,141,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:205,safety,except,except,205,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:141,usability,error,error,141,I'm having this issue where I read in and merge multiple anndata's with concat. I can't run any of the plotting functions because I get this error. I tried to convert all object/string obs to categorical (except obs names) but I can't really get around it at all.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/168:225,integrability,batch,batch,225,"You can do the first already now by passing `color=genename` to `pl.diffmap`. I don't have much experience with `mnn_correct` but, if cell cycle is a problem, you can definitely still regress this out; for instance, on a per-batch level.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:225,performance,batch,batch,225,"You can do the first already now by passing `color=genename` to `pl.diffmap`. I don't have much experience with `mnn_correct` but, if cell cycle is a problem, you can definitely still regress this out; for instance, on a per-batch level.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,testability,regress,regress,184,"You can do the first already now by passing `color=genename` to `pl.diffmap`. I don't have much experience with `mnn_correct` but, if cell cycle is a problem, you can definitely still regress this out; for instance, on a per-batch level.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:96,usability,experien,experience,96,"You can do the first already now by passing `color=genename` to `pl.diffmap`. I don't have much experience with `mnn_correct` but, if cell cycle is a problem, you can definitely still regress this out; for instance, on a per-batch level.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:241,safety,test,testing,241,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point. ```. adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing. ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:241,testability,test,testing,241,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point. ```. adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing. ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:29,usability,clear,clear,29,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point. ```. adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing. ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:210,usability,visual,visualization,210,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point. ```. adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing. ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:519,availability,down,downstream,519,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:547,availability,cluster,clustering,547,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,deployability,api,api,184,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:195,deployability,api,api,195,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:547,deployability,cluster,clustering,547,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,integrability,api,api,184,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:195,integrability,api,api,195,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:433,integrability,batch,batch,433,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:511,integrability,rout,routine,511,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,interoperability,api,api,184,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:195,interoperability,api,api,195,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:433,performance,batch,batch,433,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:453,performance,perform,performed,453,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:115,safety,test,testing,115,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:596,security,ident,identification,596,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:115,testability,test,testing,115,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:135,usability,document,document,135,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:453,usability,perform,performed,453,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:136,integrability,batch,batch,136,"MNN and CCA is of great use when analyze mutli single cell libraries which are merged together, because each library maybe disturbed by batch effect.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:136,performance,batch,batch,136,"MNN and CCA is of great use when analyze mutli single cell libraries which are merged together, because each library maybe disturbed by batch effect.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:123,availability,cluster,clustering,123,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:441,availability,cluster,clustering,441,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:479,availability,cluster,clusters,479,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,deployability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:123,deployability,cluster,clustering,123,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:441,deployability,cluster,clustering,441,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:479,deployability,cluster,clusters,479,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:656,energy efficiency,model,model,656,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:49,integrability,batch,batch,49,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,integrability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:267,integrability,batch,batch,267,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:370,integrability,batch,batch,370,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:612,integrability,batch,batch,612,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,interoperability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:328,interoperability,standard,standard,328,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:495,interoperability,standard,standard,495,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,modifiability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:49,performance,batch,batch,49,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,performance,perform,performed,184,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:267,performance,batch,batch,267,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:370,performance,batch,batch,370,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:612,performance,batch,batch,612,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,reliability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:504,reliability,pra,practice,504,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:173,safety,test,testing,173,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:563,safety,test,testing,563,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,security,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:656,security,model,model,656,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:74,testability,integr,integration,74,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:173,testability,test,testing,173,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:563,testability,test,testing,563,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:16,usability,help,help,16,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:184,usability,perform,performed,184,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:77,deployability,scale,scale,77,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:77,energy efficiency,scale,scale,77,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:137,energy efficiency,optim,optimal,137,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:226,integrability,compon,components,226,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:226,interoperability,compon,components,226,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:77,modifiability,scal,scale,77,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:226,modifiability,compon,components,226,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:77,performance,scale,scale,77,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:263,performance,perform,perform,263,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:263,usability,perform,perform,263,"@falexwolf . In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:117,usability,tool,tools,117,It should be stored in adata.uns['diffmap_evals'] according to https://github.com/theislab/scanpy/blob/master/scanpy/tools/dpt.py#L17,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:110,deployability,automat,automatically,110,"Yes, the eigenvalues are stored. There is no need to choose a kernel width within in Scanpy. Anything is done automatically. The only parameters are the number of neighbors and the kernel type (`method` in `pp.neighbors`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:134,modifiability,paramet,parameters,134,"Yes, the eigenvalues are stored. There is no need to choose a kernel width within in Scanpy. Anything is done automatically. The only parameters are the number of neighbors and the kernel type (`method` in `pp.neighbors`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:110,testability,automat,automatically,110,"Yes, the eigenvalues are stored. There is no need to choose a kernel width within in Scanpy. Anything is done automatically. The only parameters are the number of neighbors and the kernel type (`method` in `pp.neighbors`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/pull/169:54,interoperability,distribut,distribution,54,Another possible extension: Allow to select different distribution plots (for all of our distribution plots):. - box plot. - [violin plot](https://seaborn.pydata.org/examples/simple_violinplots.html). - [swarm plot](https://seaborn.pydata.org/examples/scatterplot_categorical.html). - [joyplot](https://seaborn.pydata.org/examples/kde_joyplot.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/169
https://github.com/scverse/scanpy/pull/169:89,interoperability,distribut,distribution,89,Another possible extension: Allow to select different distribution plots (for all of our distribution plots):. - box plot. - [violin plot](https://seaborn.pydata.org/examples/simple_violinplots.html). - [swarm plot](https://seaborn.pydata.org/examples/scatterplot_categorical.html). - [joyplot](https://seaborn.pydata.org/examples/kde_joyplot.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/169
https://github.com/scverse/scanpy/pull/169:17,modifiability,extens,extension,17,Another possible extension: Allow to select different distribution plots (for all of our distribution plots):. - box plot. - [violin plot](https://seaborn.pydata.org/examples/simple_violinplots.html). - [swarm plot](https://seaborn.pydata.org/examples/scatterplot_categorical.html). - [joyplot](https://seaborn.pydata.org/examples/kde_joyplot.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/169
https://github.com/scverse/scanpy/issues/170:114,deployability,API,API,114,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:114,integrability,API,API,114,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:114,interoperability,API,API,114,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:25,usability,user,user,25,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:157,usability,user,user,157,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:165,usability,learn,learn,165,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:97,modifiability,paramet,parameter,97,"yes, this sanity check for `n_components` in diffmap definitely makes sense. makes sense for any parameter in any function. but is so much work! :wink: I'll add it for this case, soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:10,safety,sanit,sanity,10,"yes, this sanity check for `n_components` in diffmap definitely makes sense. makes sense for any parameter in any function. but is so much work! :wink: I'll add it for this case, soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:10,security,sanit,sanity,10,"yes, this sanity check for `n_components` in diffmap definitely makes sense. makes sense for any parameter in any function. but is so much work! :wink: I'll add it for this case, soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:87,interoperability,specif,specify,87,"yeah, this one is hard to catch, because it’s not obvious why you shouldn’t be able to specify any number of eigenvectors from 0 to n_obs. most other parameters are kinda obvious (using k=0 makes for a bad number of nearest neighbors)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/170:150,modifiability,paramet,parameters,150,"yeah, this one is hard to catch, because it’s not obvious why you shouldn’t be able to specify any number of eigenvectors from 0 to n_obs. most other parameters are kinda obvious (using k=0 makes for a bad number of nearest neighbors)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170
https://github.com/scverse/scanpy/issues/171:52,integrability,sub,submission,52,Sorry for the long wait! I sent the final files for submission to Fabian in the middle of last night... This should fix it in a backwards compat way: https://github.com/theislab/anndata/commit/8a48281fed5efe582329c05b3f7aaf1d1c7c3f07. https://github.com/theislab/anndata/commit/212adbdb8138e2a8cc1c80c8397346b264ef3448,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171
https://github.com/scverse/scanpy/issues/172:97,deployability,log,log,97,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:253,deployability,log,logarithm,253,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:301,deployability,log,log,301,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:97,safety,log,log,97,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:253,safety,log,logarithm,253,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:301,safety,log,log,301,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:97,security,log,log,97,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:253,security,log,logarithm,253,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:301,security,log,log,301,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:97,testability,log,log,97,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:204,testability,simpl,simple,204,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:229,testability,simpl,simply,229,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:253,testability,log,logarithm,253,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:301,testability,log,log,301,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:468,testability,simpl,simple,468,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:204,usability,simpl,simple,204,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:229,usability,simpl,simply,229,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:351,usability,document,documentation,351,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:468,usability,simpl,simple,468,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:161,deployability,log,log-normalized,161,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:243,deployability,log,log-normalized,243,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:321,deployability,log,log,321,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:105,integrability,batch,batch,105,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:128,integrability,batch,batch,128,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:105,performance,batch,batch,105,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:128,performance,batch,batch,128,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:210,performance,perform,performing,210,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:270,performance,time,time,270,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:161,safety,log,log-normalized,161,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:243,safety,log,log-normalized,243,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:321,safety,log,log,321,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:161,security,log,log-normalized,161,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:243,security,log,log-normalized,243,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:321,security,log,log,321,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:161,testability,log,log-normalized,161,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:243,testability,log,log-normalized,243,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:321,testability,log,log,321,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:210,usability,perform,performing,210,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:317,deployability,log,logarithmized,317,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:405,deployability,contain,contains,405,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:431,deployability,log,log,431,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:496,deployability,log,log,496,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:317,safety,log,logarithmized,317,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:431,safety,log,log,431,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:496,safety,log,log,496,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:317,security,log,logarithmized,317,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:431,security,log,log,431,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:496,security,log,log,496,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:188,testability,simpl,simply,188,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:317,testability,log,logarithmized,317,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:362,testability,simpl,simply,362,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:431,testability,log,log,431,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:496,testability,log,log,496,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:188,usability,simpl,simply,188,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:362,usability,simpl,simply,362,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:758,usability,behavi,behavior,758,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense? At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:237,deployability,fail,fail,237,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:269,deployability,log,log-transformation,269,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:273,integrability,transform,transformation,273,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:27,interoperability,compatib,compatibility,27,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:273,interoperability,transform,transformation,273,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:237,reliability,fail,fail,237,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:158,safety,except,exception,158,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:269,safety,log,log-transformation,269,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:269,security,log,log-transformation,269,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/172:269,testability,log,log-transformation,269,Makes sense that backwards compatibility has to take priority. Integer check will work for most cases (at least it checks if it's still count data). The only exception I can think of is for CPM/size factor normalized data where it would fail (although usually you'd do log-transformation after you normalize otherwise). I can't think of a better method atm either way.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/173:264,integrability,interfac,interface,264,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:264,interoperability,interfac,interface,264,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:99,modifiability,pac,package,99,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:264,modifiability,interfac,interface,264,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:299,modifiability,pac,package,299,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:285,security,access,accessing,285,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:151,deployability,API,API,151,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:421,energy efficiency,Current,Currently,421,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:588,energy efficiency,optim,optimization,588,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:151,integrability,API,API,151,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:151,interoperability,API,API,151,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:79,modifiability,pac,package,79,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:164,modifiability,pac,package,164,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:251,modifiability,pac,package,251,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:359,performance,Perform,Performance,359,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:588,performance,optimiz,optimization,588,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:315,safety,input,input,315,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:144,usability,custom,custom,144,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:315,usability,input,input,315,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:359,usability,Perform,Performance,359,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting? Given my quick overview of your package, two things you should note:. 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets. 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:532,availability,reliab,reliably,532,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:822,availability,avail,available,822,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:53,deployability,API,API,53,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:689,deployability,scale,scale,689,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:919,deployability,API,API,919,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:970,deployability,API,API,970,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1815,deployability,API,API,1815," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1827,deployability,stage,stage,1827," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1884,deployability,modul,module,1884," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1898,deployability,API,API,1898," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2254,deployability,API,API,2254," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:689,energy efficiency,scale,scale,689,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:730,energy efficiency,core,core,730,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:53,integrability,API,API,53,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:220,integrability,interfac,interface,220,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:919,integrability,API,API,919,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:970,integrability,API,API,970,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1217,integrability,event,eventually,1217,"terface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1709,integrability,batch,batch,1709," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1815,integrability,API,API,1815," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1898,integrability,API,API,1898," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2189,integrability,pub,publication,2189," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2254,integrability,API,API,2254," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:53,interoperability,API,API,53,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:220,interoperability,interfac,interface,220,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:919,interoperability,API,API,919,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:970,interoperability,API,API,970,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1237,interoperability,distribut,distribute,1237,"https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1815,interoperability,API,API,1815," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1898,interoperability,API,API,1898," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2254,interoperability,API,API,2254," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:144,modifiability,pac,packages,144,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:220,modifiability,interfac,interface,220,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:518,modifiability,pac,package,518,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:620,modifiability,pac,package,620,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:689,modifiability,scal,scale,689,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1250,modifiability,pac,package,1250,"ub.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1884,modifiability,modul,module,1884," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2288,modifiability,pac,package,2288," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:689,performance,scale,scale,689,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1386,performance,content,content,1386,"cessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1479,performance,content,content,1479," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1709,performance,batch,batch,1709," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:532,reliability,reliab,reliably,532,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:822,reliability,availab,available,822,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:600,safety,prevent,prevent,600,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:822,safety,avail,available,822,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:891,safety,avoid,avoid,891,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1026,safety,detect,detection,1026,"ed to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1527,safety,detect,detection,1527," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1614,safety,detect,detecting,1614," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1884,safety,modul,module,1884," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:103,security,access,access,103,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:600,security,preven,prevent,600,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:822,security,availab,available,822,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1026,security,detect,detection,1026,"ed to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1527,security,detect,detection,1527," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1614,security,detect,detecting,1614," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:213,testability,simpl,simple,213,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:455,testability,Simpl,Simply,455,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:46,usability,custom,custom,46,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:213,usability,simpl,simple,213,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:296,usability,tool,tools,296,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:446,usability,tool,tools,446,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:455,usability,Simpl,Simply,455,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:735,usability,tool,tools,735,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:782,usability,help,helpful,782,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:816,usability,tool,tools,816,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:942,usability,tool,tool,942,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:957,usability,Tool,Tools,957,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1115,usability,tool,tool,1115,"ndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1303,usability,tool,tool,1303,"te.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1554,usability,tool,tools,1554," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1756,usability,tool,tool,1756," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1906,usability,tool,tools,1906," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1957,usability,tool,tool,1957," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2068,usability,Tool,Tools,2068," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2098,usability,tool,tools,2098," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:2227,usability,tool,tool,2227," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:46,availability,avail,available,46,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:66,deployability,version,version,66,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:326,deployability,releas,release,326,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:66,integrability,version,version,66,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:66,modifiability,version,version,66,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:46,reliability,availab,available,46,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:46,safety,avail,available,46,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:46,security,availab,available,46,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:269,availability,down,downstream,269,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:78,safety,detect,detection,78,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:221,safety,compl,completely,221,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:78,security,detect,detection,78,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:221,security,compl,completely,221,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:33,usability,learn,learning,33,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:88,usability,tool,tool,88,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:188,usability,user,users,188,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:232,usability,clear,clear,232,"Good to hear! Looking forward to learning more about it. PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:112,availability,avail,available,112,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:53,energy efficiency,current,currently,53,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:112,reliability,availab,available,112,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:102,safety,detect,detection,102,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:112,safety,avail,available,112,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:102,security,detect,detection,102,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:112,security,availab,available,112,It looks like this may have stalled a bit. Is anyone currently working on making some form of doublet detection available from scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:96,deployability,version,version,96,"Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:96,integrability,version,version,96,"Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:85,interoperability,compatib,compatible,85,"Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:96,modifiability,version,version,96,"Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:140,testability,hook,hook,140,"Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:98,deployability,version,version,98,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:187,deployability,updat,updates,187,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:98,integrability,version,version,98,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:87,interoperability,compatib,compatible,87,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:98,modifiability,version,version,98,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:187,safety,updat,updates,187,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:187,security,updat,updates,187,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:142,testability,hook,hook,142,"> Hi @ivirshup, I've been meaning to get back to this. I've just started on an AnnData-compatible version of Scrublet which should be easy to hook up to Scanpy. Will keep you posted. Any updates on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:83,integrability,wrap,wrapper,83,"I can apply pretty easily scrublet from the original python package, so I. guess a wrapper should be something fast to implement :) I was thinking. about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:412,integrability,sub,subscribed,412,"I can apply pretty easily scrublet from the original python package, so I. guess a wrapper should be something fast to implement :) I was thinking. about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:83,interoperability,wrapper,wrapper,83,"I can apply pretty easily scrublet from the original python package, so I. guess a wrapper should be something fast to implement :) I was thinking. about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:60,modifiability,pac,package,60,"I can apply pretty easily scrublet from the original python package, so I. guess a wrapper should be something fast to implement :) I was thinking. about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:786,security,auth,auth,786,"I can apply pretty easily scrublet from the original python package, so I. guess a wrapper should be something fast to implement :) I was thinking. about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. 🙂. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,deployability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:156,energy efficiency,core,core,156,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,integrability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:143,integrability,wrap,wrapper,143,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,interoperability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:143,interoperability,wrapper,wrapper,143,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,modifiability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,reliability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,security,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:55,testability,integr,integrate,55,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:136,testability,simpl,simple,136,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:136,usability,simpl,simple,136,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:271,usability,document,documentation,271,"@cartal @SamueleSoraggi . For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:77,availability,redund,redundancy,77,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:77,deployability,redundan,redundancy,77,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,deployability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:269,energy efficiency,estimat,estimated,269,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:988,energy efficiency,core,core,988,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:93,integrability,compon,components,93,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,integrability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:975,integrability,wrap,wrapper,975,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:93,interoperability,compon,components,93,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,interoperability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:975,interoperability,wrapper,wrapper,975,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:93,modifiability,compon,components,93,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,modifiability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:401,performance,tune,tune,401,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:77,reliability,redundan,redundancy,77,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:429,reliability,Doe,Does,429,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:513,reliability,doe,does,513,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,reliability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:77,safety,redund,redundancy,77,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,security,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1598,security,auth,auth,1598,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:218,testability,simul,simulation,218,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:455,testability,simul,simulation,455,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:598,testability,simul,simulate,598,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:884,testability,integr,integrate,884,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:968,testability,simpl,simple,968,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:968,usability,simpl,simple,968,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:1108,usability,document,documentation,1108,"Your way sounds sure better, many things into the scrublet algorithm are in. redundancy with components of scanpy. It will sure look great :). Just one thing: in the scrublet paper they suggest always to just run the. simulation of doublets and look at the expected vs estimated fraction of. doublets before removing doublets. If those two values do not match, they. say one should rerun scrublet and tune the expected fraction. Does your script only run simulation of doublets and output the doublets. score, or does it also remove doublets at once? If you do the latter, then. one is not able to simulate doublets more than once to adjust the expected. doublet fraction. Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi. > <https://github.com/SamueleSoraggi>. > For some reason I decided to integrate Scrublet using Scanpy's functions. > where possible, rather than making a simple wrapper. The core functionality. > is up and running in this fork <https://github.com/swolock/scanpy>, and. > now I just need to add documentation, make some of the code more. > Scanpythonic(?), and add an example. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:23,integrability,sub,submit,23,@swolock why don't you submit a PR? I just tested your code and seems to work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:43,safety,test,tested,43,@swolock why don't you submit a PR? I just tested your code and seems to work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:43,testability,test,tested,43,@swolock why don't you submit a PR? I just tested your code and seems to work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,deployability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:86,energy efficiency,current,currently,86,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,integrability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,interoperability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,modifiability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,reliability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,security,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:37,testability,integr,integrate,37,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:65,usability,workflow,workflows,65,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,deployability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,integrability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,interoperability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,modifiability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,reliability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,security,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:121,testability,integr,integrated,121,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:51,usability,workflow,workflow,51,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:100,usability,prefer,prefer,100,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:177,availability,avail,available,177,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:15,deployability,instal,installing,15,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:105,deployability,instal,install,105,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:192,energy efficiency,current,current,192,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:220,energy efficiency,Current,Current,220,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:123,modifiability,Pac,PackagesNotFoundError,123,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:160,modifiability,pac,packages,160,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:759,modifiability,pac,package,759,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:177,reliability,availab,available,177,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:177,safety,avail,available,177,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:177,security,availab,available,177,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:75,usability,help,help,75,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:788,usability,navigat,navigate,788,"I have problem installing and importing scrublet on windows please can you help me. Here is my code !pip install scrublet. PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64. - https://conda.anaconda.org/conda-forge/noarch. - https://repo.anaconda.com/pkgs/main/win-64. - https://repo.anaconda.com/pkgs/main/noarch. - https://repo.anaconda.com/pkgs/r/win-64. - https://repo.anaconda.com/pkgs/r/noarch. - https://repo.anaconda.com/pkgs/msys2/win-64. - https://repo.anaconda.com/pkgs/msys2/noarch. - https://conda.anaconda.org/pytorch/win-64. - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're. looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:118,modifiability,maintain,maintained,118,I started work to move `scrublet` into scanpy (since the last commit was 3 years ago and it’s safe to assume it’s not maintained anymore). https://github.com/scverse/scanpy/pull/2703,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:94,safety,safe,safe,94,I started work to move `scrublet` into scanpy (since the last commit was 3 years ago and it’s safe to assume it’s not maintained anymore). https://github.com/scverse/scanpy/pull/2703,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/173:118,safety,maintain,maintained,118,I started work to move `scrublet` into scanpy (since the last commit was 3 years ago and it’s safe to assume it’s not maintained anymore). https://github.com/scverse/scanpy/pull/2703,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/174:463,deployability,updat,updated,463,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:195,energy efficiency,optim,optimizing,195,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:611,energy efficiency,current,current,611,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:195,performance,optimiz,optimizing,195,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:463,safety,updat,updated,463,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:463,security,updat,updated,463,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:25,usability,behavi,behavior,25,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:68,deployability,version,version,68,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:130,deployability,version,version,130,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:41,energy efficiency,current,currently,41,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:68,integrability,version,version,68,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:130,integrability,version,version,130,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:68,modifiability,version,version,68,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:130,modifiability,version,version,130,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:195,modifiability,evolv,evolved,195,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:208,modifiability,pac,package,208,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:15,deployability,updat,updated,15,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:71,deployability,updat,updated,71,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:15,safety,updat,updated,15,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:71,safety,updat,updated,71,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:15,security,updat,updated,15,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:71,security,updat,updated,71,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:56,testability,understand,understand,56,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:43,usability,help,helps,43,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/pull/175:11,testability,simpl,simply,11,"Wow! Again simply awesome! :smile:. PS: Sorry for the late response, I was on holidays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175
https://github.com/scverse/scanpy/pull/175:11,usability,simpl,simply,11,"Wow! Again simply awesome! :smile:. PS: Sorry for the late response, I was on holidays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175
https://github.com/scverse/scanpy/pull/176:152,availability,avail,available,152,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:424,availability,error,error,424,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:590,availability,error,error,590,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:78,deployability,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:271,deployability,instal,installation,271,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:334,deployability,instal,installation,334,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:375,deployability,instal,installed,375,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:466,deployability,instal,installed,466,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:78,integrability,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:430,integrability,messag,messages,430,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:179,interoperability,platform,platforms,179,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:430,interoperability,messag,messages,430,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:78,modifiability,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:612,modifiability,pac,packages,612,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:108,performance,time,time,108,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:424,performance,error,error,424,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:590,performance,error,error,590,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:152,reliability,availab,available,152,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:78,safety,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:152,safety,avail,available,152,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:424,safety,error,error,424,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:590,safety,error,error,590,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:152,security,availab,available,152,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:78,testability,depend,dependencies,78,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:284,usability,document,document,284,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:424,usability,error,error,424,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:590,usability,error,error,590,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:8,testability,understand,understandable,8,"Totally understandable, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:171,deployability,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:246,deployability,instal,install,246,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:279,deployability,instal,install,279,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:171,integrability,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:171,modifiability,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:171,safety,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:171,testability,depend,dependencies,171,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:58,deployability,instal,installation,58,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:132,modifiability,pac,packages,132,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/pull/176:147,modifiability,evolv,evolve,147,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176
https://github.com/scverse/scanpy/issues/177:0,energy efficiency,Current,Current,0,Current tSNE implementation unfortunately doesn't visualize the kNN graph constructed via `sc.pp.neighbors()` but rather reconstruct it's own kNN graph via euclidean distance. Therefore tSNE calls here ignore the provided jaccard metric. . Can you use any other visualization method to see what Louvain groups correspond to e.g. `sc.tl.draw_graph()` or `sc.tl.umap()` and `sc.pl.*`? (It'll probably not look so nice.),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:42,reliability,doe,doesn,42,Current tSNE implementation unfortunately doesn't visualize the kNN graph constructed via `sc.pp.neighbors()` but rather reconstruct it's own kNN graph via euclidean distance. Therefore tSNE calls here ignore the provided jaccard metric. . Can you use any other visualization method to see what Louvain groups correspond to e.g. `sc.tl.draw_graph()` or `sc.tl.umap()` and `sc.pl.*`? (It'll probably not look so nice.),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:50,usability,visual,visualize,50,Current tSNE implementation unfortunately doesn't visualize the kNN graph constructed via `sc.pp.neighbors()` but rather reconstruct it's own kNN graph via euclidean distance. Therefore tSNE calls here ignore the provided jaccard metric. . Can you use any other visualization method to see what Louvain groups correspond to e.g. `sc.tl.draw_graph()` or `sc.tl.umap()` and `sc.pl.*`? (It'll probably not look so nice.),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:262,usability,visual,visualization,262,Current tSNE implementation unfortunately doesn't visualize the kNN graph constructed via `sc.pp.neighbors()` but rather reconstruct it's own kNN graph via euclidean distance. Therefore tSNE calls here ignore the provided jaccard metric. . Can you use any other visualization method to see what Louvain groups correspond to e.g. `sc.tl.draw_graph()` or `sc.tl.umap()` and `sc.pl.*`? (It'll probably not look so nice.),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:229,availability,cluster,clustering,229,"Hey @gokceneraslan, thanks for looking at this. . . I wouldn't expect these to completely correspond. But, I would expect there to be some level of concordance between them, especially as the these two methods (jaccard-based kNN clustering + tSNE vis) were used in tandem to assign the ""ground truth"" annotations shown above a few months back. Seemed like odd enough behavior is was worth flagging as a potential issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:229,deployability,cluster,clustering,229,"Hey @gokceneraslan, thanks for looking at this. . . I wouldn't expect these to completely correspond. But, I would expect there to be some level of concordance between them, especially as the these two methods (jaccard-based kNN clustering + tSNE vis) were used in tandem to assign the ""ground truth"" annotations shown above a few months back. Seemed like odd enough behavior is was worth flagging as a potential issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:79,safety,compl,completely,79,"Hey @gokceneraslan, thanks for looking at this. . . I wouldn't expect these to completely correspond. But, I would expect there to be some level of concordance between them, especially as the these two methods (jaccard-based kNN clustering + tSNE vis) were used in tandem to assign the ""ground truth"" annotations shown above a few months back. Seemed like odd enough behavior is was worth flagging as a potential issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:79,security,compl,completely,79,"Hey @gokceneraslan, thanks for looking at this. . . I wouldn't expect these to completely correspond. But, I would expect there to be some level of concordance between them, especially as the these two methods (jaccard-based kNN clustering + tSNE vis) were used in tandem to assign the ""ground truth"" annotations shown above a few months back. Seemed like odd enough behavior is was worth flagging as a potential issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:367,usability,behavi,behavior,367,"Hey @gokceneraslan, thanks for looking at this. . . I wouldn't expect these to completely correspond. But, I would expect there to be some level of concordance between them, especially as the these two methods (jaccard-based kNN clustering + tSNE vis) were used in tandem to assign the ""ground truth"" annotations shown above a few months back. Seemed like odd enough behavior is was worth flagging as a potential issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:202,availability,cluster,clusters,202,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:354,availability,cluster,clusters,354,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:764,availability,cluster,clusters,764,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:983,availability,cluster,clusters,983,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1290,availability,cluster,clusters,1290,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:202,deployability,cluster,clusters,202,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:354,deployability,cluster,clusters,354,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:764,deployability,cluster,clusters,764,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:983,deployability,cluster,clusters,983,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1290,deployability,cluster,clusters,1290,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:726,interoperability,specif,specifies,726,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1061,interoperability,specif,specified,1061,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1097,reliability,doe,does,1097,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:688,safety,compl,completely,688,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:688,security,compl,completely,688,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1353,security,hack,hacky,1353,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:519,testability,simpl,simply,519,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:230,usability,user,user-images,230,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:399,usability,user,user-images,399,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:519,usability,simpl,simply,519,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:721,usability,user,user,721,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:1078,usability,user,user,1078,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:128,deployability,continu,continuous,128,Hi Gökcen: makes sense! Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:147,modifiability,variab,variables,147,Hi Gökcen: makes sense! Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:46,safety,compl,completely,46,Hi Gökcen: makes sense! Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:46,security,compl,completely,46,Hi Gökcen: makes sense! Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:568,availability,down,downsampled,568,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:628,availability,cluster,clustering,628,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:309,deployability,log,logged,309,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:537,deployability,version,version,537,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:628,deployability,cluster,clustering,628,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:537,integrability,version,version,537,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:411,interoperability,share,shared,411,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:537,modifiability,version,version,537,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:309,safety,log,logged,309,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:552,safety,compl,complete,552,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:309,security,log,logged,309,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:552,security,compl,complete,552,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:309,testability,log,logged,309,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:753,usability,behavi,behavior,753,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:819,usability,tool,tools,819,"The Jaccard metric, taken from umap, I believe, returns for a pair of vectors x and y the Jaccard distance between their sets of nonzeros. ([Code](https://github.com/theislab/scanpy/blob/7975f0774c0737bccfc312be0f2a81a3922c2185/scanpy/neighbors/umap/distances.py#L156).). In the case that you feed the raw or logged gene expression matrix, this is reasonable: it's the fraction of genes with nonzero expression shared by the two cells. If you compute this on PCA vectors, however, which are essentially all nonzero, you are getting some version of the complete graph, downsampled to k in some random way. This would explain why clustering and embedding based on that graph is garbage. While investigating this, we (me and @sidneymbell) came across some behavior that may or may not be the desired default. If you call `tools._utils.choose_representation` with `use_rep=None, n_pcs=None`, then it will return `X_pca` (all columns) because `X_pca[:,:None] = X_pca`, which will be the top 50 PCs if one is running with defaults. I might have expected this to instead return `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:435,deployability,api,api,435,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:446,deployability,api,api,446,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:475,deployability,api,api,475,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:854,energy efficiency,model,model,854,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:435,integrability,api,api,435,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:446,integrability,api,api,446,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:475,integrability,api,api,475,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:435,interoperability,api,api,435,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:446,interoperability,api,api,446,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:475,interoperability,api,api,475,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:328,modifiability,variab,variables,328,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:153,reliability,doe,does,153,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:854,security,model,model,854,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:219,usability,tool,tools,219,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:347,usability,document,documentation,347,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:594,usability,tool,tools,594,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:624,usability,behavi,behavior,624,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:46,usability,document,documentation,46,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:63,usability,clear,clear,63,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/177:366,usability,behavi,behavior,366,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/178:114,deployability,version,version,114,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:132,deployability,releas,released,132,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:114,integrability,version,version,114,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:114,modifiability,version,version,114,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:272,availability,cluster,clusters,272,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:195,deployability,observ,observation,195,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:272,deployability,cluster,clusters,272,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:309,deployability,observ,observation,309,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:606,deployability,version,version,606,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:624,deployability,releas,released,624,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:134,integrability,sub,subdivides,134,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:606,integrability,version,version,606,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:606,modifiability,version,version,606,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:174,security,hack,hack,174,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:892,security,auth,auth,892,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:195,testability,observ,observation,195,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:309,testability,observ,observation,309,"I thought about adding that functionality. Probably is useful when the. second obs_key has few categories because the second category subdivides. the first category. A quick hack is to add a new observation that is the. combination of the first and second keys. Eg. For 4 clusters, and 2 cell. types, the new observation would be cluster_1_cell_type_1,. cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>. wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,. > cell_type et al.,), and, when will the new version scanpy be released? >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:70,deployability,observ,observation,70,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:158,deployability,releas,release,158,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:172,integrability,sub,subversion,172,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:70,testability,observ,observation,70,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:50,usability,visual,visualizing,50,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/179:65,deployability,api,api,65,"```restructuredtext. Returns . ------- . adata : :class:`~scanpy.api.AnnData` . Annotated data matrix, where obsevations/cells are named by their . barcode and variables/genes by gene name. The data matrix is stored in . `adata.X`, cell names in `adata.obs_names` and gene names in . `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179
https://github.com/scverse/scanpy/issues/179:65,integrability,api,api,65,"```restructuredtext. Returns . ------- . adata : :class:`~scanpy.api.AnnData` . Annotated data matrix, where obsevations/cells are named by their . barcode and variables/genes by gene name. The data matrix is stored in . `adata.X`, cell names in `adata.obs_names` and gene names in . `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179
https://github.com/scverse/scanpy/issues/179:65,interoperability,api,api,65,"```restructuredtext. Returns . ------- . adata : :class:`~scanpy.api.AnnData` . Annotated data matrix, where obsevations/cells are named by their . barcode and variables/genes by gene name. The data matrix is stored in . `adata.X`, cell names in `adata.obs_names` and gene names in . `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179
https://github.com/scverse/scanpy/issues/179:160,modifiability,variab,variables,160,"```restructuredtext. Returns . ------- . adata : :class:`~scanpy.api.AnnData` . Annotated data matrix, where obsevations/cells are named by their . barcode and variables/genes by gene name. The data matrix is stored in . `adata.X`, cell names in `adata.obs_names` and gene names in . `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179
https://github.com/scverse/scanpy/pull/180:15,deployability,updat,update,15,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180
https://github.com/scverse/scanpy/pull/180:26,deployability,instal,installation,26,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180
https://github.com/scverse/scanpy/pull/180:15,safety,updat,update,15,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180
https://github.com/scverse/scanpy/pull/180:15,security,updat,update,15,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180
https://github.com/scverse/scanpy/issues/181:102,modifiability,paramet,parameters,102,"And, I want change the default colors of the tsne plot, but I don't know the relationship between the parameters 'palette' and 'color_map' and I have tried some settings, but they don't work, Can you help me ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:200,usability,help,help,200,"And, I want change the default colors of the tsne plot, but I don't know the relationship between the parameters 'palette' and 'color_map' and I have tried some settings, but they don't work, Can you help me ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:280,availability,cluster,cluster,280,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:373,availability,cluster,cluster,373,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:280,deployability,cluster,cluster,280,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:373,deployability,cluster,cluster,373,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:151,reliability,doe,doesn,151,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:77,security,modif,modifying,77,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:294,usability,visual,visualized,294,"Hi! You can always change the colors of a categorical annotation by directly modifying, e.g. `adata.uns['louvain_colors']`. It's a bug that `palettes` doesn't change it when the colors field is present in `adata.uns`. I'll try to fix this today. How do you want the mean for each cluster to be visualized or stored? If you run `pl.paga(adata, color='mygene')` the mean per cluster is plotted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:71,availability,cluster,cluster,71,"Some times we need to know the average expression of each gene in each cluster, it's better to store it in a pandas dataframe with rows corresponding to each gene and columns corresponding to each cluster, then we can export it to a csv file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:197,availability,cluster,cluster,197,"Some times we need to know the average expression of each gene in each cluster, it's better to store it in a pandas dataframe with rows corresponding to each gene and columns corresponding to each cluster, then we can export it to a csv file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:71,deployability,cluster,cluster,71,"Some times we need to know the average expression of each gene in each cluster, it's better to store it in a pandas dataframe with rows corresponding to each gene and columns corresponding to each cluster, then we can export it to a csv file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:197,deployability,cluster,cluster,197,"Some times we need to know the average expression of each gene in each cluster, it's better to store it in a pandas dataframe with rows corresponding to each gene and columns corresponding to each cluster, then we can export it to a csv file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:5,performance,time,times,5,"Some times we need to know the average expression of each gene in each cluster, it's better to store it in a pandas dataframe with rows corresponding to each gene and columns corresponding to each cluster, then we can export it to a csv file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:714,availability,cluster,cluster,714,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:949,availability,cluster,clustering,949,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:989,availability,cluster,cluster,989,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1423,availability,cluster,clusters,1423,"e stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1491,availability,cluster,clusters,1491,"ey given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1536,availability,cluster,clusters,1536," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1770,availability,cluster,cluster,1770," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:313,deployability,contain,containing,313,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:714,deployability,cluster,cluster,714,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:949,deployability,cluster,clustering,949,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:989,deployability,cluster,cluster,989,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1423,deployability,cluster,clusters,1423,"e stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1491,deployability,cluster,clusters,1491,"ey given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1536,deployability,cluster,clusters,1536," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1639,deployability,scale,scale,1639," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1770,deployability,cluster,cluster,1770," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1910,deployability,continu,continue,1910," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1639,energy efficiency,scale,scale,1639," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1273,interoperability,format,format,1273,"puts:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with inform",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1639,modifiability,scal,scale,1639," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:1639,performance,scale,scale,1639," # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```. You just need to add a python dictionary called 'marker_dict' where keys are strings and values are lists of strings. I think it's explained in the function itself as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:274,safety,Input,Inputs,274,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:527,safety,input,input,527,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:775,safety,Test,Test,775,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:780,safety,input,inputs,780,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:775,testability,Test,Test,775,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:274,usability,Input,Inputs,274,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:527,usability,input,input,527,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:780,usability,input,inputs,780,"I have written a function to do this... I could add it to scanpy. But for the moment, this is it:. ```. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """"""A function go get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:108,integrability,sub,submit,108,"Hi,. I haven't added it yet. But I have a new impetus to do so due to some reviewer's comments... so I will submit a pull request soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:75,safety,review,reviewer,75,"Hi,. I haven't added it yet. But I have a new impetus to do so due to some reviewer's comments... so I will submit a pull request soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:75,testability,review,reviewer,75,"Hi,. I haven't added it yet. But I have a new impetus to do so due to some reviewer's comments... so I will submit a pull request soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:923,availability,consist,consistent,923,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:698,deployability,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:698,integrability,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:127,modifiability,layer,layer,127,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:163,modifiability,layer,layer,163,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:202,modifiability,layer,layers,202,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:209,modifiability,layer,layer,209,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:698,modifiability,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:698,safety,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:698,testability,depend,depending,698,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:923,usability,consist,consistent,923,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. if layer is not None:. getX = lambda x: x.layers[layer]. else:. getX = lambda x: x.X. if gene_symbols is not None:. new_idx = adata.var[idx]. else:. new_idx = adata.var_names. grouped = adata.obs.groupby(group_key). out = pd.DataFrame(. np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. columns=list(grouped.groups.keys()),. index=adata.var_names. ). for group, idx in grouped.indices.items():. X = getX(adata[idx]). out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). return out. ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:991,availability,consist,consistent,991,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:760,deployability,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:760,integrability,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:137,modifiability,layer,layer,137,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:175,modifiability,layer,layer,175,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:216,modifiability,layer,layers,216,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:223,modifiability,layer,layer,223,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:760,modifiability,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:760,safety,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:760,testability,depend,depending,760,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/181:991,usability,consist,consistent,991,"> As a general approach to this kind of problem, I write functions like this:. > . > ```python. > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):. > if layer is not None:. > getX = lambda x: x.layers[layer]. > else:. > getX = lambda x: x.X. > if gene_symbols is not None:. > new_idx = adata.var[idx]. > else:. > new_idx = adata.var_names. > . > grouped = adata.obs.groupby(group_key). > out = pd.DataFrame(. > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),. > columns=list(grouped.groups.keys()),. > index=adata.var_names. > ). > . > for group, idx in grouped.indices.items():. > X = getX(adata[idx]). > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)). > return out. > ```. > . > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. > . > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:. ```python. out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181
https://github.com/scverse/scanpy/issues/182:142,availability,error,error,142,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:404,availability,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:165,deployability,log,log,165,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:654,deployability,Version,Versions,654,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:667,deployability,Resourc,Resources,667,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1732,deployability,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:404,energy efficiency,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:667,energy efficiency,Resourc,Resources,667,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:203,integrability,queue,queue,203,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:654,integrability,Version,Versions,654,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:814,integrability,queue,queue,814,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1732,integrability,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:723,interoperability,Specif,Specific,723,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:654,modifiability,Version,Versions,654,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1732,modifiability,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:142,performance,error,error,142,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:203,performance,queue,queue,203,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:404,performance,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:667,performance,Resourc,Resources,667,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:688,performance,Content,Contents,688,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:814,performance,queue,queue,814,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:404,reliability,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:142,safety,error,error,142,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:165,safety,log,log,165,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:233,safety,Except,Exception,233,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:275,safety,Except,Exception,275,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:336,safety,Except,Exception,336,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:404,safety,fault,fault,404,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:667,safety,Resourc,Resources,667,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1732,safety,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:165,security,log,log,165,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:383,security,Sign,Signal,383,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:445,security,SIGN,SIGNAL,445,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:165,testability,log,log,165,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:667,testability,Resourc,Resources,667,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1732,testability,depend,dependency,1732,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:48,usability,Learn,Learn,48,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:142,usability,error,error,142,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1696,usability,help,help,1696,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1753,usability,Learn,Learn,1753,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```. Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV). Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110. Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11. Termination Reason: Namespace SIGNAL, Code 0xb. Terminating Process: exc handler [0]. VM Regions Near 0x110:. --> . __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:. crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread. 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108. 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546. 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757. 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182. 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807. 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215. 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191. 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463. 8 org.python.python 	0x000000010dead0e6 call_function + 491. 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659. 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747. ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:10,deployability,upgrad,upgrading,10,**Note:** upgrading to Python 3.7.0 (from 3.6.5.1) made the issue disappear for me,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:10,modifiability,upgrad,upgrading,10,**Note:** upgrading to Python 3.7.0 (from 3.6.5.1) made the issue disappear for me,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:77,deployability,version,version,77,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:107,deployability,updat,updating,107,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:128,deployability,updat,updated,128,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:77,integrability,version,version,77,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:77,modifiability,version,version,77,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:107,safety,updat,updating,107,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:128,safety,updat,updated,128,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:107,security,updat,updating,107,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:128,security,updat,updated,128,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/185:22,deployability,automat,automatically,22,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:127,deployability,automat,automatically,127,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:22,testability,automat,automatically,22,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:127,testability,automat,automatically,127,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:36,usability,visual,visualized,36,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:141,usability,visual,visualized,141,"Numeric annotation is automatically visualized with a gradient color (`color_map` argument). String/ Categorical annotation is automatically visualized using palettes (`palette` argument, and `.uns['ANNO_colors']` annotation). If you provide the 'Age' annotation as a numeric (e.g. an integer): `8` instead of `'8Weeks'`, everything will work out fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/187:97,deployability,releas,release,97,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:152,deployability,version,version-,152,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:332,deployability,api,api,332,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:729,deployability,releas,released,729,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:396,energy efficiency,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:152,integrability,version,version-,152,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:332,integrability,api,api,332,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:396,integrability,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:332,interoperability,api,api,332,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:396,interoperability,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:152,modifiability,version,version-,152,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:396,modifiability,adapt,adapt,396,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:19,safety,reme,remember,19,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:78,usability,person,personally,78,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,. Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:76,deployability,log,log,76,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:36,integrability,transform,transformation,36,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:80,integrability,transform,transformation,80,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:134,integrability,batch,batch,134,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:36,interoperability,transform,transformation,36,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:80,interoperability,transform,transformation,80,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:113,interoperability,incompatib,incompatibility,113,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:188,interoperability,compatib,compatible,188,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:134,performance,batch,batch,134,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:76,safety,log,log,76,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:76,security,log,log,76,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:76,testability,log,log,76,"@falexwolf . MAGIC uses root square transformation, not the frequently used log transformation, which causes the incompatibility with batch correction methods, such as CCA and MNN. Is DCA compatible with MNN and CCA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:189,deployability,log,log,189,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:47,integrability,transform,transform,47,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:165,integrability,transform,transformed,165,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:222,integrability,sub,sublinear,222,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:232,integrability,transform,transformation,232,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:47,interoperability,transform,transform,47,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:91,interoperability,incompatib,incompatible,91,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:165,interoperability,transform,transformed,165,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:232,interoperability,transform,transformation,232,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:120,safety,input,inputs,120,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:189,safety,log,log,189,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:189,security,log,log,189,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:189,testability,log,log,189,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:120,usability,input,inputs,120,"@wangjiawen2013 we recommend using square root transform with MAGIC but it's certainly not incompatible. So long as the inputs have been library size normalized and transformed with any of log, sqrt, arcsinh or some other sublinear transformation, MAGIC will work just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:102,deployability,API,API,102,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:184,deployability,API,API,184,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:54,integrability,messag,message,54,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:102,integrability,API,API,102,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:184,integrability,API,API,184,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:54,interoperability,messag,message,54,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:102,interoperability,API,API,102,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:184,interoperability,API,API,184,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:207,security,modif,modification,207,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:171,deployability,artifact,artifact,171,"@wangjiawen2013 if you have any questions about MAGIC, I recommend you post them in [the MAGIC repo](https://github.com/KrishnaswamyLab/MAGIC). The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/187:319,energy efficiency,measur,measure,319,"@wangjiawen2013 if you have any questions about MAGIC, I recommend you post them in [the MAGIC repo](https://github.com/KrishnaswamyLab/MAGIC). The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187
https://github.com/scverse/scanpy/issues/188:100,deployability,log,logarithmized,100,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:145,deployability,log,log,145,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:134,modifiability,paramet,parameter,134,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:100,safety,log,logarithmized,100,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:145,safety,log,log,145,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:100,security,log,logarithmized,100,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:145,security,log,log,145,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:100,testability,log,logarithmized,100,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:145,testability,log,log,145,Scanpy precisely reproduces Seurat's output as outlined in the first tutorial. You can also feed in logarithmized data by passing the parameter `log=True`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/189:119,testability,simpl,simply,119,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:119,usability,simpl,simply,119,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:163,usability,experien,experience,163,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:186,integrability,repositor,repository,186,"One comprehensive benchmark is [this one](https://ieeexplore.ieee.org/document/8388285/) by Zhang et al (not so up-to-date anymore, though). It'd be nice to establish a ""live"" benchmark repository and compare all methods in a transparent, comprehensive and up-to-date way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:186,interoperability,repositor,repository,186,"One comprehensive benchmark is [this one](https://ieeexplore.ieee.org/document/8388285/) by Zhang et al (not so up-to-date anymore, though). It'd be nice to establish a ""live"" benchmark repository and compare all methods in a transparent, comprehensive and up-to-date way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:70,usability,document,document,70,"One comprehensive benchmark is [this one](https://ieeexplore.ieee.org/document/8388285/) by Zhang et al (not so up-to-date anymore, though). It'd be nice to establish a ""live"" benchmark repository and compare all methods in a transparent, comprehensive and up-to-date way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:240,availability,down,downstream,240,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:277,availability,cluster,clustering,277,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:277,deployability,cluster,clustering,277,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:273,integrability,sub,sub-clustering,273,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:422,interoperability,standard,standard,422,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:431,usability,tool,toolkit,431,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:491,usability,document,documentation,491,"@gokceneraslan Yes, I agree a transparent benchmark repo would be very valuable. . I'd also like to see a detailed breakdown of the limitations of each method or imputation in general. It seems problematic to me to use imputed data for all downstream analyses, for example sub-clustering or DGE analysis, but I can't find a discussion of those limitations anywhere. I'm a little wary of imputation methods being part of a standard toolkit without sufficient discussion of limitations in the documentation somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:283,availability,consist,consistent,283,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:160,deployability,api,api,160,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:91,integrability,topic,topic,91,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:160,integrability,api,api,160,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:294,integrability,interfac,interfaces,294,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:392,integrability,interfac,interface,392,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:488,integrability,repositor,repository,488,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:160,interoperability,api,api,160,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:294,interoperability,interfac,interfaces,294,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:392,interoperability,interfac,interface,392,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:488,interoperability,repositor,repository,488,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:294,modifiability,interfac,interfaces,294,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:392,modifiability,interfac,interface,392,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:22,safety,compl,completely,22,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:22,security,compl,completely,22,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:228,security,access,access,228,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:55,usability,document,documentation,55,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:248,usability,tool,tools,248,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:283,usability,consist,consistent,283,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:313,usability,user,users,313,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:354,usability,tool,tools,354,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
